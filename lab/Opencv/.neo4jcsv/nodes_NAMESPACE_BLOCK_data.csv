2,NAMESPACE_BLOCK,<empty>,,<unknown>,<global>,,<global>,1
5,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn.hpp,include\opencv2\dnn.hpp:<global>,,<global>,1
12,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\all_layers.hpp,include\opencv2\dnn\all_layers.hpp:<global>,,<global>,1
16,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{

/** @defgroup dnnLayerList Partial List of Implemented Layers
  @{
  This subsection of dnn module contains information about built-in layers and their descriptions.

  Classes listed here, in fact, provides C++ API for creating instances of built-in layers.
  In addition to this way of layers instantiation, there is a more common factory API (see @ref dnnLayerFactory), it allows to create layers dynamically (by name) and register new ones.
  You can use both API, but factory API is less convenient for native C++ programming and basically designed for use inside importers (see @ref readNetFromCaffe(), @ref readNetFromTorch(), @ref readNetFromTensorflow()).

  Built-in layers partially reproduce functionality of corresponding Caffe and Torch7 layers.
  In particular, the following layers and Caffe importer were tested to reproduce <a href=""http://caffe.berkeleyvision.org/tutorial/layers.html"">Caffe</a>...",1,include\opencv2\dnn\all_layers.hpp,cv,46,cv,1
17,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{

/** @defgroup dnnLayerList Partial List of Implemented Layers
  @{
  This subsection of dnn module contains information about built-in layers and their descriptions.

  Classes listed here, in fact, provides C++ API for creating instances of built-in layers.
  In addition to this way of layers instantiation, there is a more common factory API (see @ref dnnLayerFactory), it allows to create layers dynamically (by name) and register new ones.
  You can use both API, but factory API is less convenient for native C++ programming and basically designed for use inside importers (see @ref readNetFromCaffe(), @ref readNetFromTorch(), @ref readNetFromTensorflow()).

  Built-in layers partially reproduce functionality of corresponding Caffe and Torch7 layers.
  In particular, the following layers and Caffe importer were tested to reproduce <a href=""http://caffe.berkeleyvision.org/tutorial/layers.html"">Caffe</a> functionality:...",1,include\opencv2\dnn\all_layers.hpp,cv.dnn,47,dnn,1
140,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\dict.hpp,include\opencv2\dnn\dict.hpp:<global>,,<global>,1
144,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{

/** @brief This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64.
 *  @todo Maybe int64 is useless because double type exactly stores at least 2^52 integers.
 */
struct CV_EXPORTS_W DictValue
{
    DictValue(const DictValue &r);
    explicit DictValue(bool i)           : type(Param::INT), pi(new AutoBuffer<int64,1>) { (*pi)[0] = i ? 1 : 0; }       //!< Constructs integer scalar
    explicit DictValue(int64 i = 0)      : type(Param::INT), pi(new AutoBuffer<int64,1>) { (*pi)[0] = i; }       //!< Constructs integer scalar
    CV_WRAP explicit DictValue(int i)    : type(Param::INT), pi(new AutoBuffer<int64,1>) { (*pi)[0] = i; }       //!< Constructs integer scalar
    explicit DictValue(unsigned p)       : type(Param::INT), pi(new AutoBuffer<int64,1>) { (*pi)[0] = p; }       //!< Constructs integer scalar
    CV_WRAP explicit DictValue(double p)         :...",1,include\opencv2\dnn\dict.hpp,cv,51,cv,1
145,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{

/** @brief This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64.
 *  @todo Maybe int64 is useless because double type exactly stores at least 2^52 integers.
 */
struct CV_EXPORTS_W DictValue
{
    DictValue(const DictValue &r);
    explicit DictValue(bool i)           : type(Param::INT), pi(new AutoBuffer<int64,1>) { (*pi)[0] = i ? 1 : 0; }       //!< Constructs integer scalar
    explicit DictValue(int64 i = 0)      : type(Param::INT), pi(new AutoBuffer<int64,1>) { (*pi)[0] = i; }       //!< Constructs integer scalar
    CV_WRAP explicit DictValue(int i)    : type(Param::INT), pi(new AutoBuffer<int64,1>) { (*pi)[0] = i; }       //!< Constructs integer scalar
    explicit DictValue(unsigned p)       : type(Param::INT), pi(new AutoBuffer<int64,1>) { (*pi)[0] = p; }       //!< Constructs integer scalar
    CV_WRAP explicit DictValue(double p)         : type(Param::RE...",1,include\opencv2\dnn\dict.hpp,cv.dnn,52,dnn,1
166,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\dnn.hpp,include\opencv2\dnn\dnn.hpp:<global>,,<global>,1
170,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {

namespace accessor {
class DnnNetAccessor;  // forward declaration
}

CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{

    typedef std::vector<int> MatShape;

    /**
     * @brief Enum of computation backends supported by layers.
     * @see Net::setPreferableBackend
     */
    enum Backend
    {
        //! DNN_BACKEND_DEFAULT equals to OPENCV_DNN_BACKEND_DEFAULT, which can be defined using CMake or a configuration parameter
        DNN_BACKEND_DEFAULT = 0,
        DNN_BACKEND_HALIDE,
        DNN_BACKEND_INFERENCE_ENGINE,            //!< Intel OpenVINO computational backend
                                                 //!< @note Tutorial how to build OpenCV with OpenVINO: @ref tutorial_dnn_openvino
        DNN_BACKEND_OPENCV,
        DNN_BACKEND_VKCOM,
        DNN_BACKEND_CUDA,
        DNN_BACKEND_WEBNN,
        DNN_BACKEND_TIMVX,
        DNN_BACKEND_CANN,
#if defined(__OPENCV_BUILD) || defined(BUILD_PLUGIN)
#if !defined(OPENCV_BINDING_PARSE...",1,include\opencv2\dnn\dnn.hpp,cv,53,cv,1
171,NAMESPACE_BLOCK,"namespace dnn {

namespace accessor {
class DnnNetAccessor;  // forward declaration
}

CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{

    typedef std::vector<int> MatShape;

    /**
     * @brief Enum of computation backends supported by layers.
     * @see Net::setPreferableBackend
     */
    enum Backend
    {
        //! DNN_BACKEND_DEFAULT equals to OPENCV_DNN_BACKEND_DEFAULT, which can be defined using CMake or a configuration parameter
        DNN_BACKEND_DEFAULT = 0,
        DNN_BACKEND_HALIDE,
        DNN_BACKEND_INFERENCE_ENGINE,            //!< Intel OpenVINO computational backend
                                                 //!< @note Tutorial how to build OpenCV with OpenVINO: @ref tutorial_dnn_openvino
        DNN_BACKEND_OPENCV,
        DNN_BACKEND_VKCOM,
        DNN_BACKEND_CUDA,
        DNN_BACKEND_WEBNN,
        DNN_BACKEND_TIMVX,
        DNN_BACKEND_CANN,
#if defined(__OPENCV_BUILD) || defined(BUILD_PLUGIN)
#if !defined(OPENCV_BINDING_PARSER)
        DNN_...",1,include\opencv2\dnn\dnn.hpp,cv.dnn,54,dnn,1
172,NAMESPACE_BLOCK,"namespace accessor {
class DnnNetAccessor;  // forward declaration
}",1,include\opencv2\dnn\dnn.hpp,cv.dnn.accessor,56,accessor,1
178,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\dnn.inl.hpp,include\opencv2\dnn\dnn.inl.hpp:<global>,,<global>,1
182,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

template<typename TypeIter>
DictValue DictValue::arrayInt(TypeIter begin, int size)
{
    DictValue res(Param::INT, new AutoBuffer<int64, 1>(size));
    for (int j = 0; j < size; begin++, j++)
        (*res.pi)[j] = *begin;
    return res;
}

template<typename TypeIter>
DictValue DictValue::arrayReal(TypeIter begin, int size)
{
    DictValue res(Param::REAL, new AutoBuffer<double, 1>(size));
    for (int j = 0; j < size; begin++, j++)
        (*res.pd)[j] = *begin;
    return res;
}

template<typename TypeIter>
DictValue DictValue::arrayString(TypeIter begin, int size)
{
    DictValue res(Param::STRING, new AutoBuffer<String, 1>(size));
    for (int j = 0; j < size; begin++, j++)
        (*res.ps)[j] = *begin;
    return res;
}

template<>
inline DictValue DictValue::get<DictValue>(int idx) const
{
    CV_Assert(idx == -1);
    return *this;
}

template<>
inline int64 DictValue::get<int64>(int idx) const
{
    CV_Assert((idx ==...",1,include\opencv2\dnn\dnn.inl.hpp,cv,47,cv,1
183,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

template<typename TypeIter>
DictValue DictValue::arrayInt(TypeIter begin, int size)
{
    DictValue res(Param::INT, new AutoBuffer<int64, 1>(size));
    for (int j = 0; j < size; begin++, j++)
        (*res.pi)[j] = *begin;
    return res;
}

template<typename TypeIter>
DictValue DictValue::arrayReal(TypeIter begin, int size)
{
    DictValue res(Param::REAL, new AutoBuffer<double, 1>(size));
    for (int j = 0; j < size; begin++, j++)
        (*res.pd)[j] = *begin;
    return res;
}

template<typename TypeIter>
DictValue DictValue::arrayString(TypeIter begin, int size)
{
    DictValue res(Param::STRING, new AutoBuffer<String, 1>(size));
    for (int j = 0; j < size; begin++, j++)
        (*res.ps)[j] = *begin;
    return res;
}

template<>
inline DictValue DictValue::get<DictValue>(int idx) const
{
    CV_Assert(idx == -1);
    return *this;
}

template<>
inline int64 DictValue::get<int64>(int idx) const
{
    CV_Assert((idx == -1 && size() =...",1,include\opencv2\dnn\dnn.inl.hpp,cv.dnn,48,dnn,1
1396,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\layer.details.hpp,include\opencv2\dnn\layer.details.hpp:<global>,,<global>,1
1400,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

/** @brief Registers layer constructor in runtime.
*   @param type string, containing type name of the layer.
*   @param constructorFunc pointer to the function of type LayerRegister::Constructor, which creates the layer.
*   @details This macros must be placed inside the function code.
*/
#define CV_DNN_REGISTER_LAYER_FUNC(type, constructorFunc) \
    cv::dnn::LayerFactory::registerLayer(#type, constructorFunc);

/** @brief Registers layer class in runtime.
 *  @param type string, containing type name of the layer.
 *  @param class C++ class, derived from Layer.
 *  @details This macros must be placed inside the function code.
 */
#define CV_DNN_REGISTER_LAYER_CLASS(type, class) \
    cv::dnn::LayerFactory::registerLayer(#type, cv::dnn::details::_layerDynamicRegisterer<class>);

/** @brief Registers layer constructor on module load time.
*   @param type string, containing type name of the layer.
*   @param constructorFunc poin...",1,include\opencv2\dnn\layer.details.hpp,cv,10,cv,1
1401,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

/** @brief Registers layer constructor in runtime.
*   @param type string, containing type name of the layer.
*   @param constructorFunc pointer to the function of type LayerRegister::Constructor, which creates the layer.
*   @details This macros must be placed inside the function code.
*/
#define CV_DNN_REGISTER_LAYER_FUNC(type, constructorFunc) \
    cv::dnn::LayerFactory::registerLayer(#type, constructorFunc);

/** @brief Registers layer class in runtime.
 *  @param type string, containing type name of the layer.
 *  @param class C++ class, derived from Layer.
 *  @details This macros must be placed inside the function code.
 */
#define CV_DNN_REGISTER_LAYER_CLASS(type, class) \
    cv::dnn::LayerFactory::registerLayer(#type, cv::dnn::details::_layerDynamicRegisterer<class>);

/** @brief Registers layer constructor on module load time.
*   @param type string, containing type name of the layer.
*   @param constructorFunc pointer to the func...",1,include\opencv2\dnn\layer.details.hpp,cv.dnn,11,dnn,1
1403,NAMESPACE_BLOCK,"namespace details {

template<typename LayerClass>
Ptr<Layer> _layerDynamicRegisterer(LayerParams &params)
{
    return Ptr<Layer>(LayerClass::create(params));
}

//allows automatically register created layer on module load time
class _LayerStaticRegisterer
{
    String type;
public:

    _LayerStaticRegisterer(const String &layerType, LayerFactory::Constructor layerConstructor)
    {
        this->type = layerType;
        LayerFactory::registerLayer(layerType, layerConstructor);
    }

    ~_LayerStaticRegisterer()
    {
        LayerFactory::unregisterLayer(type);
    }
};

}",1,include\opencv2\dnn\layer.details.hpp,cv.dnn.details,48,details,2
1448,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\layer.hpp,include\opencv2\dnn\layer.hpp:<global>,,<global>,1
1452,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{
//!
//! @defgroup dnnLayerFactory Utilities for New Layers Registration
//! @{

/** @brief %Layer factory allows to create instances of registered layers. */
class CV_EXPORTS LayerFactory
{
public:

    //! Each Layer class must provide this function to the factory
    typedef Ptr<Layer>(*Constructor)(LayerParams &params);

    //! Registers the layer class with typename @p type and specified @p constructor. Thread-safe.
    static void registerLayer(const String &type, Constructor constructor);

    //! Unregisters registered layer with specified type name. Thread-safe.
    static void unregisterLayer(const String &type);

    //! Check if layer is registered.
    static bool isLayerRegistered(const std::string& type);

    /** @brief Creates instance of registered layer.
     *  @param type type name of creating layer.
     *  @param params parameters which will be used for layer initialization.
    ...",1,include\opencv2\dnn\layer.hpp,cv,46,cv,1
1453,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{
//!
//! @defgroup dnnLayerFactory Utilities for New Layers Registration
//! @{

/** @brief %Layer factory allows to create instances of registered layers. */
class CV_EXPORTS LayerFactory
{
public:

    //! Each Layer class must provide this function to the factory
    typedef Ptr<Layer>(*Constructor)(LayerParams &params);

    //! Registers the layer class with typename @p type and specified @p constructor. Thread-safe.
    static void registerLayer(const String &type, Constructor constructor);

    //! Unregisters registered layer with specified type name. Thread-safe.
    static void unregisterLayer(const String &type);

    //! Check if layer is registered.
    static bool isLayerRegistered(const std::string& type);

    /** @brief Creates instance of registered layer.
     *  @param type type name of creating layer.
     *  @param params parameters which will be used for layer initialization.
     *  @note Threa...",1,include\opencv2\dnn\layer.hpp,cv.dnn,47,dnn,1
1459,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\layer_reg.private.hpp,include\opencv2\dnn\layer_reg.private.hpp:<global>,,<global>,1
1463,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{

typedef std::map<std::string, std::vector<LayerFactory::Constructor> > LayerFactory_Impl;

//! Register layer types of DNN model.
//!
//! @note In order to thread-safely access the factory, see getLayerFactoryMutex() function.
LayerFactory_Impl& getLayerFactoryImpl();

//! Get the mutex guarding @ref LayerFactory_Impl, see getLayerFactoryImpl() function.
Mutex& getLayerFactoryMutex();

//! @}
CV__DNN_INLINE_NS_END
}
}",1,include\opencv2\dnn\layer_reg.private.hpp,cv,9,cv,1
1464,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN
//! @addtogroup dnn
//! @{

typedef std::map<std::string, std::vector<LayerFactory::Constructor> > LayerFactory_Impl;

//! Register layer types of DNN model.
//!
//! @note In order to thread-safely access the factory, see getLayerFactoryMutex() function.
LayerFactory_Impl& getLayerFactoryImpl();

//! Get the mutex guarding @ref LayerFactory_Impl, see getLayerFactoryImpl() function.
Mutex& getLayerFactoryMutex();

//! @}
CV__DNN_INLINE_NS_END
}",1,include\opencv2\dnn\layer_reg.private.hpp,cv.dnn,10,dnn,1
1487,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\shape_utils.hpp,include\opencv2\dnn\shape_utils.hpp:<global>,,<global>,1
1491,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

//Slicing

struct _Range : public cv::Range
{
    _Range(const Range &r) : cv::Range(r) {}
    _Range(int start_, int size_ = 1) : cv::Range(start_, start_ + size_) {}
};

static inline Mat slice(const Mat &m, const _Range &r0)
{
    Range ranges[CV_MAX_DIM];
    for (int i = 1; i < m.dims; i++)
        ranges[i] = Range::all();
    ranges[0] = r0;
    return m(&ranges[0]);
}

static inline Mat slice(const Mat &m, const _Range &r0, const _Range &r1)
{
    CV_Assert(m.dims >= 2);
    Range ranges[CV_MAX_DIM];
    for (int i = 2; i < m.dims; i++)
        ranges[i] = Range::all();
    ranges[0] = r0;
    ranges[1] = r1;
    return m(&ranges[0]);
}

static inline Mat slice(const Mat &m, const _Range &r0, const _Range &r1, const _Range &r2)
{
    CV_Assert(m.dims >= 3);
    Range ranges[CV_MAX_DIM];
    for (int i = 3; i < m.dims; i++)
        ranges[i] = Range::all();
    ranges[0] = r0;
    ranges[1] = r1;
    ranges[2] = r2;
    ...",1,include\opencv2\dnn\shape_utils.hpp,cv,51,cv,1
1492,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

//Slicing

struct _Range : public cv::Range
{
    _Range(const Range &r) : cv::Range(r) {}
    _Range(int start_, int size_ = 1) : cv::Range(start_, start_ + size_) {}
};

static inline Mat slice(const Mat &m, const _Range &r0)
{
    Range ranges[CV_MAX_DIM];
    for (int i = 1; i < m.dims; i++)
        ranges[i] = Range::all();
    ranges[0] = r0;
    return m(&ranges[0]);
}

static inline Mat slice(const Mat &m, const _Range &r0, const _Range &r1)
{
    CV_Assert(m.dims >= 2);
    Range ranges[CV_MAX_DIM];
    for (int i = 2; i < m.dims; i++)
        ranges[i] = Range::all();
    ranges[0] = r0;
    ranges[1] = r1;
    return m(&ranges[0]);
}

static inline Mat slice(const Mat &m, const _Range &r0, const _Range &r1, const _Range &r2)
{
    CV_Assert(m.dims >= 3);
    Range ranges[CV_MAX_DIM];
    for (int i = 3; i < m.dims; i++)
        ranges[i] = Range::all();
    ranges[0] = r0;
    ranges[1] = r1;
    ranges[2] = r2;
    return m(&range...",1,include\opencv2\dnn\shape_utils.hpp,cv.dnn,52,dnn,1
1838,NAMESPACE_BLOCK,namespace {inline bool is_neg(int i) { return i < 0; }},1,include\opencv2\dnn\shape_utils.hpp,cv.dnn.anonymous_namespace_0,151,,12
2350,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\utils\debug_utils.hpp,include\opencv2\dnn\utils\debug_utils.hpp:<global>,,<global>,1
2354,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN

/**
 * @brief Skip model import after diagnostic run in readNet() functions.
 * @param[in] skip Indicates whether to skip the import.
 *
 * This is an internal OpenCV function not intended for users.
 */
CV_EXPORTS void skipModelImport(bool skip);

CV__DNN_INLINE_NS_END
}}",1,include\opencv2\dnn\utils\debug_utils.hpp,cv,10,cv,1
2355,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

/**
 * @brief Skip model import after diagnostic run in readNet() functions.
 * @param[in] skip Indicates whether to skip the import.
 *
 * This is an internal OpenCV function not intended for users.
 */
CV_EXPORTS void skipModelImport(bool skip);

CV__DNN_INLINE_NS_END
}",16,include\opencv2\dnn\utils\debug_utils.hpp,cv.dnn,10,dnn,1
2360,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\utils\inference_engine.hpp,include\opencv2\dnn\utils\inference_engine.hpp:<global>,,<global>,1
2364,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN


/* Values for 'OPENCV_DNN_BACKEND_INFERENCE_ENGINE_TYPE' parameter */
/// @deprecated
#define CV_DNN_BACKEND_INFERENCE_ENGINE_NN_BUILDER_API     ""NN_BUILDER""
/// @deprecated
#define CV_DNN_BACKEND_INFERENCE_ENGINE_NGRAPH             ""NGRAPH""

/** @brief Returns Inference Engine internal backend API.
 *
 * See values of `CV_DNN_BACKEND_INFERENCE_ENGINE_*` macros.
 *
 * `OPENCV_DNN_BACKEND_INFERENCE_ENGINE_TYPE` runtime parameter (environment variable) is ignored since 4.6.0.
 *
 * @deprecated
 */
CV_EXPORTS_W cv::String getInferenceEngineBackendType();

/** @brief Specify Inference Engine internal backend API.
 *
 * See values of `CV_DNN_BACKEND_INFERENCE_ENGINE_*` macros.
 *
 * @returns previous value of internal backend API
 *
 * @deprecated
 */
CV_EXPORTS_W cv::String setInferenceEngineBackendType(const cv::String& newBackendType);


/** @brief Release a Myriad device (binded by OpenCV).
 *
 * Single Myriad device cannot be ...",1,include\opencv2\dnn\utils\inference_engine.hpp,cv,13,cv,1
2365,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


/* Values for 'OPENCV_DNN_BACKEND_INFERENCE_ENGINE_TYPE' parameter */
/// @deprecated
#define CV_DNN_BACKEND_INFERENCE_ENGINE_NN_BUILDER_API     ""NN_BUILDER""
/// @deprecated
#define CV_DNN_BACKEND_INFERENCE_ENGINE_NGRAPH             ""NGRAPH""

/** @brief Returns Inference Engine internal backend API.
 *
 * See values of `CV_DNN_BACKEND_INFERENCE_ENGINE_*` macros.
 *
 * `OPENCV_DNN_BACKEND_INFERENCE_ENGINE_TYPE` runtime parameter (environment variable) is ignored since 4.6.0.
 *
 * @deprecated
 */
CV_EXPORTS_W cv::String getInferenceEngineBackendType();

/** @brief Specify Inference Engine internal backend API.
 *
 * See values of `CV_DNN_BACKEND_INFERENCE_ENGINE_*` macros.
 *
 * @returns previous value of internal backend API
 *
 * @deprecated
 */
CV_EXPORTS_W cv::String setInferenceEngineBackendType(const cv::String& newBackendType);


/** @brief Release a Myriad device (binded by OpenCV).
 *
 * Single Myriad device cannot be shared across m...",16,include\opencv2\dnn\utils\inference_engine.hpp,cv.dnn,13,dnn,1
2368,NAMESPACE_BLOCK,<empty>,,include\opencv2\dnn\version.hpp,include\opencv2\dnn\version.hpp:<global>,,<global>,1
2372,NAMESPACE_BLOCK,namespace cv { namespace dnn { namespace CV__DNN_INLINE_NS { } using namespace CV__DNN_INLINE_NS; }},1,include\opencv2\dnn\version.hpp,cv,15,cv,1
2373,NAMESPACE_BLOCK,namespace dnn { namespace CV__DNN_INLINE_NS { } using namespace CV__DNN_INLINE_NS; },16,include\opencv2\dnn\version.hpp,cv.dnn,15,dnn,1
2399,NAMESPACE_BLOCK,<empty>,,misc\caffe\opencv-caffe.pb.cc,misc\caffe\opencv-caffe.pb.cc:<global>,,<global>,1
2404,NAMESPACE_BLOCK,"namespace opencv_caffe {
constexpr BlobShape::BlobShape(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : dim_()
  , _dim_cached_byte_size_(0){}
struct BlobShapeDefaultTypeInternal {
  constexpr BlobShapeDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~BlobShapeDefaultTypeInternal() {}
  union {
    BlobShape _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT BlobShapeDefaultTypeInternal _BlobShape_default_instance_;
constexpr BlobProto::BlobProto(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : data_()
  , diff_()
  , double_data_()
  , double_diff_()
  , raw_data_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , shape_(nullptr)
  , num_(0)
  , channels_(0)
  , height_(0)
  , width_(0)
  , raw_data_type_(0)
{}
struct BlobProtoDefaultTypeInternal {
  constexpr BlobProtoDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ...",1,misc\caffe\opencv-caffe.pb.cc,opencv_caffe,19,opencv_caffe,2
4162,NAMESPACE_BLOCK,"namespace opencv_caffe {
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* PriorBoxParameter_CodeType_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_opencv_2dcaffe_2eproto);
  return file_level_enum_descriptors_opencv_2dcaffe_2eproto[0];
}
bool PriorBoxParameter_CodeType_IsValid(int value) {
  switch (value) {
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
constexpr PriorBoxParameter_CodeType PriorBoxParameter::CORNER;
constexpr PriorBoxParameter_CodeType PriorBoxParameter::CENTER_SIZE;
constexpr PriorBoxParameter_CodeType PriorBoxParameter::CodeType_MIN;
constexpr PriorBoxParameter_CodeType PriorBoxParameter::CodeType_MAX;
constexpr int PriorBoxParameter::CodeType_ARRAYSIZE;
#endif  // (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* ...",1,misc\caffe\opencv-caffe.pb.cc,opencv_caffe,3358,opencv_caffe,22
109292,NAMESPACE_BLOCK,<empty>,,misc\caffe\opencv-caffe.pb.hpp,misc\caffe\opencv-caffe.pb.hpp:<global>,,<global>,1
109297,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\caffe\opencv-caffe.pb.hpp,internal,39,internal,2
109325,NAMESPACE_BLOCK,"namespace opencv_caffe {
class AccuracyParameter;
struct AccuracyParameterDefaultTypeInternal;
extern AccuracyParameterDefaultTypeInternal _AccuracyParameter_default_instance_;
class ArgMaxParameter;
struct ArgMaxParameterDefaultTypeInternal;
extern ArgMaxParameterDefaultTypeInternal _ArgMaxParameter_default_instance_;
class BatchNormParameter;
struct BatchNormParameterDefaultTypeInternal;
extern BatchNormParameterDefaultTypeInternal _BatchNormParameter_default_instance_;
class BiasParameter;
struct BiasParameterDefaultTypeInternal;
extern BiasParameterDefaultTypeInternal _BiasParameter_default_instance_;
class BlobProto;
struct BlobProtoDefaultTypeInternal;
extern BlobProtoDefaultTypeInternal _BlobProto_default_instance_;
class BlobProtoVector;
struct BlobProtoVectorDefaultTypeInternal;
extern BlobProtoVectorDefaultTypeInternal _BlobProtoVector_default_instance_;
class BlobShape;
struct BlobShapeDefaultTypeInternal;
extern BlobShapeDefaultTypeInternal _BlobShape_default_instance_;
...",1,misc\caffe\opencv-caffe.pb.hpp,opencv_caffe,57,opencv_caffe,6
109896,NAMESPACE_BLOCK,"namespace opencv_caffe {

enum PriorBoxParameter_CodeType : int {
  PriorBoxParameter_CodeType_CORNER = 1,
  PriorBoxParameter_CodeType_CENTER_SIZE = 2
};
bool PriorBoxParameter_CodeType_IsValid(int value);
constexpr PriorBoxParameter_CodeType PriorBoxParameter_CodeType_CodeType_MIN = PriorBoxParameter_CodeType_CORNER;
constexpr PriorBoxParameter_CodeType PriorBoxParameter_CodeType_CodeType_MAX = PriorBoxParameter_CodeType_CENTER_SIZE;
constexpr int PriorBoxParameter_CodeType_CodeType_ARRAYSIZE = PriorBoxParameter_CodeType_CodeType_MAX + 1;

const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* PriorBoxParameter_CodeType_descriptor();
template<typename T>
inline const std::string& PriorBoxParameter_CodeType_Name(T enum_t_value) {
  static_assert(::std::is_same<T, PriorBoxParameter_CodeType>::value ||
    ::std::is_integral<T>::value,
    ""Incorrect type passed to function PriorBoxParameter_CodeType_Name."");
  return ::PROTOBUF_NAMESPACE_ID::internal::NameOfEnum(
    PriorBoxParameter_CodeT...",1,misc\caffe\opencv-caffe.pb.hpp,opencv_caffe,345,opencv_caffe,80
221910,NAMESPACE_BLOCK,<empty>,,misc\java\src\cpp\dnn_converters.cpp,misc\java\src\cpp\dnn_converters.cpp:<global>,,<global>,1
222406,NAMESPACE_BLOCK,<empty>,,misc\java\src\cpp\dnn_converters.hpp,misc\java\src\cpp\dnn_converters.hpp:<global>,,<global>,1
222469,NAMESPACE_BLOCK,<empty>,,misc\onnx\opencv-onnx.pb.cc,misc\onnx\opencv-onnx.pb.cc:<global>,,<global>,1
222474,NAMESPACE_BLOCK,"namespace opencv_onnx {
constexpr AttributeProto::AttributeProto(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : floats_()
  , ints_()
  , strings_()
  , tensors_()
  , graphs_()
  , name_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , s_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , doc_string_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , ref_attr_name_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , t_(nullptr)
  , g_(nullptr)
  , i_(int64_t{0})
  , f_(0)
  , type_(0)
{}
struct AttributeProtoDefaultTypeInternal {
  constexpr AttributeProtoDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~AttributeProtoDefaultTypeInternal() {}
  union {
    AttributeProto _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT AttributeProtoDefaultTypeInternal _AttributeProto_default_instance_;
constexpr ValueInfoProto::ValueInfoProto(
...",1,misc\onnx\opencv-onnx.pb.cc,opencv_onnx,19,opencv_onnx,2
222840,NAMESPACE_BLOCK,"namespace opencv_onnx {
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* AttributeProto_AttributeType_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_opencv_2donnx_2eproto);
  return file_level_enum_descriptors_opencv_2donnx_2eproto[0];
}
bool AttributeProto_AttributeType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
    case 8:
    case 9:
    case 10:
      return true;
    default:
      return false;
  }
}

#if (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
constexpr AttributeProto_AttributeType AttributeProto::UNDEFINED;
constexpr AttributeProto_AttributeType AttributeProto::FLOAT;
constexpr AttributeProto_AttributeType AttributeProto::INT;
constexpr AttributeProto_AttributeType AttributeProto::STRING;
constexpr AttributeProto_AttributeType AttributeProto::TENSOR;
constexpr AttributeProto_AttributeType Attribute...",1,misc\onnx\opencv-onnx.pb.cc,opencv_onnx,563,opencv_onnx,22
240825,NAMESPACE_BLOCK,<empty>,,misc\onnx\opencv-onnx.pb.hpp,misc\onnx\opencv-onnx.pb.hpp:<global>,,<global>,1
240830,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\onnx\opencv-onnx.pb.hpp,internal,39,internal,2
240858,NAMESPACE_BLOCK,"namespace opencv_onnx {
class AttributeProto;
struct AttributeProtoDefaultTypeInternal;
extern AttributeProtoDefaultTypeInternal _AttributeProto_default_instance_;
class GraphProto;
struct GraphProtoDefaultTypeInternal;
extern GraphProtoDefaultTypeInternal _GraphProto_default_instance_;
class ModelProto;
struct ModelProtoDefaultTypeInternal;
extern ModelProtoDefaultTypeInternal _ModelProto_default_instance_;
class NodeProto;
struct NodeProtoDefaultTypeInternal;
extern NodeProtoDefaultTypeInternal _NodeProto_default_instance_;
class OperatorSetIdProto;
struct OperatorSetIdProtoDefaultTypeInternal;
extern OperatorSetIdProtoDefaultTypeInternal _OperatorSetIdProto_default_instance_;
class StringStringEntryProto;
struct StringStringEntryProtoDefaultTypeInternal;
extern StringStringEntryProtoDefaultTypeInternal _StringStringEntryProto_default_instance_;
class TensorProto;
struct TensorProtoDefaultTypeInternal;
extern TensorProtoDefaultTypeInternal _TensorProto_default_instance_;
class Ten...",1,misc\onnx\opencv-onnx.pb.hpp,opencv_onnx,57,opencv_onnx,6
240965,NAMESPACE_BLOCK,"namespace opencv_onnx {

enum AttributeProto_AttributeType : int {
  AttributeProto_AttributeType_UNDEFINED = 0,
  AttributeProto_AttributeType_FLOAT = 1,
  AttributeProto_AttributeType_INT = 2,
  AttributeProto_AttributeType_STRING = 3,
  AttributeProto_AttributeType_TENSOR = 4,
  AttributeProto_AttributeType_GRAPH = 5,
  AttributeProto_AttributeType_FLOATS = 6,
  AttributeProto_AttributeType_INTS = 7,
  AttributeProto_AttributeType_STRINGS = 8,
  AttributeProto_AttributeType_TENSORS = 9,
  AttributeProto_AttributeType_GRAPHS = 10
};
bool AttributeProto_AttributeType_IsValid(int value);
constexpr AttributeProto_AttributeType AttributeProto_AttributeType_AttributeType_MIN = AttributeProto_AttributeType_UNDEFINED;
constexpr AttributeProto_AttributeType AttributeProto_AttributeType_AttributeType_MAX = AttributeProto_AttributeType_GRAPHS;
constexpr int AttributeProto_AttributeType_AttributeType_ARRAYSIZE = AttributeProto_AttributeType_AttributeType_MAX + 1;

const ::PROTOBUF_NAMESPACE_...",1,misc\onnx\opencv-onnx.pb.hpp,opencv_onnx,113,opencv_onnx,22
258798,NAMESPACE_BLOCK,<empty>,,misc\python\pyopencv_dnn.hpp,misc\python\pyopencv_dnn.hpp:<global>,,<global>,1
258825,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\attr_value.pb.cc,misc\tensorflow\attr_value.pb.cc:<global>,,<global>,1
258830,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
constexpr AttrValue_ListValue::AttrValue_ListValue(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : s_()
  , i_()
  , _i_cached_byte_size_(0)
  , f_()
  , b_()
  , type_()
  , _type_cached_byte_size_(0)
  , shape_()
  , tensor_(){}
struct AttrValue_ListValueDefaultTypeInternal {
  constexpr AttrValue_ListValueDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~AttrValue_ListValueDefaultTypeInternal() {}
  union {
    AttrValue_ListValue _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT AttrValue_ListValueDefaultTypeInternal _AttrValue_ListValue_default_instance_;
constexpr AttrValue::AttrValue(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : _oneof_case_{}{}
struct AttrValueDefaultTypeInternal {
  constexpr AttrValueDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~AttrValueDefaultTypeInternal() {}
  union {...",1,misc\tensorflow\attr_value.pb.cc,opencv_tensorflow,19,opencv_tensorflow,2
258997,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class AttrValue_ListValue::_Internal {
 public:
};

void AttrValue_ListValue::clear_shape() {
  shape_.Clear();
}
void AttrValue_ListValue::clear_tensor() {
  tensor_.Clear();
}
AttrValue_ListValue::AttrValue_ListValue(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  s_(arena),
  i_(arena),
  f_(arena),
  b_(arena),
  type_(arena),
  shape_(arena),
  tensor_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:opencv_tensorflow.AttrValue.ListValue)
}
AttrValue_ListValue::AttrValue_ListValue(const AttrValue_ListValue& from)
  : ::PROTOBUF_NAMESPACE_ID::Message(),
      s_(from.s_),
      i_(from.i_),
      f_(from.f_),
      b_(from.b_),
      type_(from.type_),
      shape_(from.shape_),
      tensor_...",1,misc\tensorflow\attr_value.pb.cc,opencv_tensorflow,187,opencv_tensorflow,24
263413,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\attr_value.pb.hpp,misc\tensorflow\attr_value.pb.hpp:<global>,,<global>,1
263418,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\tensorflow\attr_value.pb.hpp,internal,44,internal,2
263446,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
class AttrValue;
struct AttrValueDefaultTypeInternal;
extern AttrValueDefaultTypeInternal _AttrValue_default_instance_;
class AttrValue_ListValue;
struct AttrValue_ListValueDefaultTypeInternal;
extern AttrValue_ListValueDefaultTypeInternal _AttrValue_ListValue_default_instance_;
class NameAttrList;
struct NameAttrListDefaultTypeInternal;
extern NameAttrListDefaultTypeInternal _NameAttrList_default_instance_;
class NameAttrList_AttrEntry_DoNotUse;
struct NameAttrList_AttrEntry_DoNotUseDefaultTypeInternal;
extern NameAttrList_AttrEntry_DoNotUseDefaultTypeInternal _NameAttrList_AttrEntry_DoNotUse_default_instance_;
}",1,misc\tensorflow\attr_value.pb.hpp,opencv_tensorflow,62,opencv_tensorflow,6
263481,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class AttrValue_ListValue final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:opencv_tensorflow.AttrValue.ListValue) */ {
 public:
  inline AttrValue_ListValue() : AttrValue_ListValue(nullptr) {}
  ~AttrValue_ListValue() override;
  explicit constexpr AttrValue_ListValue(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  AttrValue_ListValue(const AttrValue_ListValue& from);
  AttrValue_ListValue(AttrValue_ListValue&& from) noexcept
    : AttrValue_ListValue() {
    *this = ::std::move(from);
  }

  inline AttrValue_ListValue& operator=(const AttrValue_ListValue& from) {
    CopyFrom(from);
    return *this;
  }
  inline AttrValue_ListValue& operator=(AttrValue_ListValue&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwni...",1,misc\tensorflow\attr_value.pb.hpp,opencv_tensorflow,82,opencv_tensorflow,13
267958,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\function.pb.cc,misc\tensorflow\function.pb.cc:<global>,,<global>,1
267963,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
constexpr FunctionDefLibrary::FunctionDefLibrary(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : function_()
  , gradient_(){}
struct FunctionDefLibraryDefaultTypeInternal {
  constexpr FunctionDefLibraryDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~FunctionDefLibraryDefaultTypeInternal() {}
  union {
    FunctionDefLibrary _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT FunctionDefLibraryDefaultTypeInternal _FunctionDefLibrary_default_instance_;
constexpr FunctionDef_Node_AttrEntry_DoNotUse::FunctionDef_Node_AttrEntry_DoNotUse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct FunctionDef_Node_AttrEntry_DoNotUseDefaultTypeInternal {
  constexpr FunctionDef_Node_AttrEntry_DoNotUseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~FunctionDef_Node_AttrEntry_DoNotUseDefaultTypeInternal() {}
  union ...",1,misc\tensorflow\function.pb.cc,opencv_tensorflow,19,opencv_tensorflow,2
268150,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class FunctionDefLibrary::_Internal {
 public:
};

FunctionDefLibrary::FunctionDefLibrary(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  function_(arena),
  gradient_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:opencv_tensorflow.FunctionDefLibrary)
}
FunctionDefLibrary::FunctionDefLibrary(const FunctionDefLibrary& from)
  : ::PROTOBUF_NAMESPACE_ID::Message(),
      function_(from.function_),
      gradient_(from.gradient_) {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:opencv_tensorflow.FunctionDefLibrary)
}

inline void FunctionDefLibrary::SharedCtor() {
}

FunctionDefLibrary::~FunctionDe...",1,misc\tensorflow\function.pb.cc,opencv_tensorflow,189,opencv_tensorflow,24
272090,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\function.pb.hpp,misc\tensorflow\function.pb.hpp:<global>,,<global>,1
272095,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\tensorflow\function.pb.hpp,internal,43,internal,2
272123,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
class FunctionDef;
struct FunctionDefDefaultTypeInternal;
extern FunctionDefDefaultTypeInternal _FunctionDef_default_instance_;
class FunctionDefLibrary;
struct FunctionDefLibraryDefaultTypeInternal;
extern FunctionDefLibraryDefaultTypeInternal _FunctionDefLibrary_default_instance_;
class FunctionDef_Node;
struct FunctionDef_NodeDefaultTypeInternal;
extern FunctionDef_NodeDefaultTypeInternal _FunctionDef_Node_default_instance_;
class FunctionDef_Node_AttrEntry_DoNotUse;
struct FunctionDef_Node_AttrEntry_DoNotUseDefaultTypeInternal;
extern FunctionDef_Node_AttrEntry_DoNotUseDefaultTypeInternal _FunctionDef_Node_AttrEntry_DoNotUse_default_instance_;
class GradientDef;
struct GradientDefDefaultTypeInternal;
extern GradientDefDefaultTypeInternal _GradientDef_default_instance_;
}",1,misc\tensorflow\function.pb.hpp,opencv_tensorflow,61,opencv_tensorflow,6
272166,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class FunctionDefLibrary final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:opencv_tensorflow.FunctionDefLibrary) */ {
 public:
  inline FunctionDefLibrary() : FunctionDefLibrary(nullptr) {}
  ~FunctionDefLibrary() override;
  explicit constexpr FunctionDefLibrary(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  FunctionDefLibrary(const FunctionDefLibrary& from);
  FunctionDefLibrary(FunctionDefLibrary&& from) noexcept
    : FunctionDefLibrary() {
    *this = ::std::move(from);
  }

  inline FunctionDefLibrary& operator=(const FunctionDefLibrary& from) {
    CopyFrom(from);
    return *this;
  }
  inline FunctionDefLibrary& operator=(FunctionDefLibrary&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nu...",1,misc\tensorflow\function.pb.hpp,opencv_tensorflow,85,opencv_tensorflow,14
275550,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\graph.pb.cc,misc\tensorflow\graph.pb.cc:<global>,,<global>,1
275555,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
constexpr GraphDef::GraphDef(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : node_()
  , library_(nullptr)
  , versions_(nullptr)
  , version_(0){}
struct GraphDefDefaultTypeInternal {
  constexpr GraphDefDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~GraphDefDefaultTypeInternal() {}
  union {
    GraphDef _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT GraphDefDefaultTypeInternal _GraphDef_default_instance_;
constexpr NodeDef_AttrEntry_DoNotUse::NodeDef_AttrEntry_DoNotUse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct NodeDef_AttrEntry_DoNotUseDefaultTypeInternal {
  constexpr NodeDef_AttrEntry_DoNotUseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~NodeDef_AttrEntry_DoNotUseDefaultTypeInternal() {}
  union {
    NodeDef_AttrEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PR...",1,misc\tensorflow\graph.pb.cc,opencv_tensorflow,19,opencv_tensorflow,2
275698,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class GraphDef::_Internal {
 public:
  static const ::opencv_tensorflow::VersionDef& versions(const GraphDef* msg);
  static const ::opencv_tensorflow::FunctionDefLibrary& library(const GraphDef* msg);
};

const ::opencv_tensorflow::VersionDef&
GraphDef::_Internal::versions(const GraphDef* msg) {
  return *msg->versions_;
}
const ::opencv_tensorflow::FunctionDefLibrary&
GraphDef::_Internal::library(const GraphDef* msg) {
  return *msg->library_;
}
void GraphDef::clear_versions() {
  if (GetArenaForAllocation() == nullptr && versions_ != nullptr) {
    delete versions_;
  }
  versions_ = nullptr;
}
void GraphDef::clear_library() {
  if (GetArenaForAllocation() == nullptr && library_ != nullptr) {
    delete library_;
  }
  library_ = nullptr;
}
GraphDef::GraphDef(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Messa...",1,misc\tensorflow\graph.pb.cc,opencv_tensorflow,145,opencv_tensorflow,24
278481,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\graph.pb.hpp,misc\tensorflow\graph.pb.hpp:<global>,,<global>,1
278486,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\tensorflow\graph.pb.hpp,internal,44,internal,2
278514,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
class GraphDef;
struct GraphDefDefaultTypeInternal;
extern GraphDefDefaultTypeInternal _GraphDef_default_instance_;
class NodeDef;
struct NodeDefDefaultTypeInternal;
extern NodeDefDefaultTypeInternal _NodeDef_default_instance_;
class NodeDef_AttrEntry_DoNotUse;
struct NodeDef_AttrEntry_DoNotUseDefaultTypeInternal;
extern NodeDef_AttrEntry_DoNotUseDefaultTypeInternal _NodeDef_AttrEntry_DoNotUse_default_instance_;
}",1,misc\tensorflow\graph.pb.hpp,opencv_tensorflow,62,opencv_tensorflow,6
278541,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class GraphDef final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:opencv_tensorflow.GraphDef) */ {
 public:
  inline GraphDef() : GraphDef(nullptr) {}
  ~GraphDef() override;
  explicit constexpr GraphDef(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GraphDef(const GraphDef& from);
  GraphDef(GraphDef&& from) noexcept
    : GraphDef() {
    *this = ::std::move(from);
  }

  inline GraphDef& operator=(const GraphDef& from) {
    CopyFrom(from);
    return *this;
  }
  inline GraphDef& operator=(GraphDef&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  s...",1,misc\tensorflow\graph.pb.hpp,opencv_tensorflow,78,opencv_tensorflow,12
280759,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\op_def.pb.cc,misc\tensorflow\op_def.pb.cc:<global>,,<global>,1
280764,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
constexpr OpDef_ArgDef::OpDef_ArgDef(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : name_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , description_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , type_attr_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , number_attr_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , type_list_attr_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , type_(0)

  , is_ref_(false){}
struct OpDef_ArgDefDefaultTypeInternal {
  constexpr OpDef_ArgDefDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~OpDef_ArgDefDefaultTypeInternal() {}
  union {
    OpDef_ArgDef _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT OpDef_ArgDefDefaultTypeInternal _OpDef_ArgDef_default_instance_;
constexpr OpDef_AttrDef::OpDef_AttrDef(
  ::PROTOBUF_NAMESPACE_ID::intern...",1,misc\tensorflow\op_def.pb.cc,opencv_tensorflow,19,opencv_tensorflow,2
280951,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class OpDef_ArgDef::_Internal {
 public:
};

OpDef_ArgDef::OpDef_ArgDef(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:opencv_tensorflow.OpDef.ArgDef)
}
OpDef_ArgDef::OpDef_ArgDef(const OpDef_ArgDef& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  name_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    name_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), """", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_...",1,misc\tensorflow\op_def.pb.cc,opencv_tensorflow,229,opencv_tensorflow,24
287974,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\op_def.pb.hpp,misc\tensorflow\op_def.pb.hpp:<global>,,<global>,1
287979,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\tensorflow\op_def.pb.hpp,internal,40,internal,2
288007,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
class OpDef;
struct OpDefDefaultTypeInternal;
extern OpDefDefaultTypeInternal _OpDef_default_instance_;
class OpDef_ArgDef;
struct OpDef_ArgDefDefaultTypeInternal;
extern OpDef_ArgDefDefaultTypeInternal _OpDef_ArgDef_default_instance_;
class OpDef_AttrDef;
struct OpDef_AttrDefDefaultTypeInternal;
extern OpDef_AttrDefDefaultTypeInternal _OpDef_AttrDef_default_instance_;
class OpDeprecation;
struct OpDeprecationDefaultTypeInternal;
extern OpDeprecationDefaultTypeInternal _OpDeprecation_default_instance_;
class OpList;
struct OpListDefaultTypeInternal;
extern OpListDefaultTypeInternal _OpList_default_instance_;
}",1,misc\tensorflow\op_def.pb.hpp,opencv_tensorflow,58,opencv_tensorflow,6
288050,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class OpDef_ArgDef final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:opencv_tensorflow.OpDef.ArgDef) */ {
 public:
  inline OpDef_ArgDef() : OpDef_ArgDef(nullptr) {}
  ~OpDef_ArgDef() override;
  explicit constexpr OpDef_ArgDef(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  OpDef_ArgDef(const OpDef_ArgDef& from);
  OpDef_ArgDef(OpDef_ArgDef&& from) noexcept
    : OpDef_ArgDef() {
    *this = ::std::move(from);
  }

  inline OpDef_ArgDef& operator=(const OpDef_ArgDef& from) {
    CopyFrom(from);
    return *this;
  }
  inline OpDef_ArgDef& operator=(OpDef_ArgDef&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } e...",1,misc\tensorflow\op_def.pb.hpp,opencv_tensorflow,82,opencv_tensorflow,14
293185,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\tensor.pb.cc,misc\tensorflow\tensor.pb.cc:<global>,,<global>,1
293190,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
constexpr TensorProto::TensorProto(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : float_val_()
  , double_val_()
  , int_val_()
  , _int_val_cached_byte_size_(0)
  , string_val_()
  , scomplex_val_()
  , int64_val_()
  , _int64_val_cached_byte_size_(0)
  , bool_val_()
  , dcomplex_val_()
  , half_val_()
  , _half_val_cached_byte_size_(0)
  , tensor_content_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , tensor_shape_(nullptr)
  , dtype_(0)

  , version_number_(0){}
struct TensorProtoDefaultTypeInternal {
  constexpr TensorProtoDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~TensorProtoDefaultTypeInternal() {}
  union {
    TensorProto _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT TensorProtoDefaultTypeInternal _TensorProto_default_instance_;
}",1,misc\tensorflow\tensor.pb.cc,opencv_tensorflow,19,opencv_tensorflow,2
293281,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class TensorProto::_Internal {
 public:
  static const ::opencv_tensorflow::TensorShapeProto& tensor_shape(const TensorProto* msg);
};

const ::opencv_tensorflow::TensorShapeProto&
TensorProto::_Internal::tensor_shape(const TensorProto* msg) {
  return *msg->tensor_shape_;
}
void TensorProto::clear_tensor_shape() {
  if (GetArenaForAllocation() == nullptr && tensor_shape_ != nullptr) {
    delete tensor_shape_;
  }
  tensor_shape_ = nullptr;
}
TensorProto::TensorProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  float_val_(arena),
  double_val_(arena),
  int_val_(arena),
  string_val_(arena),
  scomplex_val_(arena),
  int64_val_(arena),
  bool_val_(arena),
  dcomplex_val_(arena),
  half_val_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
...",1,misc\tensorflow\tensor.pb.cc,opencv_tensorflow,114,opencv_tensorflow,24
295693,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\tensor.pb.hpp,misc\tensorflow\tensor.pb.hpp:<global>,,<global>,1
295698,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\tensorflow\tensor.pb.hpp,internal,40,internal,2
295726,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
class TensorProto;
struct TensorProtoDefaultTypeInternal;
extern TensorProtoDefaultTypeInternal _TensorProto_default_instance_;
}",1,misc\tensorflow\tensor.pb.hpp,opencv_tensorflow,58,opencv_tensorflow,6
295737,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class TensorProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:opencv_tensorflow.TensorProto) */ {
 public:
  inline TensorProto() : TensorProto(nullptr) {}
  ~TensorProto() override;
  explicit constexpr TensorProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  TensorProto(const TensorProto& from);
  TensorProto(TensorProto&& from) noexcept
    : TensorProto() {
    *this = ::std::move(from);
  }

  inline TensorProto& operator=(const TensorProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline TensorProto& operator=(TensorProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      Cop...",1,misc\tensorflow\tensor.pb.hpp,opencv_tensorflow,66,opencv_tensorflow,10
298132,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\tensor_shape.pb.cc,misc\tensorflow\tensor_shape.pb.cc:<global>,,<global>,1
298137,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
constexpr TensorShapeProto_Dim::TensorShapeProto_Dim(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : name_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , size_(int64_t{0}){}
struct TensorShapeProto_DimDefaultTypeInternal {
  constexpr TensorShapeProto_DimDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~TensorShapeProto_DimDefaultTypeInternal() {}
  union {
    TensorShapeProto_Dim _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT TensorShapeProto_DimDefaultTypeInternal _TensorShapeProto_Dim_default_instance_;
constexpr TensorShapeProto::TensorShapeProto(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : dim_()
  , unknown_rank_(false){}
struct TensorShapeProtoDefaultTypeInternal {
  constexpr TensorShapeProtoDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~TensorShapeProtoDefaultTypeI...",1,misc\tensorflow\tensor_shape.pb.cc,opencv_tensorflow,19,opencv_tensorflow,2
298240,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class TensorShapeProto_Dim::_Internal {
 public:
};

TensorShapeProto_Dim::TensorShapeProto_Dim(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:opencv_tensorflow.TensorShapeProto.Dim)
}
TensorShapeProto_Dim::TensorShapeProto_Dim(const TensorShapeProto_Dim& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  name_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    name_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), """", GetArenaForAllocation());
  #endif // P...",1,misc\tensorflow\tensor_shape.pb.cc,opencv_tensorflow,100,opencv_tensorflow,22
299660,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\tensor_shape.pb.hpp,misc\tensorflow\tensor_shape.pb.hpp:<global>,,<global>,1
299665,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\tensorflow\tensor_shape.pb.hpp,internal,38,internal,2
299693,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
class TensorShapeProto;
struct TensorShapeProtoDefaultTypeInternal;
extern TensorShapeProtoDefaultTypeInternal _TensorShapeProto_default_instance_;
class TensorShapeProto_Dim;
struct TensorShapeProto_DimDefaultTypeInternal;
extern TensorShapeProto_DimDefaultTypeInternal _TensorShapeProto_Dim_default_instance_;
}",1,misc\tensorflow\tensor_shape.pb.hpp,opencv_tensorflow,56,opencv_tensorflow,6
299712,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class TensorShapeProto_Dim final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:opencv_tensorflow.TensorShapeProto.Dim) */ {
 public:
  inline TensorShapeProto_Dim() : TensorShapeProto_Dim(nullptr) {}
  ~TensorShapeProto_Dim() override;
  explicit constexpr TensorShapeProto_Dim(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  TensorShapeProto_Dim(const TensorShapeProto_Dim& from);
  TensorShapeProto_Dim(TensorShapeProto_Dim&& from) noexcept
    : TensorShapeProto_Dim() {
    *this = ::std::move(from);
  }

  inline TensorShapeProto_Dim& operator=(const TensorShapeProto_Dim& from) {
    CopyFrom(from);
    return *this;
  }
  inline TensorShapeProto_Dim& operator=(TensorShapeProto_Dim&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
   ...",1,misc\tensorflow\tensor_shape.pb.hpp,opencv_tensorflow,68,opencv_tensorflow,11
300751,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\types.pb.cc,misc\tensorflow\types.pb.cc:<global>,,<global>,1
300756,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
}",1,misc\tensorflow\types.pb.cc,opencv_tensorflow,19,opencv_tensorflow,2
300813,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* DataType_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_types_2eproto);
  return file_level_enum_descriptors_types_2eproto[0];
}
bool DataType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
    case 8:
    case 9:
    case 10:
    case 11:
    case 12:
    case 13:
    case 14:
    case 15:
    case 16:
    case 17:
    case 18:
    case 19:
    case 101:
    case 102:
    case 103:
    case 104:
    case 105:
    case 106:
    case 107:
    case 108:
    case 109:
    case 110:
    case 111:
    case 112:
    case 113:
    case 114:
    case 115:
    case 116:
    case 117:
    case 118:
    case 119:
      return true;
    default:
      return false;
  }
}


// @@protoc_insertion_point(namespace_scope)
}",1,misc\tensorflow\types.pb.cc,opencv_tensorflow,61,opencv_tensorflow,22
300955,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\types.pb.hpp,misc\tensorflow\types.pb.hpp:<global>,,<global>,1
300960,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\tensorflow\types.pb.hpp,internal,37,internal,2
300990,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

enum DataType : int {
  DT_INVALID = 0,
  DT_FLOAT = 1,
  DT_DOUBLE = 2,
  DT_INT32 = 3,
  DT_UINT8 = 4,
  DT_INT16 = 5,
  DT_INT8 = 6,
  DT_STRING = 7,
  DT_COMPLEX64 = 8,
  DT_INT64 = 9,
  DT_BOOL = 10,
  DT_QINT8 = 11,
  DT_QUINT8 = 12,
  DT_QINT32 = 13,
  DT_BFLOAT16 = 14,
  DT_QINT16 = 15,
  DT_QUINT16 = 16,
  DT_UINT16 = 17,
  DT_COMPLEX128 = 18,
  DT_HALF = 19,
  DT_FLOAT_REF = 101,
  DT_DOUBLE_REF = 102,
  DT_INT32_REF = 103,
  DT_UINT8_REF = 104,
  DT_INT16_REF = 105,
  DT_INT8_REF = 106,
  DT_STRING_REF = 107,
  DT_COMPLEX64_REF = 108,
  DT_INT64_REF = 109,
  DT_BOOL_REF = 110,
  DT_QINT8_REF = 111,
  DT_QUINT8_REF = 112,
  DT_QINT32_REF = 113,
  DT_BFLOAT16_REF = 114,
  DT_QINT16_REF = 115,
  DT_QUINT16_REF = 116,
  DT_UINT16_REF = 117,
  DT_COMPLEX128_REF = 118,
  DT_HALF_REF = 119,
  DataType_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  DataType_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
...",1,misc\tensorflow\types.pb.hpp,opencv_tensorflow,57,opencv_tensorflow,8
301287,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\versions.pb.cc,misc\tensorflow\versions.pb.cc:<global>,,<global>,1
301292,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
constexpr VersionDef::VersionDef(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : bad_consumers_()
  , _bad_consumers_cached_byte_size_(0)
  , producer_(0)
  , min_consumer_(0){}
struct VersionDefDefaultTypeInternal {
  constexpr VersionDefDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~VersionDefDefaultTypeInternal() {}
  union {
    VersionDef _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT VersionDefDefaultTypeInternal _VersionDef_default_instance_;
}",1,misc\tensorflow\versions.pb.cc,opencv_tensorflow,19,opencv_tensorflow,2
301371,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class VersionDef::_Internal {
 public:
};

VersionDef::VersionDef(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  bad_consumers_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:opencv_tensorflow.VersionDef)
}
VersionDef::VersionDef(const VersionDef& from)
  : ::PROTOBUF_NAMESPACE_ID::Message(),
      bad_consumers_(from.bad_consumers_) {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  ::memcpy(&producer_, &from.producer_,
    static_cast<size_t>(reinterpret_cast<char*>(&min_consumer_) -
    reinterpret_cast<char*>(&producer_)) + sizeof(min_consumer_));
  // @@protoc_insertion_point(copy_constructor:opencv_tensorflow.VersionDef)
}

inline voi...",1,misc\tensorflow\versions.pb.cc,opencv_tensorflow,79,opencv_tensorflow,22
302263,NAMESPACE_BLOCK,<empty>,,misc\tensorflow\versions.pb.hpp,misc\tensorflow\versions.pb.hpp:<global>,,<global>,1
302268,NAMESPACE_BLOCK,"namespace internal {
class AnyMetadata;
}",1,misc\tensorflow\versions.pb.hpp,internal,38,internal,2
302296,NAMESPACE_BLOCK,"namespace opencv_tensorflow {
class VersionDef;
struct VersionDefDefaultTypeInternal;
extern VersionDefDefaultTypeInternal _VersionDef_default_instance_;
}",1,misc\tensorflow\versions.pb.hpp,opencv_tensorflow,56,opencv_tensorflow,6
302307,NAMESPACE_BLOCK,"namespace opencv_tensorflow {

// ===================================================================

class VersionDef final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:opencv_tensorflow.VersionDef) */ {
 public:
  inline VersionDef() : VersionDef(nullptr) {}
  ~VersionDef() override;
  explicit constexpr VersionDef(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  VersionDef(const VersionDef& from);
  VersionDef(VersionDef&& from) noexcept
    : VersionDef() {
    *this = ::std::move(from);
  }

  inline VersionDef& operator=(const VersionDef& from) {
    CopyFrom(from);
    return *this;
  }
  inline VersionDef& operator=(VersionDef&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
  ...",1,misc\tensorflow\versions.pb.hpp,opencv_tensorflow,64,opencv_tensorflow,10
302896,NAMESPACE_BLOCK,<empty>,,misc\tflite\schema_generated.hpp,misc\tflite\schema_generated.hpp:<global>,,<global>,1
302913,NAMESPACE_BLOCK,"namespace opencv_tflite {

struct CustomQuantization;
struct CustomQuantizationBuilder;

struct QuantizationParameters;
struct QuantizationParametersBuilder;

struct Int32Vector;
struct Int32VectorBuilder;

struct Uint16Vector;
struct Uint16VectorBuilder;

struct Uint8Vector;
struct Uint8VectorBuilder;

struct DimensionMetadata;
struct DimensionMetadataBuilder;

struct SparsityParameters;
struct SparsityParametersBuilder;

struct VariantSubType;
struct VariantSubTypeBuilder;

struct Tensor;
struct TensorBuilder;

struct Conv2DOptions;
struct Conv2DOptionsBuilder;

struct Conv3DOptions;
struct Conv3DOptionsBuilder;

struct Pool2DOptions;
struct Pool2DOptionsBuilder;

struct DepthwiseConv2DOptions;
struct DepthwiseConv2DOptionsBuilder;

struct ConcatEmbeddingsOptions;
struct ConcatEmbeddingsOptionsBuilder;

struct LSHProjectionOptions;
struct LSHProjectionOptionsBuilder;

struct SVDFOptions;
struct SVDFOptionsBuilder;

struct RNNOptions;
struct RNNOptionsBuilder;

struct SequenceRNNOp...",1,misc\tflite\schema_generated.hpp,opencv_tflite,16,opencv_tflite,2
323769,NAMESPACE_BLOCK,<empty>,,perf\perf_caffe.cpp,perf\perf_caffe.cpp:<global>,,<global>,1
323778,NAMESPACE_BLOCK,<empty>,,perf\perf_common.cpp,perf\perf_common.cpp:<global>,,<global>,1
323789,NAMESPACE_BLOCK,<empty>,,perf\perf_convolution.cpp,perf\perf_convolution.cpp:<global>,,<global>,1
323793,NAMESPACE_BLOCK,"namespace opencv_test {

// Flops_Kernel_Input_OutCN_Group_Stride_Pad_Dilation_PadAdjust_PadMode_Bias
struct TestSize_ {
    int width, height;
    operator Size() const { return Size(width, height); }
};
struct ConvParam_t {
    struct TestSize_ kernel;
    struct BlobShape { int dims[4]; } shapeIn;
    int outCN;
    int groups;
    struct TestSize_ stride;
    struct TestSize_ dilation;
    struct TestSize_ pad;
    struct TestSize_ padAdjust;
    const char* padMode;
    bool hasBias;
    double declared_flops;
};
// Details: #12142
// Last update: 2023-11
// Extended and classified: #24547
static const ConvParam_t testConvolution_Configs[] = {
    /* GFLOPS 3.398 x 20 = 67.956 */ {{7, 7}, {{1, 128, 46, 46}}, 128, 1, {1, 1}, {1, 1}, {3, 3}, {0, 0}, """", true, 3397788160.},
    /* GFLOPS 16.987 x 3 = 50.962 */ {{5, 5}, {{1, 1152, 16, 16}}, 1152, 1, {1, 1}, {1, 1}, {0, 0}, {0, 0}, ""SAME"", false, 16987226112.},
    /* GFLOPS 23.122 x 2 = 46.244 */ {{5, 5}, {{1, 672, 32, 32}}, 672, 1...",1,perf\perf_convolution.cpp,opencv_test,9,opencv_test,1
344179,NAMESPACE_BLOCK,<empty>,,perf\perf_convolution1d.cpp,perf\perf_convolution1d.cpp:<global>,,<global>,1
344183,NAMESPACE_BLOCK,"namespace opencv_test {

struct Conv1DParam_t {
    int kernel;
    struct BlobShape { int dims[3]; } shapeIn;
    int outCN;
    int groups;
    int stride;
    int dilation;
    int pad[2];
    const char* padMode;
    bool hasBias;
    double declared_flops;
};
// Details: #12142
static const Conv1DParam_t testConvolution1DConfigs[] = {
        {3, {{1, 6, 10}}, 6, 1, 1, 1, {0, 0}, ""VALID"", true, 1776.},
        {3, {{1, 2, 19}}, 2, 2, 2, 1, {1, 1}, """", true, 260.},
        {3, {{1, 2, 25}}, 2, 2, 1, 1, {2, 2}, ""SAME"", false, 650.},
};

struct Conv1DParamID
{
    enum {
        CONV_0 = 0,
        CONV_LAST = sizeof(testConvolution1DConfigs) / sizeof(testConvolution1DConfigs[0])
    };
    int val_;
    Conv1DParamID(int val = 0) : val_(val) {}
    operator int() const { return val_; }
    static ::testing::internal::ParamGenerator<Conv1DParamID> all()
    {
        enum { NUM = (int)CONV_LAST };
        Conv1DParamID v_[NUM]; for (int i = 0; i < NUM; ++i) { v_[i] = Conv1DParamID...",1,perf\perf_convolution1d.cpp,opencv_test,8,opencv_test,1
344538,NAMESPACE_BLOCK,<empty>,,perf\perf_convolution3d.cpp,perf\perf_convolution3d.cpp:<global>,,<global>,1
344542,NAMESPACE_BLOCK,"namespace opencv_test {

struct Conv3DParam_t {
    int kernel[3];
    struct BlobShape { int dims[5]; } shapeIn;
    int outCN;
    int groups;
    int stride[3];
    int dilation[3];
    int pad[6];
    const char* padMode;
    bool hasBias;
    double declared_flops;
};
// Details: #12142
static const Conv3DParam_t testConvolution3DConfigs[] = {
    {{3, 3, 3}, {{1, 6, 10, 38, 50}}, 6, 1, {1, 1, 1}, {1, 1, 1}, {0, 0, 0, 0, 0, 0}, ""VALID"", true, 26956800.},
    {{3, 3, 3}, {{1, 2, 19, 19, 19}}, 2, 2, {2, 2, 2}, {1, 1, 1}, {1, 1, 1, 1, 1, 1}, """", true, 218000.},
    {{3, 3, 3}, {{1, 2, 25, 19, 19}}, 2, 2, {1, 2, 2}, {1, 1, 1}, {2, 2, 2, 2, 2, 2}, ""SAME"", false, 545000.},
    {{3, 3, 3}, {{1, 11, 9, 150, 200}}, 11, 1, {1, 1, 1}, {1, 1, 1}, {0, 0, 0, 0, 0, 0}, ""VALID"", true, 1342562760.},
    {{3, 3, 3}, {{1, 10, 98, 10, 10}}, 10, 1, {1, 1, 1}, {1, 1, 1}, {1, 0, 1, 1, 0,1}, ""SAME"", false, 53018000.},
    {{5, 5, 5}, {{1, 6, 19, 19, 19}}, 6, 2, {1, 1, 1}, {1, 1, 1}, {0, 0, 0, 0, 0, 0}...",1,perf\perf_convolution3d.cpp,opencv_test,8,opencv_test,1
345536,NAMESPACE_BLOCK,<empty>,,perf\perf_einsum.cpp,perf\perf_einsum.cpp:<global>,,<global>,1
345540,NAMESPACE_BLOCK,"namespace opencv_test {

struct EinsumParams {
    int inputSize;
    int outputSize;
    std::string equation;
    std::vector<MatShape> einsumInpShapes;
    EinsumParams(std::string equation_, std::vector<MatShape> einsumInpShapes_ = std::vector<MatShape>())
    {
        inputSize = einsumInpShapes_.size();
        equation = equation_;
        einsumInpShapes = einsumInpShapes_;
    }
};

static inline void PrintTo(const EinsumParams& params, ::std::ostream* os) {
     (*os) << ""Equation="" << params.equation << "" "";

        (*os) << ""InputShape={"";
        for(int i = 0; i < params.einsumInpShapes.size(); i++)
        {
            (*os) << ""{"";
            for(int j = 0; j < params.einsumInpShapes[i].size(); j++)
            {
                (*os) << params.einsumInpShapes[i][j] << ((j < params.einsumInpShapes[i].size() - 1) ?  "", "" : """");
            }
            (*os) << ((i < params.einsumInpShapes.size() - 1) ? ""}, "" : ""}"");
        }
        (*os) << ""}"";
}

// test cas...",1,perf\perf_einsum.cpp,opencv_test,7,opencv_test,1
345919,NAMESPACE_BLOCK,<empty>,,perf\perf_gemm.cpp,perf\perf_gemm.cpp:<global>,,<global>,1
345922,NAMESPACE_BLOCK,"namespace opencv_test {

struct GemmParam_t {
    std::vector<int> a_shape;
    std::vector<int> b_shape;
    std::vector<int> c_shape;
    bool trans_a;
    bool trans_b;

    GemmParam_t(std::vector<int> a_shape_, std::vector<int> b_shape_, std::vector<int> c_shape_ = {}, bool trans_a_ = false, bool trans_b_ = false)
        : a_shape(a_shape_), b_shape(b_shape_), c_shape(c_shape_), trans_a(trans_a_), trans_b(trans_b_) {}
};

// TODO: Dsiable most of the test cases except vision transformers to save time
static const GemmParam_t test_gemm_configs[] = {
    // vision transformers cases
    { {  768,  768 }, {  768,  768 }, {  768 } },
    { { 1024, 1024 }, { 1024, 1024 }, { 1024 } },
    { {   50,  768 }, {  768, 2304 } },
    { {  197,  768 }, {  768, 2304 } },
    { {   50, 1024 }, { 1024, 3072 } },
    { {  197, 1024 }, { 1024, 3072 } },

// these cases are commented to save testing time
/*
    // square mat
    { {   64,   64 }, {   64,   64 } },
    { {  128,  128 }, {  128,  ...",1,perf\perf_gemm.cpp,opencv_test,10,opencv_test,1
346327,NAMESPACE_BLOCK,<empty>,,perf\perf_layer.cpp,perf\perf_layer.cpp:<global>,,<global>,1
346331,NAMESPACE_BLOCK,"namespace opencv_test {

struct Layer_Slice : public TestBaseWithParam<tuple<Backend, Target> >
{
    template<int DIMS>
    void test_slice(const int* inputShape, const int* begin, const int* end)
    {
        int backendId = get<0>(GetParam());
        int targetId = get<1>(GetParam());

        Mat input(DIMS, inputShape, CV_32FC1, Scalar::all(0));
        for (int i = 0; i < (int)input.total(); ++i)
            input.ptr<float>()[i] = (float)(i & 4095);

        std::vector<Range> range(DIMS);
        for (int i = 0; i < DIMS; ++i)
            range[i] = Range(begin[i], end[i]);

        Net net;
        LayerParams lp;
        lp.type = ""Slice"";
        lp.name = ""testLayer"";
        lp.set(""begin"", DictValue::arrayInt<int*>((int*)&begin[0], DIMS));
        lp.set(""end"", DictValue::arrayInt<int*>((int*)&end[0], DIMS));
        net.addLayerToPrev(lp.name, lp.type, lp);

        // warmup
        {
            net.setInput(input);
            net.setPreferableBackend(backendId);...",1,perf\perf_layer.cpp,opencv_test,8,opencv_test,1
348968,NAMESPACE_BLOCK,<empty>,,perf\perf_main.cpp,perf\perf_main.cpp:<global>,,<global>,1
348987,NAMESPACE_BLOCK,<empty>,,perf\perf_net.cpp,perf\perf_net.cpp:<global>,,<global>,1
348991,NAMESPACE_BLOCK,"namespace opencv_test {

class DNNTestNetwork : public ::perf::TestBaseWithParam< tuple<Backend, Target> >
{
public:
    dnn::Backend backend;
    dnn::Target target;

    dnn::Net net;

    DNNTestNetwork()
    {
        backend = (dnn::Backend)(int)get<0>(GetParam());
        target = (dnn::Target)(int)get<1>(GetParam());
    }

    void processNet(std::string weights, std::string proto, std::string halide_scheduler,
                    const std::vector<std::tuple<Mat, std::string>>& inputs, const std::string& outputLayer = """"){
        weights = findDataFile(weights, false);
        if (!proto.empty())
            proto = findDataFile(proto);
        if (backend == DNN_BACKEND_HALIDE)
        {
            if (halide_scheduler == ""disabled"")
                throw cvtest::SkipTestException(""Halide test is disabled"");
            if (!halide_scheduler.empty())
                halide_scheduler = findDataFile(std::string(""dnn/halide_scheduler_"") + (target == DNN_TARGET_OPENCL ? ""ope...",1,perf\perf_net.cpp,opencv_test,15,opencv_test,1
349355,NAMESPACE_BLOCK,<empty>,,perf\perf_precomp.hpp,perf\perf_precomp.hpp:<global>,,<global>,1
349359,NAMESPACE_BLOCK,"namespace opencv_test {
using namespace perf;
using namespace cv::dnn;
}",1,perf\perf_precomp.hpp,opencv_test,9,opencv_test,1
349363,NAMESPACE_BLOCK,<empty>,,perf\perf_recurrent.cpp,perf\perf_recurrent.cpp:<global>,,<global>,1
349367,NAMESPACE_BLOCK,"namespace opencv_test {

struct LstmParams {
    // Batch size
    int nrSamples;

    // Size of the input vector
    int inputSize;

    // Size of the internal state vector
    int hiddenSize;

    // Number of timesteps for the LSTM
    int nrSteps;
};

static inline void PrintTo(const LstmParams& params, ::std::ostream* os) {
    (*os) << ""BATCH="" << params.nrSamples
        << "", IN="" << params.inputSize
        << "", HIDDEN="" << params.hiddenSize
        << "", TS="" << params.nrSteps;
}

static const LstmParams testLstmConfigs[] = {
    {1, 192, 192, 100},
    {1, 1024, 192, 100},
    {1, 64, 192, 100},
    {1, 192, 512, 100},
    {64, 192, 192, 2},
    {64, 1024, 192, 2},
    {64, 64, 192, 2},
    {64, 192, 512, 2},
    {128, 192, 192, 2},
    {128, 1024, 192, 2},
    {128, 64, 192, 2},
    {128, 192, 512, 2}
};

class Layer_LSTM : public TestBaseWithParam<LstmParams> {};

PERF_TEST_P_(Layer_LSTM, lstm) {
    const LstmParams& params = GetParam();
    LayerParams lp;
    lp.t...",1,perf\perf_recurrent.cpp,opencv_test,7,opencv_test,1
349491,NAMESPACE_BLOCK,<empty>,,src\backend.cpp,src\backend.cpp:<global>,,<global>,1
349495,NAMESPACE_BLOCK,"namespace cv { namespace dnn_backend {

NetworkBackend::~NetworkBackend()
{
    // nothing
}

}}",1,src\backend.cpp,cv,24,cv,1
349496,NAMESPACE_BLOCK,"namespace dnn_backend {

NetworkBackend::~NetworkBackend()
{
    // nothing
}

}",16,src\backend.cpp,cv.dnn_backend,24,dnn_backend,1
349506,NAMESPACE_BLOCK,<empty>,,src\backend.hpp,src\backend.hpp:<global>,,<global>,1
349510,NAMESPACE_BLOCK,"namespace cv { namespace dnn_backend {

using namespace cv::dnn;

class CV_EXPORTS NetworkBackend
{
public:
    virtual ~NetworkBackend();

    virtual void switchBackend(Net& net) = 0;

    /**
    @param loaderID use empty """" for auto
    @param model see cv::dnn::readNetwork
    @param config see cv::dnn::readNetwork
    */
    virtual Net readNetwork(const std::string& loaderID, const std::string& model, const std::string& config) = 0;

    /** @overload */
    virtual Net readNetwork(
        const std::string& loaderID,
        const uchar* bufferModelConfigPtr, size_t bufferModelConfigSize,
        const uchar* bufferWeightsPtr, size_t bufferWeightsSize
    ) = 0;

    // TODO: target as string + configuration
    virtual bool checkTarget(Target target) = 0;
};


}  // namespace dnn_backend
}",1,src\backend.hpp,cv,10,cv,1
349511,NAMESPACE_BLOCK,"namespace dnn_backend {

using namespace cv::dnn;

class CV_EXPORTS NetworkBackend
{
public:
    virtual ~NetworkBackend();

    virtual void switchBackend(Net& net) = 0;

    /**
    @param loaderID use empty """" for auto
    @param model see cv::dnn::readNetwork
    @param config see cv::dnn::readNetwork
    */
    virtual Net readNetwork(const std::string& loaderID, const std::string& model, const std::string& config) = 0;

    /** @overload */
    virtual Net readNetwork(
        const std::string& loaderID,
        const uchar* bufferModelConfigPtr, size_t bufferModelConfigSize,
        const uchar* bufferWeightsPtr, size_t bufferWeightsSize
    ) = 0;

    // TODO: target as string + configuration
    virtual bool checkTarget(Target target) = 0;
};


}",16,src\backend.hpp,cv.dnn_backend,10,dnn_backend,1
349536,NAMESPACE_BLOCK,<empty>,,src\caffe\caffe_importer.cpp,src\caffe\caffe_importer.cpp:<global>,,<global>,1
349540,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_PROTOBUF
using ::google::protobuf::RepeatedFieldRef;
using ::google::protobuf::Message;
using ::google::protobuf::Descriptor;
using ::google::protobuf::FieldDescriptor;
using ::google::protobuf::Reflection;

namespace
{

template<typename T>
static cv::String toString(const T &v)
{
    std::ostringstream ss;
    ss << v;
    return ss.str();
}

static inline
MatShape parseBlobShape(const caffe::BlobShape& _input_shape)
{
    MatShape shape;
    for (int i = 0; i < _input_shape.dim_size(); i++)
    {
        shape.push_back((int)_input_shape.dim(i));
    }
    return shape;
}

class CaffeImporter
{
    FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;

    caffe::NetParameter net;
    caffe::NetParameter netBinary;

public:

    CaffeImporter(const char *prototxt, const char *caffeModel)
    {
        CV_TRACE_FUNCTION();

        ReadNetParamsFromTextFileOrDie(prototxt, &net);

        if (caffeModel && caffeMod...",1,src\caffe\caffe_importer.cpp,cv,58,cv,1
349541,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_PROTOBUF
using ::google::protobuf::RepeatedFieldRef;
using ::google::protobuf::Message;
using ::google::protobuf::Descriptor;
using ::google::protobuf::FieldDescriptor;
using ::google::protobuf::Reflection;

namespace
{

template<typename T>
static cv::String toString(const T &v)
{
    std::ostringstream ss;
    ss << v;
    return ss.str();
}

static inline
MatShape parseBlobShape(const caffe::BlobShape& _input_shape)
{
    MatShape shape;
    for (int i = 0; i < _input_shape.dim_size(); i++)
    {
        shape.push_back((int)_input_shape.dim(i));
    }
    return shape;
}

class CaffeImporter
{
    FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;

    caffe::NetParameter net;
    caffe::NetParameter netBinary;

public:

    CaffeImporter(const char *prototxt, const char *caffeModel)
    {
        CV_TRACE_FUNCTION();

        ReadNetParamsFromTextFileOrDie(prototxt, &net);

        if (caffeModel && caffeModel[0])
        ...",1,src\caffe\caffe_importer.cpp,cv.dnn,59,dnn,1
349610,NAMESPACE_BLOCK,<empty>,,src\caffe\caffe_io.cpp,src\caffe\caffe_io.cpp:<global>,,<global>,1
349617,NAMESPACE_BLOCK,<empty>,,src\caffe\caffe_io.hpp,src\caffe\caffe_io.hpp:<global>,,<global>,1
349628,NAMESPACE_BLOCK,<empty>,,src\caffe\caffe_shrinker.cpp,src\caffe\caffe_shrinker.cpp:<global>,,<global>,1
349632,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_PROTOBUF

void shrinkCaffeModel(const String& src, const String& dst, const std::vector<String>& layersTypes)
{
    CV_TRACE_FUNCTION();

    std::vector<String> types(layersTypes);
    if (types.empty())
    {
        types.push_back(""Convolution"");
        types.push_back(""InnerProduct"");
    }

    caffe::NetParameter net;
    ReadNetParamsFromBinaryFileOrDie(src.c_str(), &net);

    for (int i = 0; i < net.layer_size(); ++i)
    {
        caffe::LayerParameter* lp = net.mutable_layer(i);
        if (std::find(types.begin(), types.end(), lp->type()) == types.end())
        {
            continue;
        }
        for (int j = 0; j < lp->blobs_size(); ++j)
        {
            caffe::BlobProto* blob = lp->mutable_blobs(j);
            CV_Assert(blob->data_size() != 0);  // float32 array.

            Mat floats(1, blob->data_size(), CV_32FC1, (void*)blob->data().data());
            Mat halfs(1, blob->data_size(...",1,src\caffe\caffe_shrinker.cpp,cv,15,cv,1
349633,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_PROTOBUF

void shrinkCaffeModel(const String& src, const String& dst, const std::vector<String>& layersTypes)
{
    CV_TRACE_FUNCTION();

    std::vector<String> types(layersTypes);
    if (types.empty())
    {
        types.push_back(""Convolution"");
        types.push_back(""InnerProduct"");
    }

    caffe::NetParameter net;
    ReadNetParamsFromBinaryFileOrDie(src.c_str(), &net);

    for (int i = 0; i < net.layer_size(); ++i)
    {
        caffe::LayerParameter* lp = net.mutable_layer(i);
        if (std::find(types.begin(), types.end(), lp->type()) == types.end())
        {
            continue;
        }
        for (int j = 0; j < lp->blobs_size(); ++j)
        {
            caffe::BlobProto* blob = lp->mutable_blobs(j);
            CV_Assert(blob->data_size() != 0);  // float32 array.

            Mat floats(1, blob->data_size(), CV_32FC1, (void*)blob->data().data());
            Mat halfs(1, blob->data_size(), CV_16SC1);
 ...",16,src\caffe\caffe_shrinker.cpp,cv.dnn,15,dnn,1
349659,NAMESPACE_BLOCK,<empty>,,src\caffe\glog_emulator.hpp,src\caffe\glog_emulator.hpp:<global>,,<global>,1
349663,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class GLogWrapper
{
    const char *file, *func, *type, *cond_str;
    int line;
    bool cond_status, exit_loop;
    std::stringstream sstream;

public:

    GLogWrapper(const char *_file, const char *_func, int _line,
          const char *_type,
          const char *_cond_str = NULL, bool _cond_status = true
    ) :
        file(_file), func(_func), type(_type), cond_str(_cond_str),
        line(_line), cond_status(_cond_status), exit_loop(true) {}

    std::iostream &stream()
    {
        return sstream;
    }

    bool exit()
    {
        return exit_loop;
    }

    void check()
    {
        exit_loop = false;

        if (cond_str && !cond_status)
        {
            cv::error(cv::Error::StsError, ""FAILED: "" + String(cond_str) + "". "" + sstream.str(), func, file, line);
        }
        else if (!cond_str && strcmp(type, ""CHECK""))
        {
            #ifndef NDEBUG
            if (!std::strcmp(type, ""INFO""))
                std::cout <<...",1,src\caffe\glog_emulator.hpp,cv,53,cv,1
349664,NAMESPACE_BLOCK,"namespace dnn
{

class GLogWrapper
{
    const char *file, *func, *type, *cond_str;
    int line;
    bool cond_status, exit_loop;
    std::stringstream sstream;

public:

    GLogWrapper(const char *_file, const char *_func, int _line,
          const char *_type,
          const char *_cond_str = NULL, bool _cond_status = true
    ) :
        file(_file), func(_func), type(_type), cond_str(_cond_str),
        line(_line), cond_status(_cond_status), exit_loop(true) {}

    std::iostream &stream()
    {
        return sstream;
    }

    bool exit()
    {
        return exit_loop;
    }

    void check()
    {
        exit_loop = false;

        if (cond_str && !cond_status)
        {
            cv::error(cv::Error::StsError, ""FAILED: "" + String(cond_str) + "". "" + sstream.str(), func, file, line);
        }
        else if (!cond_str && strcmp(type, ""CHECK""))
        {
            #ifndef NDEBUG
            if (!std::strcmp(type, ""INFO""))
                std::cout << sstream.str() ...",1,src\caffe\glog_emulator.hpp,cv.dnn,55,dnn,1
349848,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cublas.hpp,src\cuda4dnn\csl\cublas.hpp:<global>,,<global>,1
349851,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cublas {

    /** @brief exception class for errors thrown by the cuBLAS API */
    class cuBLASException : public CUDAException {
    public:
        using CUDAException::CUDAException;
    };

    namespace detail {
        static void check(cublasStatus_t status, const char* func, const char* file, int line) {
            auto cublasGetErrorString = [](cublasStatus_t err) {
                switch (err) {
                case CUBLAS_STATUS_SUCCESS: return ""CUBLAS_STATUS_SUCCESS"";
                case CUBLAS_STATUS_NOT_INITIALIZED: return ""CUBLAS_STATUS_NOT_INITIALIZED"";
                case CUBLAS_STATUS_ALLOC_FAILED: return ""CUBLAS_STATUS_ALLOC_FAILED"";
                case CUBLAS_STATUS_INVALID_VALUE: return ""CUBLAS_STATUS_INVALID_VALUE"";
                case CUBLAS_STATUS_ARCH_MISMATCH: return ""CUBLAS_STATUS_ARCH_MISMATCH"";
                case CUBLAS_STATUS_MAPPING_ERROR: return ""CUBLAS_STATUS_MAPPIN...",1,src\cuda4dnn\csl\cublas.hpp,cv,24,cv,1
349852,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cublas {

    /** @brief exception class for errors thrown by the cuBLAS API */
    class cuBLASException : public CUDAException {
    public:
        using CUDAException::CUDAException;
    };

    namespace detail {
        static void check(cublasStatus_t status, const char* func, const char* file, int line) {
            auto cublasGetErrorString = [](cublasStatus_t err) {
                switch (err) {
                case CUBLAS_STATUS_SUCCESS: return ""CUBLAS_STATUS_SUCCESS"";
                case CUBLAS_STATUS_NOT_INITIALIZED: return ""CUBLAS_STATUS_NOT_INITIALIZED"";
                case CUBLAS_STATUS_ALLOC_FAILED: return ""CUBLAS_STATUS_ALLOC_FAILED"";
                case CUBLAS_STATUS_INVALID_VALUE: return ""CUBLAS_STATUS_INVALID_VALUE"";
                case CUBLAS_STATUS_ARCH_MISMATCH: return ""CUBLAS_STATUS_ARCH_MISMATCH"";
                case CUBLAS_STATUS_MAPPING_ERROR: return ""CUBLAS_STATUS_MAPPING_ERROR"";
     ...",16,src\cuda4dnn\csl\cublas.hpp,cv.dnn,24,dnn,1
349853,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cublas {

    /** @brief exception class for errors thrown by the cuBLAS API */
    class cuBLASException : public CUDAException {
    public:
        using CUDAException::CUDAException;
    };

    namespace detail {
        static void check(cublasStatus_t status, const char* func, const char* file, int line) {
            auto cublasGetErrorString = [](cublasStatus_t err) {
                switch (err) {
                case CUBLAS_STATUS_SUCCESS: return ""CUBLAS_STATUS_SUCCESS"";
                case CUBLAS_STATUS_NOT_INITIALIZED: return ""CUBLAS_STATUS_NOT_INITIALIZED"";
                case CUBLAS_STATUS_ALLOC_FAILED: return ""CUBLAS_STATUS_ALLOC_FAILED"";
                case CUBLAS_STATUS_INVALID_VALUE: return ""CUBLAS_STATUS_INVALID_VALUE"";
                case CUBLAS_STATUS_ARCH_MISMATCH: return ""CUBLAS_STATUS_ARCH_MISMATCH"";
                case CUBLAS_STATUS_MAPPING_ERROR: return ""CUBLAS_STATUS_MAPPING_ERROR"";
                case ...",32,src\cuda4dnn\csl\cublas.hpp,cv.dnn.cuda4dnn,24,cuda4dnn,1
349854,NAMESPACE_BLOCK,"namespace csl { namespace cublas {

    /** @brief exception class for errors thrown by the cuBLAS API */
    class cuBLASException : public CUDAException {
    public:
        using CUDAException::CUDAException;
    };

    namespace detail {
        static void check(cublasStatus_t status, const char* func, const char* file, int line) {
            auto cublasGetErrorString = [](cublasStatus_t err) {
                switch (err) {
                case CUBLAS_STATUS_SUCCESS: return ""CUBLAS_STATUS_SUCCESS"";
                case CUBLAS_STATUS_NOT_INITIALIZED: return ""CUBLAS_STATUS_NOT_INITIALIZED"";
                case CUBLAS_STATUS_ALLOC_FAILED: return ""CUBLAS_STATUS_ALLOC_FAILED"";
                case CUBLAS_STATUS_INVALID_VALUE: return ""CUBLAS_STATUS_INVALID_VALUE"";
                case CUBLAS_STATUS_ARCH_MISMATCH: return ""CUBLAS_STATUS_ARCH_MISMATCH"";
                case CUBLAS_STATUS_MAPPING_ERROR: return ""CUBLAS_STATUS_MAPPING_ERROR"";
                case CUBLAS_STATUS_EXECUTI...",53,src\cuda4dnn\csl\cublas.hpp,cv.dnn.cuda4dnn.csl,24,csl,1
349855,NAMESPACE_BLOCK,"namespace cublas {

    /** @brief exception class for errors thrown by the cuBLAS API */
    class cuBLASException : public CUDAException {
    public:
        using CUDAException::CUDAException;
    };

    namespace detail {
        static void check(cublasStatus_t status, const char* func, const char* file, int line) {
            auto cublasGetErrorString = [](cublasStatus_t err) {
                switch (err) {
                case CUBLAS_STATUS_SUCCESS: return ""CUBLAS_STATUS_SUCCESS"";
                case CUBLAS_STATUS_NOT_INITIALIZED: return ""CUBLAS_STATUS_NOT_INITIALIZED"";
                case CUBLAS_STATUS_ALLOC_FAILED: return ""CUBLAS_STATUS_ALLOC_FAILED"";
                case CUBLAS_STATUS_INVALID_VALUE: return ""CUBLAS_STATUS_INVALID_VALUE"";
                case CUBLAS_STATUS_ARCH_MISMATCH: return ""CUBLAS_STATUS_ARCH_MISMATCH"";
                case CUBLAS_STATUS_MAPPING_ERROR: return ""CUBLAS_STATUS_MAPPING_ERROR"";
                case CUBLAS_STATUS_EXECUTION_FAILED: retur...",69,src\cuda4dnn\csl\cublas.hpp,cv.dnn.cuda4dnn.csl.cublas,24,cublas,1
349857,NAMESPACE_BLOCK,"namespace detail {
        static void check(cublasStatus_t status, const char* func, const char* file, int line) {
            auto cublasGetErrorString = [](cublasStatus_t err) {
                switch (err) {
                case CUBLAS_STATUS_SUCCESS: return ""CUBLAS_STATUS_SUCCESS"";
                case CUBLAS_STATUS_NOT_INITIALIZED: return ""CUBLAS_STATUS_NOT_INITIALIZED"";
                case CUBLAS_STATUS_ALLOC_FAILED: return ""CUBLAS_STATUS_ALLOC_FAILED"";
                case CUBLAS_STATUS_INVALID_VALUE: return ""CUBLAS_STATUS_INVALID_VALUE"";
                case CUBLAS_STATUS_ARCH_MISMATCH: return ""CUBLAS_STATUS_ARCH_MISMATCH"";
                case CUBLAS_STATUS_MAPPING_ERROR: return ""CUBLAS_STATUS_MAPPING_ERROR"";
                case CUBLAS_STATUS_EXECUTION_FAILED: return ""CUBLAS_STATUS_EXECUTION_FAILED"";
                case CUBLAS_STATUS_INTERNAL_ERROR: return ""CUBLAS_STATUS_INTERNAL_ERROR"";
                case CUBLAS_STATUS_NOT_SUPPORTED: return ""CUBLAS_STATUS_NOT_SUPPORT...",5,src\cuda4dnn\csl\cublas.hpp,cv.dnn.cuda4dnn.csl.cublas.detail,32,detail,2
351302,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn.hpp,src\cuda4dnn\csl\cudnn.hpp:<global>,,<global>,1
351359,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\activation.hpp,src\cuda4dnn\csl\cudnn\activation.hpp:<global>,,<global>,1
351363,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    class ActivationDescriptor {
    public:
        enum class ActivationType {
            IDENTITY,
            RELU,
            CLIPPED_RELU,
            TANH,
            SIGMOID,
            ELU
        };

        ActivationDescriptor() noexcept : descriptor{ nullptr } { }
        ActivationDescriptor(const ActivationDescriptor&) = delete;
        ActivationDescriptor(ActivationDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /* `relu_ceiling_or_elu_alpha`:
         * - `alpha` coefficient in ELU activation
         * - `ceiling` for CLIPPED_RELU activation
         */
        ActivationDescriptor(ActivationType type, double relu_ceiling_or_elu_alpha = 0.0) {
            CUDA4DNN_CHECK_CUDNN(cudnnCreateActivationDescriptor(&descriptor));
            try {
                const auto mode = [type] {
       ...",1,src\cuda4dnn\csl\cudnn\activation.hpp,cv,10,cv,1
351364,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    class ActivationDescriptor {
    public:
        enum class ActivationType {
            IDENTITY,
            RELU,
            CLIPPED_RELU,
            TANH,
            SIGMOID,
            ELU
        };

        ActivationDescriptor() noexcept : descriptor{ nullptr } { }
        ActivationDescriptor(const ActivationDescriptor&) = delete;
        ActivationDescriptor(ActivationDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /* `relu_ceiling_or_elu_alpha`:
         * - `alpha` coefficient in ELU activation
         * - `ceiling` for CLIPPED_RELU activation
         */
        ActivationDescriptor(ActivationType type, double relu_ceiling_or_elu_alpha = 0.0) {
            CUDA4DNN_CHECK_CUDNN(cudnnCreateActivationDescriptor(&descriptor));
            try {
                const auto mode = [type] {
                    sw...",16,src\cuda4dnn\csl\cudnn\activation.hpp,cv.dnn,10,dnn,1
351365,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

    class ActivationDescriptor {
    public:
        enum class ActivationType {
            IDENTITY,
            RELU,
            CLIPPED_RELU,
            TANH,
            SIGMOID,
            ELU
        };

        ActivationDescriptor() noexcept : descriptor{ nullptr } { }
        ActivationDescriptor(const ActivationDescriptor&) = delete;
        ActivationDescriptor(ActivationDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /* `relu_ceiling_or_elu_alpha`:
         * - `alpha` coefficient in ELU activation
         * - `ceiling` for CLIPPED_RELU activation
         */
        ActivationDescriptor(ActivationType type, double relu_ceiling_or_elu_alpha = 0.0) {
            CUDA4DNN_CHECK_CUDNN(cudnnCreateActivationDescriptor(&descriptor));
            try {
                const auto mode = [type] {
                    switch(type) {
   ...",32,src\cuda4dnn\csl\cudnn\activation.hpp,cv.dnn.cuda4dnn,10,cuda4dnn,1
351366,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

    class ActivationDescriptor {
    public:
        enum class ActivationType {
            IDENTITY,
            RELU,
            CLIPPED_RELU,
            TANH,
            SIGMOID,
            ELU
        };

        ActivationDescriptor() noexcept : descriptor{ nullptr } { }
        ActivationDescriptor(const ActivationDescriptor&) = delete;
        ActivationDescriptor(ActivationDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /* `relu_ceiling_or_elu_alpha`:
         * - `alpha` coefficient in ELU activation
         * - `ceiling` for CLIPPED_RELU activation
         */
        ActivationDescriptor(ActivationType type, double relu_ceiling_or_elu_alpha = 0.0) {
            CUDA4DNN_CHECK_CUDNN(cudnnCreateActivationDescriptor(&descriptor));
            try {
                const auto mode = [type] {
                    switch(type) {
                        ...",53,src\cuda4dnn\csl\cudnn\activation.hpp,cv.dnn.cuda4dnn.csl,10,csl,1
351367,NAMESPACE_BLOCK,"namespace cudnn {

    class ActivationDescriptor {
    public:
        enum class ActivationType {
            IDENTITY,
            RELU,
            CLIPPED_RELU,
            TANH,
            SIGMOID,
            ELU
        };

        ActivationDescriptor() noexcept : descriptor{ nullptr } { }
        ActivationDescriptor(const ActivationDescriptor&) = delete;
        ActivationDescriptor(ActivationDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /* `relu_ceiling_or_elu_alpha`:
         * - `alpha` coefficient in ELU activation
         * - `ceiling` for CLIPPED_RELU activation
         */
        ActivationDescriptor(ActivationType type, double relu_ceiling_or_elu_alpha = 0.0) {
            CUDA4DNN_CHECK_CUDNN(cudnnCreateActivationDescriptor(&descriptor));
            try {
                const auto mode = [type] {
                    switch(type) {
                        case ActivationT...",69,src\cuda4dnn\csl\cudnn\activation.hpp,cv.dnn.cuda4dnn.csl.cudnn,10,cudnn,1
351486,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\convolution.hpp,src\cuda4dnn\csl\cudnn\convolution.hpp:<global>,,<global>,1
351490,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** describe convolution filters
     *
     * @tparam  T   type of elements in the kernels
     */
    template <class T>
    class FilterDescriptor {
    public:
        FilterDescriptor() noexcept : descriptor{ nullptr } { }
        FilterDescriptor(const FilterDescriptor&) = delete;
        FilterDescriptor(FilterDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a filter descriptor from the filter dimensions provided in \p shape
         *
         * Shape dimensions:
         * 0: number of filters
         * 1: number of input feature maps
         * 2..n: kernel dimensions
         *
         * Exception Guarantee: Strong
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer>()))>
        FilterDescriptor(const SequenceContain...",1,src\cuda4dnn\csl\cudnn\convolution.hpp,cv,23,cv,1
351491,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** describe convolution filters
     *
     * @tparam  T   type of elements in the kernels
     */
    template <class T>
    class FilterDescriptor {
    public:
        FilterDescriptor() noexcept : descriptor{ nullptr } { }
        FilterDescriptor(const FilterDescriptor&) = delete;
        FilterDescriptor(FilterDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a filter descriptor from the filter dimensions provided in \p shape
         *
         * Shape dimensions:
         * 0: number of filters
         * 1: number of input feature maps
         * 2..n: kernel dimensions
         *
         * Exception Guarantee: Strong
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer>()))>
        FilterDescriptor(const SequenceContainer& shape) {
  ...",16,src\cuda4dnn\csl\cudnn\convolution.hpp,cv.dnn,23,dnn,1
351492,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

    /** describe convolution filters
     *
     * @tparam  T   type of elements in the kernels
     */
    template <class T>
    class FilterDescriptor {
    public:
        FilterDescriptor() noexcept : descriptor{ nullptr } { }
        FilterDescriptor(const FilterDescriptor&) = delete;
        FilterDescriptor(FilterDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a filter descriptor from the filter dimensions provided in \p shape
         *
         * Shape dimensions:
         * 0: number of filters
         * 1: number of input feature maps
         * 2..n: kernel dimensions
         *
         * Exception Guarantee: Strong
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer>()))>
        FilterDescriptor(const SequenceContainer& shape) {
            constr...",32,src\cuda4dnn\csl\cudnn\convolution.hpp,cv.dnn.cuda4dnn,23,cuda4dnn,1
351493,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

    /** describe convolution filters
     *
     * @tparam  T   type of elements in the kernels
     */
    template <class T>
    class FilterDescriptor {
    public:
        FilterDescriptor() noexcept : descriptor{ nullptr } { }
        FilterDescriptor(const FilterDescriptor&) = delete;
        FilterDescriptor(FilterDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a filter descriptor from the filter dimensions provided in \p shape
         *
         * Shape dimensions:
         * 0: number of filters
         * 1: number of input feature maps
         * 2..n: kernel dimensions
         *
         * Exception Guarantee: Strong
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer>()))>
        FilterDescriptor(const SequenceContainer& shape) {
            constructor(shape.begin(), ...",53,src\cuda4dnn\csl\cudnn\convolution.hpp,cv.dnn.cuda4dnn.csl,23,csl,1
351494,NAMESPACE_BLOCK,"namespace cudnn {

    /** describe convolution filters
     *
     * @tparam  T   type of elements in the kernels
     */
    template <class T>
    class FilterDescriptor {
    public:
        FilterDescriptor() noexcept : descriptor{ nullptr } { }
        FilterDescriptor(const FilterDescriptor&) = delete;
        FilterDescriptor(FilterDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a filter descriptor from the filter dimensions provided in \p shape
         *
         * Shape dimensions:
         * 0: number of filters
         * 1: number of input feature maps
         * 2..n: kernel dimensions
         *
         * Exception Guarantee: Strong
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer>()))>
        FilterDescriptor(const SequenceContainer& shape) {
            constructor(shape.begin(), shape.end());
  ...",69,src\cuda4dnn\csl\cudnn\convolution.hpp,cv.dnn.cuda4dnn.csl.cudnn,23,cudnn,1
353094,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\cudnn.hpp,src\cuda4dnn\csl\cudnn\cudnn.hpp:<global>,,<global>,1
353098,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** @brief exception class for errors thrown by the cuDNN API */
    class cuDNNException : public CUDAException {
    public:
        cuDNNException(cudnnStatus_t code, const std::string& msg, const std::string& func, const std::string& file, int line)
            : CUDAException(Error::GpuApiCallError, msg, func, file, line), cudnnError{code}
        {
        }

        cudnnStatus_t getCUDNNStatus() const noexcept { return cudnnError; }

    private:
        cudnnStatus_t cudnnError;
    };

    namespace detail {
        inline void check(cudnnStatus_t status, const char* func, const char* file, int line) {
            if (status != CUDNN_STATUS_SUCCESS)
                throw cuDNNException(status, cudnnGetErrorString(status), func, file, line);
        }

        /** get_data_type<T> returns the equivalent cudnn enumeration constant for type T */
        using cudnn_data_enum_type = decl...",1,src\cuda4dnn\csl\cudnn\cudnn.hpp,cv,24,cv,1
353099,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** @brief exception class for errors thrown by the cuDNN API */
    class cuDNNException : public CUDAException {
    public:
        cuDNNException(cudnnStatus_t code, const std::string& msg, const std::string& func, const std::string& file, int line)
            : CUDAException(Error::GpuApiCallError, msg, func, file, line), cudnnError{code}
        {
        }

        cudnnStatus_t getCUDNNStatus() const noexcept { return cudnnError; }

    private:
        cudnnStatus_t cudnnError;
    };

    namespace detail {
        inline void check(cudnnStatus_t status, const char* func, const char* file, int line) {
            if (status != CUDNN_STATUS_SUCCESS)
                throw cuDNNException(status, cudnnGetErrorString(status), func, file, line);
        }

        /** get_data_type<T> returns the equivalent cudnn enumeration constant for type T */
        using cudnn_data_enum_type = decltype(CUDNN_DATA...",16,src\cuda4dnn\csl\cudnn\cudnn.hpp,cv.dnn,24,dnn,1
353100,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

    /** @brief exception class for errors thrown by the cuDNN API */
    class cuDNNException : public CUDAException {
    public:
        cuDNNException(cudnnStatus_t code, const std::string& msg, const std::string& func, const std::string& file, int line)
            : CUDAException(Error::GpuApiCallError, msg, func, file, line), cudnnError{code}
        {
        }

        cudnnStatus_t getCUDNNStatus() const noexcept { return cudnnError; }

    private:
        cudnnStatus_t cudnnError;
    };

    namespace detail {
        inline void check(cudnnStatus_t status, const char* func, const char* file, int line) {
            if (status != CUDNN_STATUS_SUCCESS)
                throw cuDNNException(status, cudnnGetErrorString(status), func, file, line);
        }

        /** get_data_type<T> returns the equivalent cudnn enumeration constant for type T */
        using cudnn_data_enum_type = decltype(CUDNN_DATA_FLOAT);
       ...",32,src\cuda4dnn\csl\cudnn\cudnn.hpp,cv.dnn.cuda4dnn,24,cuda4dnn,1
353101,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

    /** @brief exception class for errors thrown by the cuDNN API */
    class cuDNNException : public CUDAException {
    public:
        cuDNNException(cudnnStatus_t code, const std::string& msg, const std::string& func, const std::string& file, int line)
            : CUDAException(Error::GpuApiCallError, msg, func, file, line), cudnnError{code}
        {
        }

        cudnnStatus_t getCUDNNStatus() const noexcept { return cudnnError; }

    private:
        cudnnStatus_t cudnnError;
    };

    namespace detail {
        inline void check(cudnnStatus_t status, const char* func, const char* file, int line) {
            if (status != CUDNN_STATUS_SUCCESS)
                throw cuDNNException(status, cudnnGetErrorString(status), func, file, line);
        }

        /** get_data_type<T> returns the equivalent cudnn enumeration constant for type T */
        using cudnn_data_enum_type = decltype(CUDNN_DATA_FLOAT);
        template <class> cud...",53,src\cuda4dnn\csl\cudnn\cudnn.hpp,cv.dnn.cuda4dnn.csl,24,csl,1
353102,NAMESPACE_BLOCK,"namespace cudnn {

    /** @brief exception class for errors thrown by the cuDNN API */
    class cuDNNException : public CUDAException {
    public:
        cuDNNException(cudnnStatus_t code, const std::string& msg, const std::string& func, const std::string& file, int line)
            : CUDAException(Error::GpuApiCallError, msg, func, file, line), cudnnError{code}
        {
        }

        cudnnStatus_t getCUDNNStatus() const noexcept { return cudnnError; }

    private:
        cudnnStatus_t cudnnError;
    };

    namespace detail {
        inline void check(cudnnStatus_t status, const char* func, const char* file, int line) {
            if (status != CUDNN_STATUS_SUCCESS)
                throw cuDNNException(status, cudnnGetErrorString(status), func, file, line);
        }

        /** get_data_type<T> returns the equivalent cudnn enumeration constant for type T */
        using cudnn_data_enum_type = decltype(CUDNN_DATA_FLOAT);
        template <class> cudnn_data_enum_typ...",69,src\cuda4dnn\csl\cudnn\cudnn.hpp,cv.dnn.cuda4dnn.csl.cudnn,24,cudnn,1
353120,NAMESPACE_BLOCK,"namespace detail {
        inline void check(cudnnStatus_t status, const char* func, const char* file, int line) {
            if (status != CUDNN_STATUS_SUCCESS)
                throw cuDNNException(status, cudnnGetErrorString(status), func, file, line);
        }

        /** get_data_type<T> returns the equivalent cudnn enumeration constant for type T */
        using cudnn_data_enum_type = decltype(CUDNN_DATA_FLOAT);
        template <class> cudnn_data_enum_type get_data_type();
        template <> inline cudnn_data_enum_type get_data_type<half>() { return CUDNN_DATA_HALF; }
        template <> inline cudnn_data_enum_type get_data_type<float>() { return CUDNN_DATA_FLOAT; }
    }",5,src\cuda4dnn\csl\cudnn\cudnn.hpp,cv.dnn.cuda4dnn.csl.cudnn.detail,40,detail,2
353900,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\lrn.hpp,src\cuda4dnn\csl\cudnn\lrn.hpp:<global>,,<global>,1
353904,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    class LRNDescriptor {
    public:
        enum class LRNType {
            ACROSS_CHANNELS,
            WITHIN_CHANNEL
        };

        LRNDescriptor() noexcept : descriptor{ nullptr } { }
        LRNDescriptor(const LRNDescriptor&) = delete;
        LRNDescriptor(LRNDescriptor&& other) noexcept
            : descriptor{ other.descriptor }, type{ other.type } {
            other.descriptor = nullptr;
        }

        /** sets up a LRN descriptor
         *
         * @param local_size    size of the normalization window
         * @param alpha         variance scaling parameter
         * @param beta          power parameter
         * @param k             bias parameter
         *
         * @note \p alpha is divided by the window width in across channels mode
         * @note \p alpha is divided by the (window width)^spatialDimensions in within channel mode
         *
         * @note t...",1,src\cuda4dnn\csl\cudnn\lrn.hpp,cv,19,cv,1
353905,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    class LRNDescriptor {
    public:
        enum class LRNType {
            ACROSS_CHANNELS,
            WITHIN_CHANNEL
        };

        LRNDescriptor() noexcept : descriptor{ nullptr } { }
        LRNDescriptor(const LRNDescriptor&) = delete;
        LRNDescriptor(LRNDescriptor&& other) noexcept
            : descriptor{ other.descriptor }, type{ other.type } {
            other.descriptor = nullptr;
        }

        /** sets up a LRN descriptor
         *
         * @param local_size    size of the normalization window
         * @param alpha         variance scaling parameter
         * @param beta          power parameter
         * @param k             bias parameter
         *
         * @note \p alpha is divided by the window width in across channels mode
         * @note \p alpha is divided by the (window width)^spatialDimensions in within channel mode
         *
         * @note the \p alpha, \p...",16,src\cuda4dnn\csl\cudnn\lrn.hpp,cv.dnn,19,dnn,1
353906,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

    class LRNDescriptor {
    public:
        enum class LRNType {
            ACROSS_CHANNELS,
            WITHIN_CHANNEL
        };

        LRNDescriptor() noexcept : descriptor{ nullptr } { }
        LRNDescriptor(const LRNDescriptor&) = delete;
        LRNDescriptor(LRNDescriptor&& other) noexcept
            : descriptor{ other.descriptor }, type{ other.type } {
            other.descriptor = nullptr;
        }

        /** sets up a LRN descriptor
         *
         * @param local_size    size of the normalization window
         * @param alpha         variance scaling parameter
         * @param beta          power parameter
         * @param k             bias parameter
         *
         * @note \p alpha is divided by the window width in across channels mode
         * @note \p alpha is divided by the (window width)^spatialDimensions in within channel mode
         *
         * @note the \p alpha, \p beta and \p k w...",32,src\cuda4dnn\csl\cudnn\lrn.hpp,cv.dnn.cuda4dnn,19,cuda4dnn,1
353907,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

    class LRNDescriptor {
    public:
        enum class LRNType {
            ACROSS_CHANNELS,
            WITHIN_CHANNEL
        };

        LRNDescriptor() noexcept : descriptor{ nullptr } { }
        LRNDescriptor(const LRNDescriptor&) = delete;
        LRNDescriptor(LRNDescriptor&& other) noexcept
            : descriptor{ other.descriptor }, type{ other.type } {
            other.descriptor = nullptr;
        }

        /** sets up a LRN descriptor
         *
         * @param local_size    size of the normalization window
         * @param alpha         variance scaling parameter
         * @param beta          power parameter
         * @param k             bias parameter
         *
         * @note \p alpha is divided by the window width in across channels mode
         * @note \p alpha is divided by the (window width)^spatialDimensions in within channel mode
         *
         * @note the \p alpha, \p beta and \p k will be type casted to...",53,src\cuda4dnn\csl\cudnn\lrn.hpp,cv.dnn.cuda4dnn.csl,19,csl,1
353908,NAMESPACE_BLOCK,"namespace cudnn {

    class LRNDescriptor {
    public:
        enum class LRNType {
            ACROSS_CHANNELS,
            WITHIN_CHANNEL
        };

        LRNDescriptor() noexcept : descriptor{ nullptr } { }
        LRNDescriptor(const LRNDescriptor&) = delete;
        LRNDescriptor(LRNDescriptor&& other) noexcept
            : descriptor{ other.descriptor }, type{ other.type } {
            other.descriptor = nullptr;
        }

        /** sets up a LRN descriptor
         *
         * @param local_size    size of the normalization window
         * @param alpha         variance scaling parameter
         * @param beta          power parameter
         * @param k             bias parameter
         *
         * @note \p alpha is divided by the window width in across channels mode
         * @note \p alpha is divided by the (window width)^spatialDimensions in within channel mode
         *
         * @note the \p alpha, \p beta and \p k will be type casted to the tensor data...",69,src\cuda4dnn\csl\cudnn\lrn.hpp,cv.dnn.cuda4dnn.csl.cudnn,19,cudnn,1
354577,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\pooling.hpp,src\cuda4dnn\csl\cudnn\pooling.hpp:<global>,,<global>,1
354581,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    class PoolingDescriptor {
    public:
        enum class PoolingType {
            MAX,
            MAX_DETERMINISTIC,
            AVERAGE_EXCLUDE_PADDING,
            AVERAGE_INCLUDE_PADDING
        };

        PoolingDescriptor() noexcept : descriptor{ nullptr } { }
        PoolingDescriptor(const PoolingDescriptor&) = delete;
        PoolingDescriptor(PoolingDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a pooling descriptor
         *
         * Pre-conditions:
         * - \p window_size, \p padding and \p stride must have the same size
         *
         * The length of the containers is interpreted as the order of the pooling operation.
         *
         * Exception Guarantee: Basic
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<Se...",1,src\cuda4dnn\csl\cudnn\pooling.hpp,cv,23,cv,1
354582,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    class PoolingDescriptor {
    public:
        enum class PoolingType {
            MAX,
            MAX_DETERMINISTIC,
            AVERAGE_EXCLUDE_PADDING,
            AVERAGE_INCLUDE_PADDING
        };

        PoolingDescriptor() noexcept : descriptor{ nullptr } { }
        PoolingDescriptor(const PoolingDescriptor&) = delete;
        PoolingDescriptor(PoolingDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a pooling descriptor
         *
         * Pre-conditions:
         * - \p window_size, \p padding and \p stride must have the same size
         *
         * The length of the containers is interpreted as the order of the pooling operation.
         *
         * Exception Guarantee: Basic
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer...",16,src\cuda4dnn\csl\cudnn\pooling.hpp,cv.dnn,23,dnn,1
354583,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

    class PoolingDescriptor {
    public:
        enum class PoolingType {
            MAX,
            MAX_DETERMINISTIC,
            AVERAGE_EXCLUDE_PADDING,
            AVERAGE_INCLUDE_PADDING
        };

        PoolingDescriptor() noexcept : descriptor{ nullptr } { }
        PoolingDescriptor(const PoolingDescriptor&) = delete;
        PoolingDescriptor(PoolingDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a pooling descriptor
         *
         * Pre-conditions:
         * - \p window_size, \p padding and \p stride must have the same size
         *
         * The length of the containers is interpreted as the order of the pooling operation.
         *
         * Exception Guarantee: Basic
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer>()))>
        P...",32,src\cuda4dnn\csl\cudnn\pooling.hpp,cv.dnn.cuda4dnn,23,cuda4dnn,1
354584,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

    class PoolingDescriptor {
    public:
        enum class PoolingType {
            MAX,
            MAX_DETERMINISTIC,
            AVERAGE_EXCLUDE_PADDING,
            AVERAGE_INCLUDE_PADDING
        };

        PoolingDescriptor() noexcept : descriptor{ nullptr } { }
        PoolingDescriptor(const PoolingDescriptor&) = delete;
        PoolingDescriptor(PoolingDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a pooling descriptor
         *
         * Pre-conditions:
         * - \p window_size, \p padding and \p stride must have the same size
         *
         * The length of the containers is interpreted as the order of the pooling operation.
         *
         * Exception Guarantee: Basic
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer>()))>
        PoolingDescriptor(
   ...",53,src\cuda4dnn\csl\cudnn\pooling.hpp,cv.dnn.cuda4dnn.csl,23,csl,1
354585,NAMESPACE_BLOCK,"namespace cudnn {

    class PoolingDescriptor {
    public:
        enum class PoolingType {
            MAX,
            MAX_DETERMINISTIC,
            AVERAGE_EXCLUDE_PADDING,
            AVERAGE_INCLUDE_PADDING
        };

        PoolingDescriptor() noexcept : descriptor{ nullptr } { }
        PoolingDescriptor(const PoolingDescriptor&) = delete;
        PoolingDescriptor(PoolingDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a pooling descriptor
         *
         * Pre-conditions:
         * - \p window_size, \p padding and \p stride must have the same size
         *
         * The length of the containers is interpreted as the order of the pooling operation.
         *
         * Exception Guarantee: Basic
         */
        template <class SequenceContainer, typename = decltype(std::begin(std::declval<SequenceContainer>()))>
        PoolingDescriptor(
            const S...",69,src\cuda4dnn\csl\cudnn\pooling.hpp,cv.dnn.cuda4dnn.csl.cudnn,23,cudnn,1
355176,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\recurrent.hpp,src\cuda4dnn\csl\cudnn\recurrent.hpp:<global>,,<global>,1
355180,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

/**
 */
class DropoutDescriptor
{
public:
    DropoutDescriptor() noexcept = default;
    DropoutDescriptor(const DropoutDescriptor &) = delete;
    DropoutDescriptor(DropoutDescriptor &&other) noexcept : descriptor{other.descriptor}
    {
        states = std::move(other.states);
        other.descriptor = nullptr;
    }

    /**
     */
    DropoutDescriptor(const Handle &handle, float dropout)
    {
        CUDA4DNN_CHECK_CUDNN(cudnnCreateDropoutDescriptor(&descriptor));

        // we need additional memory for dropout descriptor
        size_t stateSize;
        CUDA4DNN_CHECK_CUDNN(cudnnDropoutGetStatesSize(handle.get(), &stateSize));
        states.reset(stateSize);

        try
        {
            auto seed = 1234ull; // Pick a seed.
            CUDA4DNN_CHECK_CUDNN(cudnnSetDropoutDescriptor(descriptor, handle.get(), dropout,
                                                           sta...",1,src\cuda4dnn\csl\cudnn\recurrent.hpp,cv,12,cv,1
355181,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

/**
 */
class DropoutDescriptor
{
public:
    DropoutDescriptor() noexcept = default;
    DropoutDescriptor(const DropoutDescriptor &) = delete;
    DropoutDescriptor(DropoutDescriptor &&other) noexcept : descriptor{other.descriptor}
    {
        states = std::move(other.states);
        other.descriptor = nullptr;
    }

    /**
     */
    DropoutDescriptor(const Handle &handle, float dropout)
    {
        CUDA4DNN_CHECK_CUDNN(cudnnCreateDropoutDescriptor(&descriptor));

        // we need additional memory for dropout descriptor
        size_t stateSize;
        CUDA4DNN_CHECK_CUDNN(cudnnDropoutGetStatesSize(handle.get(), &stateSize));
        states.reset(stateSize);

        try
        {
            auto seed = 1234ull; // Pick a seed.
            CUDA4DNN_CHECK_CUDNN(cudnnSetDropoutDescriptor(descriptor, handle.get(), dropout,
                                                           states.get().get()...",16,src\cuda4dnn\csl\cudnn\recurrent.hpp,cv.dnn,12,dnn,1
355182,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

/**
 */
class DropoutDescriptor
{
public:
    DropoutDescriptor() noexcept = default;
    DropoutDescriptor(const DropoutDescriptor &) = delete;
    DropoutDescriptor(DropoutDescriptor &&other) noexcept : descriptor{other.descriptor}
    {
        states = std::move(other.states);
        other.descriptor = nullptr;
    }

    /**
     */
    DropoutDescriptor(const Handle &handle, float dropout)
    {
        CUDA4DNN_CHECK_CUDNN(cudnnCreateDropoutDescriptor(&descriptor));

        // we need additional memory for dropout descriptor
        size_t stateSize;
        CUDA4DNN_CHECK_CUDNN(cudnnDropoutGetStatesSize(handle.get(), &stateSize));
        states.reset(stateSize);

        try
        {
            auto seed = 1234ull; // Pick a seed.
            CUDA4DNN_CHECK_CUDNN(cudnnSetDropoutDescriptor(descriptor, handle.get(), dropout,
                                                           states.get().get(), stateSize, see...",32,src\cuda4dnn\csl\cudnn\recurrent.hpp,cv.dnn.cuda4dnn,12,cuda4dnn,1
355183,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

/**
 */
class DropoutDescriptor
{
public:
    DropoutDescriptor() noexcept = default;
    DropoutDescriptor(const DropoutDescriptor &) = delete;
    DropoutDescriptor(DropoutDescriptor &&other) noexcept : descriptor{other.descriptor}
    {
        states = std::move(other.states);
        other.descriptor = nullptr;
    }

    /**
     */
    DropoutDescriptor(const Handle &handle, float dropout)
    {
        CUDA4DNN_CHECK_CUDNN(cudnnCreateDropoutDescriptor(&descriptor));

        // we need additional memory for dropout descriptor
        size_t stateSize;
        CUDA4DNN_CHECK_CUDNN(cudnnDropoutGetStatesSize(handle.get(), &stateSize));
        states.reset(stateSize);

        try
        {
            auto seed = 1234ull; // Pick a seed.
            CUDA4DNN_CHECK_CUDNN(cudnnSetDropoutDescriptor(descriptor, handle.get(), dropout,
                                                           states.get().get(), stateSize, seed));
        }
      ...",53,src\cuda4dnn\csl\cudnn\recurrent.hpp,cv.dnn.cuda4dnn.csl,12,csl,1
355184,NAMESPACE_BLOCK,"namespace cudnn {

/**
 */
class DropoutDescriptor
{
public:
    DropoutDescriptor() noexcept = default;
    DropoutDescriptor(const DropoutDescriptor &) = delete;
    DropoutDescriptor(DropoutDescriptor &&other) noexcept : descriptor{other.descriptor}
    {
        states = std::move(other.states);
        other.descriptor = nullptr;
    }

    /**
     */
    DropoutDescriptor(const Handle &handle, float dropout)
    {
        CUDA4DNN_CHECK_CUDNN(cudnnCreateDropoutDescriptor(&descriptor));

        // we need additional memory for dropout descriptor
        size_t stateSize;
        CUDA4DNN_CHECK_CUDNN(cudnnDropoutGetStatesSize(handle.get(), &stateSize));
        states.reset(stateSize);

        try
        {
            auto seed = 1234ull; // Pick a seed.
            CUDA4DNN_CHECK_CUDNN(cudnnSetDropoutDescriptor(descriptor, handle.get(), dropout,
                                                           states.get().get(), stateSize, seed));
        }
        catch (...)
  ...",69,src\cuda4dnn\csl\cudnn\recurrent.hpp,cv.dnn.cuda4dnn.csl.cudnn,12,cudnn,1
355804,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\softmax.hpp,src\cuda4dnn\csl\cudnn\softmax.hpp:<global>,,<global>,1
355808,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** @brief computes softmax (or log softmax)
     *
     * @tparam          T           element type (must be `half` or `float`)
     *
     * @param           handle      valid cuDNN handle
     * @param           outputDesc  tensor descriptor for A
     * @param[out]      output      pointer to tensor in device memory
     * @param           inputDesc   tensor descriptor for C
     * @param[in]       input       pointer to tensor in device memory
     * @param           log         apply log on probabilities
     *
     * Exception Guarantee: Basic
     */
    template <class T>
    void softmax(const cudnn::Handle& handle,
        const TensorDescriptor<T>& outputDesc, DevicePtr<T> output,
        const TensorDescriptor<T>& inputDesc, DevicePtr<const T> input,
        bool log)
    {
        T alpha = 1.0, beta = 0.0;
        cudnnSoftmaxAlgorithm_t algo = log ? CUDNN_SOFTMAX_LOG : CUDNN_SO...",1,src\cuda4dnn\csl\cudnn\softmax.hpp,cv,14,cv,1
355809,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** @brief computes softmax (or log softmax)
     *
     * @tparam          T           element type (must be `half` or `float`)
     *
     * @param           handle      valid cuDNN handle
     * @param           outputDesc  tensor descriptor for A
     * @param[out]      output      pointer to tensor in device memory
     * @param           inputDesc   tensor descriptor for C
     * @param[in]       input       pointer to tensor in device memory
     * @param           log         apply log on probabilities
     *
     * Exception Guarantee: Basic
     */
    template <class T>
    void softmax(const cudnn::Handle& handle,
        const TensorDescriptor<T>& outputDesc, DevicePtr<T> output,
        const TensorDescriptor<T>& inputDesc, DevicePtr<const T> input,
        bool log)
    {
        T alpha = 1.0, beta = 0.0;
        cudnnSoftmaxAlgorithm_t algo = log ? CUDNN_SOFTMAX_LOG : CUDNN_SOFTMAX_ACCURATE;...",16,src\cuda4dnn\csl\cudnn\softmax.hpp,cv.dnn,14,dnn,1
355810,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

    /** @brief computes softmax (or log softmax)
     *
     * @tparam          T           element type (must be `half` or `float`)
     *
     * @param           handle      valid cuDNN handle
     * @param           outputDesc  tensor descriptor for A
     * @param[out]      output      pointer to tensor in device memory
     * @param           inputDesc   tensor descriptor for C
     * @param[in]       input       pointer to tensor in device memory
     * @param           log         apply log on probabilities
     *
     * Exception Guarantee: Basic
     */
    template <class T>
    void softmax(const cudnn::Handle& handle,
        const TensorDescriptor<T>& outputDesc, DevicePtr<T> output,
        const TensorDescriptor<T>& inputDesc, DevicePtr<const T> input,
        bool log)
    {
        T alpha = 1.0, beta = 0.0;
        cudnnSoftmaxAlgorithm_t algo = log ? CUDNN_SOFTMAX_LOG : CUDNN_SOFTMAX_ACCURATE;
        CUDA4DN...",32,src\cuda4dnn\csl\cudnn\softmax.hpp,cv.dnn.cuda4dnn,14,cuda4dnn,1
355811,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

    /** @brief computes softmax (or log softmax)
     *
     * @tparam          T           element type (must be `half` or `float`)
     *
     * @param           handle      valid cuDNN handle
     * @param           outputDesc  tensor descriptor for A
     * @param[out]      output      pointer to tensor in device memory
     * @param           inputDesc   tensor descriptor for C
     * @param[in]       input       pointer to tensor in device memory
     * @param           log         apply log on probabilities
     *
     * Exception Guarantee: Basic
     */
    template <class T>
    void softmax(const cudnn::Handle& handle,
        const TensorDescriptor<T>& outputDesc, DevicePtr<T> output,
        const TensorDescriptor<T>& inputDesc, DevicePtr<const T> input,
        bool log)
    {
        T alpha = 1.0, beta = 0.0;
        cudnnSoftmaxAlgorithm_t algo = log ? CUDNN_SOFTMAX_LOG : CUDNN_SOFTMAX_ACCURATE;
        CUDA4DNN_CHECK_CUDNN(
      ...",53,src\cuda4dnn\csl\cudnn\softmax.hpp,cv.dnn.cuda4dnn.csl,14,csl,1
355812,NAMESPACE_BLOCK,"namespace cudnn {

    /** @brief computes softmax (or log softmax)
     *
     * @tparam          T           element type (must be `half` or `float`)
     *
     * @param           handle      valid cuDNN handle
     * @param           outputDesc  tensor descriptor for A
     * @param[out]      output      pointer to tensor in device memory
     * @param           inputDesc   tensor descriptor for C
     * @param[in]       input       pointer to tensor in device memory
     * @param           log         apply log on probabilities
     *
     * Exception Guarantee: Basic
     */
    template <class T>
    void softmax(const cudnn::Handle& handle,
        const TensorDescriptor<T>& outputDesc, DevicePtr<T> output,
        const TensorDescriptor<T>& inputDesc, DevicePtr<const T> input,
        bool log)
    {
        T alpha = 1.0, beta = 0.0;
        cudnnSoftmaxAlgorithm_t algo = log ? CUDNN_SOFTMAX_LOG : CUDNN_SOFTMAX_ACCURATE;
        CUDA4DNN_CHECK_CUDNN(
            cudnnSoftm...",69,src\cuda4dnn\csl\cudnn\softmax.hpp,cv.dnn.cuda4dnn.csl.cudnn,14,cudnn,1
355968,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\transform.hpp,src\cuda4dnn\csl\cudnn\transform.hpp:<global>,,<global>,1
355972,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** describes a tensor transform operation
     *
     * Supported transformations:
     * - add or remove asymmetric padding
     */
    class TensorTransformDescriptor {
    public:
        TensorTransformDescriptor() noexcept : descriptor{ nullptr } { }
        TensorTransformDescriptor(const TensorTransformDescriptor&) = delete;
        TensorTransformDescriptor(TensorTransformDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a convolution descriptor
         *
         * Pre-conditions:
         * - \p padding_left and \p padding_right must have the same size
         *
         * The length of the containers is interpreted as the rank of the tensors which will be given.
         *
         * @note \p padding_left and \p padding_right may have negative values to remove padding
         *
  ...",1,src\cuda4dnn\csl\cudnn\transform.hpp,cv,17,cv,1
355973,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** describes a tensor transform operation
     *
     * Supported transformations:
     * - add or remove asymmetric padding
     */
    class TensorTransformDescriptor {
    public:
        TensorTransformDescriptor() noexcept : descriptor{ nullptr } { }
        TensorTransformDescriptor(const TensorTransformDescriptor&) = delete;
        TensorTransformDescriptor(TensorTransformDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a convolution descriptor
         *
         * Pre-conditions:
         * - \p padding_left and \p padding_right must have the same size
         *
         * The length of the containers is interpreted as the rank of the tensors which will be given.
         *
         * @note \p padding_left and \p padding_right may have negative values to remove padding
         *
         * Except...",16,src\cuda4dnn\csl\cudnn\transform.hpp,cv.dnn,17,dnn,1
355974,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

    /** describes a tensor transform operation
     *
     * Supported transformations:
     * - add or remove asymmetric padding
     */
    class TensorTransformDescriptor {
    public:
        TensorTransformDescriptor() noexcept : descriptor{ nullptr } { }
        TensorTransformDescriptor(const TensorTransformDescriptor&) = delete;
        TensorTransformDescriptor(TensorTransformDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a convolution descriptor
         *
         * Pre-conditions:
         * - \p padding_left and \p padding_right must have the same size
         *
         * The length of the containers is interpreted as the rank of the tensors which will be given.
         *
         * @note \p padding_left and \p padding_right may have negative values to remove padding
         *
         * Exception Guarantee: B...",32,src\cuda4dnn\csl\cudnn\transform.hpp,cv.dnn.cuda4dnn,17,cuda4dnn,1
355975,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

    /** describes a tensor transform operation
     *
     * Supported transformations:
     * - add or remove asymmetric padding
     */
    class TensorTransformDescriptor {
    public:
        TensorTransformDescriptor() noexcept : descriptor{ nullptr } { }
        TensorTransformDescriptor(const TensorTransformDescriptor&) = delete;
        TensorTransformDescriptor(TensorTransformDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a convolution descriptor
         *
         * Pre-conditions:
         * - \p padding_left and \p padding_right must have the same size
         *
         * The length of the containers is interpreted as the rank of the tensors which will be given.
         *
         * @note \p padding_left and \p padding_right may have negative values to remove padding
         *
         * Exception Guarantee: Basic
         */
    ...",53,src\cuda4dnn\csl\cudnn\transform.hpp,cv.dnn.cuda4dnn.csl,17,csl,1
355976,NAMESPACE_BLOCK,"namespace cudnn {

    /** describes a tensor transform operation
     *
     * Supported transformations:
     * - add or remove asymmetric padding
     */
    class TensorTransformDescriptor {
    public:
        TensorTransformDescriptor() noexcept : descriptor{ nullptr } { }
        TensorTransformDescriptor(const TensorTransformDescriptor&) = delete;
        TensorTransformDescriptor(TensorTransformDescriptor&& other) noexcept
            : descriptor{ other.descriptor } {
            other.descriptor = nullptr;
        }

        /** constructs a convolution descriptor
         *
         * Pre-conditions:
         * - \p padding_left and \p padding_right must have the same size
         *
         * The length of the containers is interpreted as the rank of the tensors which will be given.
         *
         * @note \p padding_left and \p padding_right may have negative values to remove padding
         *
         * Exception Guarantee: Basic
         */
        template <cl...",69,src\cuda4dnn\csl\cudnn\transform.hpp,cv.dnn.cuda4dnn.csl.cudnn,17,cudnn,1
356359,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\cudnn\transpose_convolution.hpp,src\cuda4dnn\csl\cudnn\transpose_convolution.hpp:<global>,,<global>,1
356363,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** wrapper around a transpose convolution algorithm
     *
     * @tparam  T   type of elements being transpose-convolved
     */
    template <class T>
    class TransposeConvolutionAlgorithm {
    public:
        TransposeConvolutionAlgorithm() noexcept : workspace_size{ 0 } { }
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&) = default;
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&&) = default;

        TransposeConvolutionAlgorithm(
            const Handle& handle,
            const ConvolutionDescriptor<T>& convDesc,
            const FilterDescriptor<T>& filterDesc,
            const TensorDescriptor<T>& inputDesc,
            const TensorDescriptor<T>& outputDesc)
        {
#if CUDNN_MAJOR >= 8
            int requestedAlgoCount = 0, returnedAlgoCount = 0;
            CUDA4DNN_CHECK_CUDNN(cudnnGetConvolutionBackwardDataAlgorithmMaxCount(hand...",1,src\cuda4dnn\csl\cudnn\transpose_convolution.hpp,cv,18,cv,1
356364,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace cudnn {

    /** wrapper around a transpose convolution algorithm
     *
     * @tparam  T   type of elements being transpose-convolved
     */
    template <class T>
    class TransposeConvolutionAlgorithm {
    public:
        TransposeConvolutionAlgorithm() noexcept : workspace_size{ 0 } { }
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&) = default;
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&&) = default;

        TransposeConvolutionAlgorithm(
            const Handle& handle,
            const ConvolutionDescriptor<T>& convDesc,
            const FilterDescriptor<T>& filterDesc,
            const TensorDescriptor<T>& inputDesc,
            const TensorDescriptor<T>& outputDesc)
        {
#if CUDNN_MAJOR >= 8
            int requestedAlgoCount = 0, returnedAlgoCount = 0;
            CUDA4DNN_CHECK_CUDNN(cudnnGetConvolutionBackwardDataAlgorithmMaxCount(handle.get(), &requ...",16,src\cuda4dnn\csl\cudnn\transpose_convolution.hpp,cv.dnn,18,dnn,1
356365,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace cudnn {

    /** wrapper around a transpose convolution algorithm
     *
     * @tparam  T   type of elements being transpose-convolved
     */
    template <class T>
    class TransposeConvolutionAlgorithm {
    public:
        TransposeConvolutionAlgorithm() noexcept : workspace_size{ 0 } { }
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&) = default;
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&&) = default;

        TransposeConvolutionAlgorithm(
            const Handle& handle,
            const ConvolutionDescriptor<T>& convDesc,
            const FilterDescriptor<T>& filterDesc,
            const TensorDescriptor<T>& inputDesc,
            const TensorDescriptor<T>& outputDesc)
        {
#if CUDNN_MAJOR >= 8
            int requestedAlgoCount = 0, returnedAlgoCount = 0;
            CUDA4DNN_CHECK_CUDNN(cudnnGetConvolutionBackwardDataAlgorithmMaxCount(handle.get(), &requestedAlgoCount))...",32,src\cuda4dnn\csl\cudnn\transpose_convolution.hpp,cv.dnn.cuda4dnn,18,cuda4dnn,1
356366,NAMESPACE_BLOCK,"namespace csl { namespace cudnn {

    /** wrapper around a transpose convolution algorithm
     *
     * @tparam  T   type of elements being transpose-convolved
     */
    template <class T>
    class TransposeConvolutionAlgorithm {
    public:
        TransposeConvolutionAlgorithm() noexcept : workspace_size{ 0 } { }
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&) = default;
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&&) = default;

        TransposeConvolutionAlgorithm(
            const Handle& handle,
            const ConvolutionDescriptor<T>& convDesc,
            const FilterDescriptor<T>& filterDesc,
            const TensorDescriptor<T>& inputDesc,
            const TensorDescriptor<T>& outputDesc)
        {
#if CUDNN_MAJOR >= 8
            int requestedAlgoCount = 0, returnedAlgoCount = 0;
            CUDA4DNN_CHECK_CUDNN(cudnnGetConvolutionBackwardDataAlgorithmMaxCount(handle.get(), &requestedAlgoCount));
            std::ve...",53,src\cuda4dnn\csl\cudnn\transpose_convolution.hpp,cv.dnn.cuda4dnn.csl,18,csl,1
356367,NAMESPACE_BLOCK,"namespace cudnn {

    /** wrapper around a transpose convolution algorithm
     *
     * @tparam  T   type of elements being transpose-convolved
     */
    template <class T>
    class TransposeConvolutionAlgorithm {
    public:
        TransposeConvolutionAlgorithm() noexcept : workspace_size{ 0 } { }
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&) = default;
        TransposeConvolutionAlgorithm(TransposeConvolutionAlgorithm&&) = default;

        TransposeConvolutionAlgorithm(
            const Handle& handle,
            const ConvolutionDescriptor<T>& convDesc,
            const FilterDescriptor<T>& filterDesc,
            const TensorDescriptor<T>& inputDesc,
            const TensorDescriptor<T>& outputDesc)
        {
#if CUDNN_MAJOR >= 8
            int requestedAlgoCount = 0, returnedAlgoCount = 0;
            CUDA4DNN_CHECK_CUDNN(cudnnGetConvolutionBackwardDataAlgorithmMaxCount(handle.get(), &requestedAlgoCount));
            std::vector<cudnnConvol...",69,src\cuda4dnn\csl\cudnn\transpose_convolution.hpp,cv.dnn.cuda4dnn.csl.cudnn,18,cudnn,1
356688,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\error.hpp,src\cuda4dnn\csl\error.hpp:<global>,,<global>,1
356692,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {
    /** @brief exception class for errors thrown by the CUDA APIs */
    class CUDAException : public cv::Exception {
    public:
        using cv::Exception::Exception;
    };

    namespace detail {
        inline void check(cudaError_t err, const char* func, const char* file, int line) {
            if (err != cudaSuccess)
                throw CUDAException(Error::GpuApiCallError, cudaGetErrorString(err), func, file, line);
        }
    }
}}}}",1,src\cuda4dnn\csl\error.hpp,cv,15,cv,1
356693,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {
    /** @brief exception class for errors thrown by the CUDA APIs */
    class CUDAException : public cv::Exception {
    public:
        using cv::Exception::Exception;
    };

    namespace detail {
        inline void check(cudaError_t err, const char* func, const char* file, int line) {
            if (err != cudaSuccess)
                throw CUDAException(Error::GpuApiCallError, cudaGetErrorString(err), func, file, line);
        }
    }
}}}",16,src\cuda4dnn\csl\error.hpp,cv.dnn,15,dnn,1
356694,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {
    /** @brief exception class for errors thrown by the CUDA APIs */
    class CUDAException : public cv::Exception {
    public:
        using cv::Exception::Exception;
    };

    namespace detail {
        inline void check(cudaError_t err, const char* func, const char* file, int line) {
            if (err != cudaSuccess)
                throw CUDAException(Error::GpuApiCallError, cudaGetErrorString(err), func, file, line);
        }
    }
}}",32,src\cuda4dnn\csl\error.hpp,cv.dnn.cuda4dnn,15,cuda4dnn,1
356695,NAMESPACE_BLOCK,"namespace csl {
    /** @brief exception class for errors thrown by the CUDA APIs */
    class CUDAException : public cv::Exception {
    public:
        using cv::Exception::Exception;
    };

    namespace detail {
        inline void check(cudaError_t err, const char* func, const char* file, int line) {
            if (err != cudaSuccess)
                throw CUDAException(Error::GpuApiCallError, cudaGetErrorString(err), func, file, line);
        }
    }
}",53,src\cuda4dnn\csl\error.hpp,cv.dnn.cuda4dnn.csl,15,csl,1
356697,NAMESPACE_BLOCK,"namespace detail {
        inline void check(cudaError_t err, const char* func, const char* file, int line) {
            if (err != cudaSuccess)
                throw CUDAException(Error::GpuApiCallError, cudaGetErrorString(err), func, file, line);
        }
    }",5,src\cuda4dnn\csl\error.hpp,cv.dnn.cuda4dnn.csl.detail,22,detail,2
356730,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\event.hpp,src\cuda4dnn\csl\event.hpp:<global>,,<global>,1
356734,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    /** @brief sharable CUDA event
     *
     * Event is a smart sharable wrapper for CUDA event handle which ensures that
     * the handle is destroyed after use.
     *
     * @note Moving an Event object to another invalidates the former
     */
    class Event {
    public:
        Event() noexcept : event{ nullptr } { }
        Event(const Event&) = delete;
        Event(Event&& other) noexcept
            : event{ other.event } {
            other.event = nullptr;
        }

        /** if \p create is `true`, a new event will be created; otherwise, an empty event object is created */
        Event(bool create, bool timing_event = false) : event{nullptr} {
            if (create) {
                unsigned int flags = (timing_event ? 0 : cudaEventDisableTiming);
                CUDA4DNN_CHECK_CUDA(cudaEventCreateWithFlags(&event, flags));
            }
        }

        ~Event() {
            try {
       ...",1,src\cuda4dnn\csl\event.hpp,cv,15,cv,1
356735,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    /** @brief sharable CUDA event
     *
     * Event is a smart sharable wrapper for CUDA event handle which ensures that
     * the handle is destroyed after use.
     *
     * @note Moving an Event object to another invalidates the former
     */
    class Event {
    public:
        Event() noexcept : event{ nullptr } { }
        Event(const Event&) = delete;
        Event(Event&& other) noexcept
            : event{ other.event } {
            other.event = nullptr;
        }

        /** if \p create is `true`, a new event will be created; otherwise, an empty event object is created */
        Event(bool create, bool timing_event = false) : event{nullptr} {
            if (create) {
                unsigned int flags = (timing_event ? 0 : cudaEventDisableTiming);
                CUDA4DNN_CHECK_CUDA(cudaEventCreateWithFlags(&event, flags));
            }
        }

        ~Event() {
            try {
                if (ev...",16,src\cuda4dnn\csl\event.hpp,cv.dnn,15,dnn,1
356736,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    /** @brief sharable CUDA event
     *
     * Event is a smart sharable wrapper for CUDA event handle which ensures that
     * the handle is destroyed after use.
     *
     * @note Moving an Event object to another invalidates the former
     */
    class Event {
    public:
        Event() noexcept : event{ nullptr } { }
        Event(const Event&) = delete;
        Event(Event&& other) noexcept
            : event{ other.event } {
            other.event = nullptr;
        }

        /** if \p create is `true`, a new event will be created; otherwise, an empty event object is created */
        Event(bool create, bool timing_event = false) : event{nullptr} {
            if (create) {
                unsigned int flags = (timing_event ? 0 : cudaEventDisableTiming);
                CUDA4DNN_CHECK_CUDA(cudaEventCreateWithFlags(&event, flags));
            }
        }

        ~Event() {
            try {
                if (event != nullptr)
...",32,src\cuda4dnn\csl\event.hpp,cv.dnn.cuda4dnn,15,cuda4dnn,1
356737,NAMESPACE_BLOCK,"namespace csl {

    /** @brief sharable CUDA event
     *
     * Event is a smart sharable wrapper for CUDA event handle which ensures that
     * the handle is destroyed after use.
     *
     * @note Moving an Event object to another invalidates the former
     */
    class Event {
    public:
        Event() noexcept : event{ nullptr } { }
        Event(const Event&) = delete;
        Event(Event&& other) noexcept
            : event{ other.event } {
            other.event = nullptr;
        }

        /** if \p create is `true`, a new event will be created; otherwise, an empty event object is created */
        Event(bool create, bool timing_event = false) : event{nullptr} {
            if (create) {
                unsigned int flags = (timing_event ? 0 : cudaEventDisableTiming);
                CUDA4DNN_CHECK_CUDA(cudaEventCreateWithFlags(&event, flags));
            }
        }

        ~Event() {
            try {
                if (event != nullptr)
                    C...",53,src\cuda4dnn\csl\event.hpp,cv.dnn.cuda4dnn.csl,15,csl,1
357126,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\memory.hpp,src\cuda4dnn\csl\memory.hpp:<global>,,<global>,1
357130,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    /* @brief smart device pointer with allocation/deallocation methods
     *
     * ManagedPtr is a smart shared device pointer which also handles memory allocation.
     */
    template <class T>
    class ManagedPtr {
        static_assert(!std::is_const<T>::value && !std::is_volatile<T>::value, ""T cannot be cv-qualified"");
        static_assert(std::is_standard_layout<T>::value, ""T must satisfy StandardLayoutType"");

    public:
        using element_type = T;

        using pointer = DevicePtr<element_type>;
        using const_pointer = DevicePtr<typename std::add_const<element_type>::type>;

        using size_type = std::size_t;

        ManagedPtr() noexcept : wrapped{ nullptr }, n{ 0 }, capacity{ 0 } { }
        ManagedPtr(const ManagedPtr&) noexcept = default;
        ManagedPtr(ManagedPtr&& other) noexcept
            : wrapped{ std::move(other.wrapped) }, n{ other.n }, capacity { other.capacity }
    ...",1,src\cuda4dnn\csl\memory.hpp,cv,20,cv,1
357131,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    /* @brief smart device pointer with allocation/deallocation methods
     *
     * ManagedPtr is a smart shared device pointer which also handles memory allocation.
     */
    template <class T>
    class ManagedPtr {
        static_assert(!std::is_const<T>::value && !std::is_volatile<T>::value, ""T cannot be cv-qualified"");
        static_assert(std::is_standard_layout<T>::value, ""T must satisfy StandardLayoutType"");

    public:
        using element_type = T;

        using pointer = DevicePtr<element_type>;
        using const_pointer = DevicePtr<typename std::add_const<element_type>::type>;

        using size_type = std::size_t;

        ManagedPtr() noexcept : wrapped{ nullptr }, n{ 0 }, capacity{ 0 } { }
        ManagedPtr(const ManagedPtr&) noexcept = default;
        ManagedPtr(ManagedPtr&& other) noexcept
            : wrapped{ std::move(other.wrapped) }, n{ other.n }, capacity { other.capacity }
        {
         ...",16,src\cuda4dnn\csl\memory.hpp,cv.dnn,20,dnn,1
357132,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    /* @brief smart device pointer with allocation/deallocation methods
     *
     * ManagedPtr is a smart shared device pointer which also handles memory allocation.
     */
    template <class T>
    class ManagedPtr {
        static_assert(!std::is_const<T>::value && !std::is_volatile<T>::value, ""T cannot be cv-qualified"");
        static_assert(std::is_standard_layout<T>::value, ""T must satisfy StandardLayoutType"");

    public:
        using element_type = T;

        using pointer = DevicePtr<element_type>;
        using const_pointer = DevicePtr<typename std::add_const<element_type>::type>;

        using size_type = std::size_t;

        ManagedPtr() noexcept : wrapped{ nullptr }, n{ 0 }, capacity{ 0 } { }
        ManagedPtr(const ManagedPtr&) noexcept = default;
        ManagedPtr(ManagedPtr&& other) noexcept
            : wrapped{ std::move(other.wrapped) }, n{ other.n }, capacity { other.capacity }
        {
            other.reset()...",32,src\cuda4dnn\csl\memory.hpp,cv.dnn.cuda4dnn,20,cuda4dnn,1
357133,NAMESPACE_BLOCK,"namespace csl {

    /* @brief smart device pointer with allocation/deallocation methods
     *
     * ManagedPtr is a smart shared device pointer which also handles memory allocation.
     */
    template <class T>
    class ManagedPtr {
        static_assert(!std::is_const<T>::value && !std::is_volatile<T>::value, ""T cannot be cv-qualified"");
        static_assert(std::is_standard_layout<T>::value, ""T must satisfy StandardLayoutType"");

    public:
        using element_type = T;

        using pointer = DevicePtr<element_type>;
        using const_pointer = DevicePtr<typename std::add_const<element_type>::type>;

        using size_type = std::size_t;

        ManagedPtr() noexcept : wrapped{ nullptr }, n{ 0 }, capacity{ 0 } { }
        ManagedPtr(const ManagedPtr&) noexcept = default;
        ManagedPtr(ManagedPtr&& other) noexcept
            : wrapped{ std::move(other.wrapped) }, n{ other.n }, capacity { other.capacity }
        {
            other.reset();
        }

        ...",53,src\cuda4dnn\csl\memory.hpp,cv.dnn.cuda4dnn.csl,20,csl,1
357713,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\nvcc_defs.hpp,src\cuda4dnn\csl\nvcc_defs.hpp:<global>,,<global>,1
357734,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\pointer.hpp,src\cuda4dnn\csl\pointer.hpp:<global>,,<global>,1
357738,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    /** @brief provides a type-safe device pointer
     *
     * DevicePtr wraps a raw pointer and mimics its behaviour. It does not implicitly convert
     * to a raw pointer. This ensures that accidental mixing of host and device pointers do not happen.
     *
     * It is meant to point to locations in device memory. Hence, it provides dereferencing or
     * array subscript capability for device code only.
     *
     * A `const DevicePtr<T>` represents an immutable pointer to a mutable memory.
     * A `DevicePtr<const T>` represents a mutable pointer to an immutable memory.
     * A `const DevicePtr<const T>` represents an immutable pointer to an immutable memory.
     *
     * A `DevicePtr<T>` can implicitly convert to `DevicePtr<const T>`.
     *
     * Specializations:
     * - DevicePtr<void>/DevicePtr<const void> do not support pointer arithmetic (but relational operators are provided)
     * - any devic...",1,src\cuda4dnn\csl\pointer.hpp,cv,20,cv,1
357739,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    /** @brief provides a type-safe device pointer
     *
     * DevicePtr wraps a raw pointer and mimics its behaviour. It does not implicitly convert
     * to a raw pointer. This ensures that accidental mixing of host and device pointers do not happen.
     *
     * It is meant to point to locations in device memory. Hence, it provides dereferencing or
     * array subscript capability for device code only.
     *
     * A `const DevicePtr<T>` represents an immutable pointer to a mutable memory.
     * A `DevicePtr<const T>` represents a mutable pointer to an immutable memory.
     * A `const DevicePtr<const T>` represents an immutable pointer to an immutable memory.
     *
     * A `DevicePtr<T>` can implicitly convert to `DevicePtr<const T>`.
     *
     * Specializations:
     * - DevicePtr<void>/DevicePtr<const void> do not support pointer arithmetic (but relational operators are provided)
     * - any device pointer point...",16,src\cuda4dnn\csl\pointer.hpp,cv.dnn,20,dnn,1
357740,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    /** @brief provides a type-safe device pointer
     *
     * DevicePtr wraps a raw pointer and mimics its behaviour. It does not implicitly convert
     * to a raw pointer. This ensures that accidental mixing of host and device pointers do not happen.
     *
     * It is meant to point to locations in device memory. Hence, it provides dereferencing or
     * array subscript capability for device code only.
     *
     * A `const DevicePtr<T>` represents an immutable pointer to a mutable memory.
     * A `DevicePtr<const T>` represents a mutable pointer to an immutable memory.
     * A `const DevicePtr<const T>` represents an immutable pointer to an immutable memory.
     *
     * A `DevicePtr<T>` can implicitly convert to `DevicePtr<const T>`.
     *
     * Specializations:
     * - DevicePtr<void>/DevicePtr<const void> do not support pointer arithmetic (but relational operators are provided)
     * - any device pointer pointing to mutable m...",32,src\cuda4dnn\csl\pointer.hpp,cv.dnn.cuda4dnn,20,cuda4dnn,1
357741,NAMESPACE_BLOCK,"namespace csl {

    /** @brief provides a type-safe device pointer
     *
     * DevicePtr wraps a raw pointer and mimics its behaviour. It does not implicitly convert
     * to a raw pointer. This ensures that accidental mixing of host and device pointers do not happen.
     *
     * It is meant to point to locations in device memory. Hence, it provides dereferencing or
     * array subscript capability for device code only.
     *
     * A `const DevicePtr<T>` represents an immutable pointer to a mutable memory.
     * A `DevicePtr<const T>` represents a mutable pointer to an immutable memory.
     * A `const DevicePtr<const T>` represents an immutable pointer to an immutable memory.
     *
     * A `DevicePtr<T>` can implicitly convert to `DevicePtr<const T>`.
     *
     * Specializations:
     * - DevicePtr<void>/DevicePtr<const void> do not support pointer arithmetic (but relational operators are provided)
     * - any device pointer pointing to mutable memory is implicitly c...",53,src\cuda4dnn\csl\pointer.hpp,cv.dnn.cuda4dnn.csl,20,csl,1
358770,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\span.hpp,src\cuda4dnn\csl\span.hpp:<global>,,<global>,1
358774,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    /** @brief provides non-owning mutable access for device arrays
     *
     *  const Span<T>/Span<T> provides mutable access to the elements unless T is const qualified
     *  const Span<T> makes the span immutable but not the elements
     */
    template <class T>
    class Span {
        static_assert(std::is_standard_layout<T>::value, ""T must satisfy StandardLayoutType"");

    public:
        using value_type = T;
        using size_type = device::size_type;
        using index_type = device::index_type;

        using pointer = DevicePtr<value_type>;
        using const_pointer = DevicePtr<typename std::add_const<value_type>::type>;
        using reference = typename std::add_lvalue_reference<value_type>::type;
        using const_reference = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>;

        Span() noexcept : ptr{ nullptr }, sz{ 0 } { }
        CUDA4DNN_HOST_DEVICE Sp...",1,src\cuda4dnn\csl\span.hpp,cv,16,cv,1
358775,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    /** @brief provides non-owning mutable access for device arrays
     *
     *  const Span<T>/Span<T> provides mutable access to the elements unless T is const qualified
     *  const Span<T> makes the span immutable but not the elements
     */
    template <class T>
    class Span {
        static_assert(std::is_standard_layout<T>::value, ""T must satisfy StandardLayoutType"");

    public:
        using value_type = T;
        using size_type = device::size_type;
        using index_type = device::index_type;

        using pointer = DevicePtr<value_type>;
        using const_pointer = DevicePtr<typename std::add_const<value_type>::type>;
        using reference = typename std::add_lvalue_reference<value_type>::type;
        using const_reference = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>;

        Span() noexcept : ptr{ nullptr }, sz{ 0 } { }
        CUDA4DNN_HOST_DEVICE Span(pointer firs...",16,src\cuda4dnn\csl\span.hpp,cv.dnn,16,dnn,1
358776,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    /** @brief provides non-owning mutable access for device arrays
     *
     *  const Span<T>/Span<T> provides mutable access to the elements unless T is const qualified
     *  const Span<T> makes the span immutable but not the elements
     */
    template <class T>
    class Span {
        static_assert(std::is_standard_layout<T>::value, ""T must satisfy StandardLayoutType"");

    public:
        using value_type = T;
        using size_type = device::size_type;
        using index_type = device::index_type;

        using pointer = DevicePtr<value_type>;
        using const_pointer = DevicePtr<typename std::add_const<value_type>::type>;
        using reference = typename std::add_lvalue_reference<value_type>::type;
        using const_reference = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>;

        Span() noexcept : ptr{ nullptr }, sz{ 0 } { }
        CUDA4DNN_HOST_DEVICE Span(pointer first, pointer last)...",32,src\cuda4dnn\csl\span.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
358777,NAMESPACE_BLOCK,"namespace csl {

    /** @brief provides non-owning mutable access for device arrays
     *
     *  const Span<T>/Span<T> provides mutable access to the elements unless T is const qualified
     *  const Span<T> makes the span immutable but not the elements
     */
    template <class T>
    class Span {
        static_assert(std::is_standard_layout<T>::value, ""T must satisfy StandardLayoutType"");

    public:
        using value_type = T;
        using size_type = device::size_type;
        using index_type = device::index_type;

        using pointer = DevicePtr<value_type>;
        using const_pointer = DevicePtr<typename std::add_const<value_type>::type>;
        using reference = typename std::add_lvalue_reference<value_type>::type;
        using const_reference = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>;

        Span() noexcept : ptr{ nullptr }, sz{ 0 } { }
        CUDA4DNN_HOST_DEVICE Span(pointer first, pointer last) noexcept : ptr{ firs...",53,src\cuda4dnn\csl\span.hpp,cv.dnn.cuda4dnn.csl,16,csl,1
358918,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\stream.hpp,src\cuda4dnn\csl\stream.hpp:<global>,,<global>,1
358922,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    /** \file stream.hpp
     *
     * Default streams are not supported as they limit flexiblity. All operations are always
     * carried out in non-default streams in the CUDA backend. The stream classes sacrifice
     * the ability to support default streams in exchange for better error detection. That is,
     * a default constructed stream represents no stream and any attempt to use it will throw an
     * exception.
     */

    /** @brief non-copyable smart CUDA stream
     *
     * UniqueStream is a smart non-sharable wrapper for CUDA stream handle which ensures that
     * the handle is destroyed after use. Unless explicitly specified by a constructor argument,
     * the stream object does not represent any stream by default.
     */
    class UniqueStream {
    public:
        UniqueStream() noexcept : stream{ 0 } { }
        UniqueStream(UniqueStream&) = delete;
        UniqueStream(UniqueStream&& othe...",1,src\cuda4dnn\csl\stream.hpp,cv,19,cv,1
358923,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    /** \file stream.hpp
     *
     * Default streams are not supported as they limit flexiblity. All operations are always
     * carried out in non-default streams in the CUDA backend. The stream classes sacrifice
     * the ability to support default streams in exchange for better error detection. That is,
     * a default constructed stream represents no stream and any attempt to use it will throw an
     * exception.
     */

    /** @brief non-copyable smart CUDA stream
     *
     * UniqueStream is a smart non-sharable wrapper for CUDA stream handle which ensures that
     * the handle is destroyed after use. Unless explicitly specified by a constructor argument,
     * the stream object does not represent any stream by default.
     */
    class UniqueStream {
    public:
        UniqueStream() noexcept : stream{ 0 } { }
        UniqueStream(UniqueStream&) = delete;
        UniqueStream(UniqueStream&& other) noexcept {
 ...",16,src\cuda4dnn\csl\stream.hpp,cv.dnn,19,dnn,1
358924,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    /** \file stream.hpp
     *
     * Default streams are not supported as they limit flexiblity. All operations are always
     * carried out in non-default streams in the CUDA backend. The stream classes sacrifice
     * the ability to support default streams in exchange for better error detection. That is,
     * a default constructed stream represents no stream and any attempt to use it will throw an
     * exception.
     */

    /** @brief non-copyable smart CUDA stream
     *
     * UniqueStream is a smart non-sharable wrapper for CUDA stream handle which ensures that
     * the handle is destroyed after use. Unless explicitly specified by a constructor argument,
     * the stream object does not represent any stream by default.
     */
    class UniqueStream {
    public:
        UniqueStream() noexcept : stream{ 0 } { }
        UniqueStream(UniqueStream&) = delete;
        UniqueStream(UniqueStream&& other) noexcept {
            strea...",32,src\cuda4dnn\csl\stream.hpp,cv.dnn.cuda4dnn,19,cuda4dnn,1
358925,NAMESPACE_BLOCK,"namespace csl {

    /** \file stream.hpp
     *
     * Default streams are not supported as they limit flexiblity. All operations are always
     * carried out in non-default streams in the CUDA backend. The stream classes sacrifice
     * the ability to support default streams in exchange for better error detection. That is,
     * a default constructed stream represents no stream and any attempt to use it will throw an
     * exception.
     */

    /** @brief non-copyable smart CUDA stream
     *
     * UniqueStream is a smart non-sharable wrapper for CUDA stream handle which ensures that
     * the handle is destroyed after use. Unless explicitly specified by a constructor argument,
     * the stream object does not represent any stream by default.
     */
    class UniqueStream {
    public:
        UniqueStream() noexcept : stream{ 0 } { }
        UniqueStream(UniqueStream&) = delete;
        UniqueStream(UniqueStream&& other) noexcept {
            stream = other.stream;
   ...",53,src\cuda4dnn\csl\stream.hpp,cv.dnn.cuda4dnn.csl,19,csl,1
359406,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\tensor.hpp,src\cuda4dnn\csl\tensor.hpp:<global>,,<global>,1
359410,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    /** \file tensor.hpp
     *
     *     TYPE     | OWNERSHIP | MUTABLE
     * ------------ + --------- + --------
     *    Tensor    |    Yes    |   Yes
     *  TensorSpan  |    No     |   Yes
     *  TensorView  |    No     |   No
     *
     * Tensor is implicitly convertible to TensorSpan and TensorView
     * TensorSpan is implicitly convertible to TensorView
     *
     * Concepts and template parameter naming convention:
     * - ""MutableTensorType"" can refer to a Tensor or TensorSpan
     * - ""ImmutableTensorType"" can refer to a Tensor, TensorSpan or TensorView
     * - ""TensorType"" can refer to a Tensor, TensorSpan or TensorView
     *
     * ""ImmutableTensorType"" is used when the tensor data might be used.
     * ""TensorType"" is used when only meta-information such as the size or shape is required, i.e. the data won't be touched
     */

    /** if the \p axis is a negative index, the equivalent positi...",1,src\cuda4dnn\csl\tensor.hpp,cv,34,cv,1
359411,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    /** \file tensor.hpp
     *
     *     TYPE     | OWNERSHIP | MUTABLE
     * ------------ + --------- + --------
     *    Tensor    |    Yes    |   Yes
     *  TensorSpan  |    No     |   Yes
     *  TensorView  |    No     |   No
     *
     * Tensor is implicitly convertible to TensorSpan and TensorView
     * TensorSpan is implicitly convertible to TensorView
     *
     * Concepts and template parameter naming convention:
     * - ""MutableTensorType"" can refer to a Tensor or TensorSpan
     * - ""ImmutableTensorType"" can refer to a Tensor, TensorSpan or TensorView
     * - ""TensorType"" can refer to a Tensor, TensorSpan or TensorView
     *
     * ""ImmutableTensorType"" is used when the tensor data might be used.
     * ""TensorType"" is used when only meta-information such as the size or shape is required, i.e. the data won't be touched
     */

    /** if the \p axis is a negative index, the equivalent positive index is ret...",16,src\cuda4dnn\csl\tensor.hpp,cv.dnn,34,dnn,1
359412,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    /** \file tensor.hpp
     *
     *     TYPE     | OWNERSHIP | MUTABLE
     * ------------ + --------- + --------
     *    Tensor    |    Yes    |   Yes
     *  TensorSpan  |    No     |   Yes
     *  TensorView  |    No     |   No
     *
     * Tensor is implicitly convertible to TensorSpan and TensorView
     * TensorSpan is implicitly convertible to TensorView
     *
     * Concepts and template parameter naming convention:
     * - ""MutableTensorType"" can refer to a Tensor or TensorSpan
     * - ""ImmutableTensorType"" can refer to a Tensor, TensorSpan or TensorView
     * - ""TensorType"" can refer to a Tensor, TensorSpan or TensorView
     *
     * ""ImmutableTensorType"" is used when the tensor data might be used.
     * ""TensorType"" is used when only meta-information such as the size or shape is required, i.e. the data won't be touched
     */

    /** if the \p axis is a negative index, the equivalent positive index is returned; otherwise...",32,src\cuda4dnn\csl\tensor.hpp,cv.dnn.cuda4dnn,34,cuda4dnn,1
359413,NAMESPACE_BLOCK,"namespace csl {

    /** \file tensor.hpp
     *
     *     TYPE     | OWNERSHIP | MUTABLE
     * ------------ + --------- + --------
     *    Tensor    |    Yes    |   Yes
     *  TensorSpan  |    No     |   Yes
     *  TensorView  |    No     |   No
     *
     * Tensor is implicitly convertible to TensorSpan and TensorView
     * TensorSpan is implicitly convertible to TensorView
     *
     * Concepts and template parameter naming convention:
     * - ""MutableTensorType"" can refer to a Tensor or TensorSpan
     * - ""ImmutableTensorType"" can refer to a Tensor, TensorSpan or TensorView
     * - ""TensorType"" can refer to a Tensor, TensorSpan or TensorView
     *
     * ""ImmutableTensorType"" is used when the tensor data might be used.
     * ""TensorType"" is used when only meta-information such as the size or shape is required, i.e. the data won't be touched
     */

    /** if the \p axis is a negative index, the equivalent positive index is returned; otherwise, returns \p axis */
...",53,src\cuda4dnn\csl\tensor.hpp,cv.dnn.cuda4dnn.csl,34,csl,1
362285,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\tensor_ops.hpp,src\cuda4dnn\csl\tensor_ops.hpp:<global>,,<global>,1
362289,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    namespace tensor_ops {

        /** @brief copies data between tensors
         *
         * Pre-conditions:
         * - \p dest and \p src must have the same shape
         *
         * Exception Guarantee: Basic
         */
        template <class T> inline
        void copy(const Stream& stream, TensorSpan<T> dest, TensorView<T> src) {
            CV_Assert(is_shape_same(dest, src));
            if (dest.get() != src.get())
                memcpy(dest.get(), src.get(), dest.size(), stream);
        }

        namespace detail {
            template <class T>
            void assertGEMMCompatiblity(const TensorSpan<T>& result, bool transa, const TensorView<T>& A, bool transb, const TensorView<T>& B) {
                /* check dimension requirements for matrix multiplication */
                if (!transa && !transb) {
                    CV_Assert(A.get_axis_size(-2) == result.get_axis_size(-2));
           ...",1,src\cuda4dnn\csl\tensor_ops.hpp,cv,30,cv,1
362290,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    namespace tensor_ops {

        /** @brief copies data between tensors
         *
         * Pre-conditions:
         * - \p dest and \p src must have the same shape
         *
         * Exception Guarantee: Basic
         */
        template <class T> inline
        void copy(const Stream& stream, TensorSpan<T> dest, TensorView<T> src) {
            CV_Assert(is_shape_same(dest, src));
            if (dest.get() != src.get())
                memcpy(dest.get(), src.get(), dest.size(), stream);
        }

        namespace detail {
            template <class T>
            void assertGEMMCompatiblity(const TensorSpan<T>& result, bool transa, const TensorView<T>& A, bool transb, const TensorView<T>& B) {
                /* check dimension requirements for matrix multiplication */
                if (!transa && !transb) {
                    CV_Assert(A.get_axis_size(-2) == result.get_axis_size(-2));
                    CV_Ass...",16,src\cuda4dnn\csl\tensor_ops.hpp,cv.dnn,30,dnn,1
362291,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    namespace tensor_ops {

        /** @brief copies data between tensors
         *
         * Pre-conditions:
         * - \p dest and \p src must have the same shape
         *
         * Exception Guarantee: Basic
         */
        template <class T> inline
        void copy(const Stream& stream, TensorSpan<T> dest, TensorView<T> src) {
            CV_Assert(is_shape_same(dest, src));
            if (dest.get() != src.get())
                memcpy(dest.get(), src.get(), dest.size(), stream);
        }

        namespace detail {
            template <class T>
            void assertGEMMCompatiblity(const TensorSpan<T>& result, bool transa, const TensorView<T>& A, bool transb, const TensorView<T>& B) {
                /* check dimension requirements for matrix multiplication */
                if (!transa && !transb) {
                    CV_Assert(A.get_axis_size(-2) == result.get_axis_size(-2));
                    CV_Assert(A.get_axis_s...",32,src\cuda4dnn\csl\tensor_ops.hpp,cv.dnn.cuda4dnn,30,cuda4dnn,1
362292,NAMESPACE_BLOCK,"namespace csl {

    namespace tensor_ops {

        /** @brief copies data between tensors
         *
         * Pre-conditions:
         * - \p dest and \p src must have the same shape
         *
         * Exception Guarantee: Basic
         */
        template <class T> inline
        void copy(const Stream& stream, TensorSpan<T> dest, TensorView<T> src) {
            CV_Assert(is_shape_same(dest, src));
            if (dest.get() != src.get())
                memcpy(dest.get(), src.get(), dest.size(), stream);
        }

        namespace detail {
            template <class T>
            void assertGEMMCompatiblity(const TensorSpan<T>& result, bool transa, const TensorView<T>& A, bool transb, const TensorView<T>& B) {
                /* check dimension requirements for matrix multiplication */
                if (!transa && !transb) {
                    CV_Assert(A.get_axis_size(-2) == result.get_axis_size(-2));
                    CV_Assert(A.get_axis_size(-1) == B.get_axis...",53,src\cuda4dnn\csl\tensor_ops.hpp,cv.dnn.cuda4dnn.csl,30,csl,1
362293,NAMESPACE_BLOCK,"namespace tensor_ops {

        /** @brief copies data between tensors
         *
         * Pre-conditions:
         * - \p dest and \p src must have the same shape
         *
         * Exception Guarantee: Basic
         */
        template <class T> inline
        void copy(const Stream& stream, TensorSpan<T> dest, TensorView<T> src) {
            CV_Assert(is_shape_same(dest, src));
            if (dest.get() != src.get())
                memcpy(dest.get(), src.get(), dest.size(), stream);
        }

        namespace detail {
            template <class T>
            void assertGEMMCompatiblity(const TensorSpan<T>& result, bool transa, const TensorView<T>& A, bool transb, const TensorView<T>& B) {
                /* check dimension requirements for matrix multiplication */
                if (!transa && !transb) {
                    CV_Assert(A.get_axis_size(-2) == result.get_axis_size(-2));
                    CV_Assert(A.get_axis_size(-1) == B.get_axis_size(-2));
         ...",5,src\cuda4dnn\csl\tensor_ops.hpp,cv.dnn.cuda4dnn.csl.tensor_ops,32,tensor_ops,1
362330,NAMESPACE_BLOCK,"namespace detail {
            template <class T>
            void assertGEMMCompatiblity(const TensorSpan<T>& result, bool transa, const TensorView<T>& A, bool transb, const TensorView<T>& B) {
                /* check dimension requirements for matrix multiplication */
                if (!transa && !transb) {
                    CV_Assert(A.get_axis_size(-2) == result.get_axis_size(-2));
                    CV_Assert(A.get_axis_size(-1) == B.get_axis_size(-2));
                    CV_Assert(B.get_axis_size(-1) == result.get_axis_size(-1));
                } else if (!transa && transb) {
                    CV_Assert(A.get_axis_size(-2) == result.get_axis_size(-2));
                    CV_Assert(A.get_axis_size(-1) == B.get_axis_size(-1));
                    CV_Assert(B.get_axis_size(-2) == result.get_axis_size(-1));
                } else if (transa && !transb) {
                    CV_Assert(A.get_axis_size(-1) == result.get_axis_size(-2));
                    CV_Assert(A.get_a...",9,src\cuda4dnn\csl\tensor_ops.hpp,cv.dnn.cuda4dnn.csl.tensor_ops.detail,48,detail,2
363943,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\csl\workspace.hpp,src\cuda4dnn\csl\workspace.hpp:<global>,,<global>,1
363947,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    /** @brief maintains a single block of reusable device memory
     *
     * Each Workspace object is intended to be used by a single entity at a time but by
     * different entities at different times. It maintains a single reusable block of memory which
     * is sufficient for the largest consumer.
     */
    class Workspace {
    public:

        /** @brief reserve \p bytes of memory */
        void require(std::size_t bytes) {
            if (bytes > ptr.size())
                ptr.reset(bytes);
        }

        /** @brief number of bytes reserved by the largest consumer */
        std::size_t size() const noexcept {
            return ptr.size();
        }

        /** @brief returns the pointer to the workspace memory */
        DevicePtr<unsigned char> get() {
            return ptr.get();
        }

    private:
        ManagedPtr<unsigned char> ptr;
    };

    /** used to compute total workspace s...",1,src\cuda4dnn\csl\workspace.hpp,cv,16,cv,1
363948,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    /** @brief maintains a single block of reusable device memory
     *
     * Each Workspace object is intended to be used by a single entity at a time but by
     * different entities at different times. It maintains a single reusable block of memory which
     * is sufficient for the largest consumer.
     */
    class Workspace {
    public:

        /** @brief reserve \p bytes of memory */
        void require(std::size_t bytes) {
            if (bytes > ptr.size())
                ptr.reset(bytes);
        }

        /** @brief number of bytes reserved by the largest consumer */
        std::size_t size() const noexcept {
            return ptr.size();
        }

        /** @brief returns the pointer to the workspace memory */
        DevicePtr<unsigned char> get() {
            return ptr.get();
        }

    private:
        ManagedPtr<unsigned char> ptr;
    };

    /** used to compute total workspace size from severa...",16,src\cuda4dnn\csl\workspace.hpp,cv.dnn,16,dnn,1
363949,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    /** @brief maintains a single block of reusable device memory
     *
     * Each Workspace object is intended to be used by a single entity at a time but by
     * different entities at different times. It maintains a single reusable block of memory which
     * is sufficient for the largest consumer.
     */
    class Workspace {
    public:

        /** @brief reserve \p bytes of memory */
        void require(std::size_t bytes) {
            if (bytes > ptr.size())
                ptr.reset(bytes);
        }

        /** @brief number of bytes reserved by the largest consumer */
        std::size_t size() const noexcept {
            return ptr.size();
        }

        /** @brief returns the pointer to the workspace memory */
        DevicePtr<unsigned char> get() {
            return ptr.get();
        }

    private:
        ManagedPtr<unsigned char> ptr;
    };

    /** used to compute total workspace size from several workspace requ...",32,src\cuda4dnn\csl\workspace.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
363950,NAMESPACE_BLOCK,"namespace csl {

    /** @brief maintains a single block of reusable device memory
     *
     * Each Workspace object is intended to be used by a single entity at a time but by
     * different entities at different times. It maintains a single reusable block of memory which
     * is sufficient for the largest consumer.
     */
    class Workspace {
    public:

        /** @brief reserve \p bytes of memory */
        void require(std::size_t bytes) {
            if (bytes > ptr.size())
                ptr.reset(bytes);
        }

        /** @brief number of bytes reserved by the largest consumer */
        std::size_t size() const noexcept {
            return ptr.size();
        }

        /** @brief returns the pointer to the workspace memory */
        DevicePtr<unsigned char> get() {
            return ptr.get();
        }

    private:
        ManagedPtr<unsigned char> ptr;
    };

    /** used to compute total workspace size from several workspace requests */
    class Wor...",53,src\cuda4dnn\csl\workspace.hpp,cv.dnn.cuda4dnn.csl,16,csl,1
364271,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\cxx_utils\is_iterator.hpp,src\cuda4dnn\cxx_utils\is_iterator.hpp:<global>,,<global>,1
364275,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace cxx_utils {

    namespace detail {
        template <class T, class Tag, class = void>
        struct is_iterator_helper : std::false_type {};

        template <class T, class Tag>
        struct is_iterator_helper<T, Tag,
                typename std::enable_if<std::is_base_of<Tag, typename std::iterator_traits<T>::iterator_category>::value, void>::type
            > : std::true_type {};
    }

    template <class T>
    using is_iterator = typename detail::is_iterator_helper<T, std::input_iterator_tag>;

    template <class T>
    using is_forward_iterator = typename detail::is_iterator_helper<T, std::forward_iterator_tag>;

}}}}",1,src\cuda4dnn\cxx_utils\is_iterator.hpp,cv,11,cv,1
364276,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace cxx_utils {

    namespace detail {
        template <class T, class Tag, class = void>
        struct is_iterator_helper : std::false_type {};

        template <class T, class Tag>
        struct is_iterator_helper<T, Tag,
                typename std::enable_if<std::is_base_of<Tag, typename std::iterator_traits<T>::iterator_category>::value, void>::type
            > : std::true_type {};
    }

    template <class T>
    using is_iterator = typename detail::is_iterator_helper<T, std::input_iterator_tag>;

    template <class T>
    using is_forward_iterator = typename detail::is_iterator_helper<T, std::forward_iterator_tag>;

}}}",16,src\cuda4dnn\cxx_utils\is_iterator.hpp,cv.dnn,11,dnn,1
364277,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace cxx_utils {

    namespace detail {
        template <class T, class Tag, class = void>
        struct is_iterator_helper : std::false_type {};

        template <class T, class Tag>
        struct is_iterator_helper<T, Tag,
                typename std::enable_if<std::is_base_of<Tag, typename std::iterator_traits<T>::iterator_category>::value, void>::type
            > : std::true_type {};
    }

    template <class T>
    using is_iterator = typename detail::is_iterator_helper<T, std::input_iterator_tag>;

    template <class T>
    using is_forward_iterator = typename detail::is_iterator_helper<T, std::forward_iterator_tag>;

}}",32,src\cuda4dnn\cxx_utils\is_iterator.hpp,cv.dnn.cuda4dnn,11,cuda4dnn,1
364278,NAMESPACE_BLOCK,"namespace cxx_utils {

    namespace detail {
        template <class T, class Tag, class = void>
        struct is_iterator_helper : std::false_type {};

        template <class T, class Tag>
        struct is_iterator_helper<T, Tag,
                typename std::enable_if<std::is_base_of<Tag, typename std::iterator_traits<T>::iterator_category>::value, void>::type
            > : std::true_type {};
    }

    template <class T>
    using is_iterator = typename detail::is_iterator_helper<T, std::input_iterator_tag>;

    template <class T>
    using is_forward_iterator = typename detail::is_iterator_helper<T, std::forward_iterator_tag>;

}",53,src\cuda4dnn\cxx_utils\is_iterator.hpp,cv.dnn.cuda4dnn.cxx_utils,11,cxx_utils,1
364279,NAMESPACE_BLOCK,"namespace detail {
        template <class T, class Tag, class = void>
        struct is_iterator_helper : std::false_type {};

        template <class T, class Tag>
        struct is_iterator_helper<T, Tag,
                typename std::enable_if<std::is_base_of<Tag, typename std::iterator_traits<T>::iterator_category>::value, void>::type
            > : std::true_type {};
    }",5,src\cuda4dnn\cxx_utils\is_iterator.hpp,cv.dnn.cuda4dnn.cxx_utils.detail,13,detail,1
364293,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\cxx_utils\resizable_static_array.hpp,src\cuda4dnn\cxx_utils\resizable_static_array.hpp:<global>,,<global>,1
364297,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace cxx_utils {

    template <class T, std::size_t maxN>
    class resizable_static_array {
        using container_type = std::array<T, maxN>;

    public:
        using value_type                = typename container_type::value_type;
        using size_type                 = typename container_type::size_type;
        using difference_type           = typename container_type::difference_type;
        using reference                 = typename container_type::reference;
        using const_reference           = typename container_type::const_reference;
        using pointer                   = typename container_type::pointer;
        using const_pointer             = typename container_type::const_pointer;
        using iterator                  = typename container_type::iterator;
        using const_iterator            = typename container_type::const_iterator;
        using reverse_iterator          = typename container...",1,src\cuda4dnn\cxx_utils\resizable_static_array.hpp,cv,13,cv,1
364298,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace cxx_utils {

    template <class T, std::size_t maxN>
    class resizable_static_array {
        using container_type = std::array<T, maxN>;

    public:
        using value_type                = typename container_type::value_type;
        using size_type                 = typename container_type::size_type;
        using difference_type           = typename container_type::difference_type;
        using reference                 = typename container_type::reference;
        using const_reference           = typename container_type::const_reference;
        using pointer                   = typename container_type::pointer;
        using const_pointer             = typename container_type::const_pointer;
        using iterator                  = typename container_type::iterator;
        using const_iterator            = typename container_type::const_iterator;
        using reverse_iterator          = typename container_type::reverse_...",16,src\cuda4dnn\cxx_utils\resizable_static_array.hpp,cv.dnn,13,dnn,1
364299,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace cxx_utils {

    template <class T, std::size_t maxN>
    class resizable_static_array {
        using container_type = std::array<T, maxN>;

    public:
        using value_type                = typename container_type::value_type;
        using size_type                 = typename container_type::size_type;
        using difference_type           = typename container_type::difference_type;
        using reference                 = typename container_type::reference;
        using const_reference           = typename container_type::const_reference;
        using pointer                   = typename container_type::pointer;
        using const_pointer             = typename container_type::const_pointer;
        using iterator                  = typename container_type::iterator;
        using const_iterator            = typename container_type::const_iterator;
        using reverse_iterator          = typename container_type::reverse_iterator;
      ...",32,src\cuda4dnn\cxx_utils\resizable_static_array.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
364300,NAMESPACE_BLOCK,"namespace cxx_utils {

    template <class T, std::size_t maxN>
    class resizable_static_array {
        using container_type = std::array<T, maxN>;

    public:
        using value_type                = typename container_type::value_type;
        using size_type                 = typename container_type::size_type;
        using difference_type           = typename container_type::difference_type;
        using reference                 = typename container_type::reference;
        using const_reference           = typename container_type::const_reference;
        using pointer                   = typename container_type::pointer;
        using const_pointer             = typename container_type::const_pointer;
        using iterator                  = typename container_type::iterator;
        using const_iterator            = typename container_type::const_iterator;
        using reverse_iterator          = typename container_type::reverse_iterator;
        using const_reverse...",53,src\cuda4dnn\cxx_utils\resizable_static_array.hpp,cv.dnn.cuda4dnn.cxx_utils,13,cxx_utils,1
364637,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\init.hpp,src\cuda4dnn\init.hpp:<global>,,<global>,1
364641,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    void checkVersions()
    {
        // https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#programming-model
        // cuDNN API Compatibility
        // Beginning in cuDNN 7, the binary compatibility of a patch and minor releases is maintained as follows:
        //     Any patch release x.y.z is forward or backward-compatible with applications built against another cuDNN patch release x.y.w (meaning, of the same major and minor version number, but having w!=z).
        //     cuDNN minor releases beginning with cuDNN 7 are binary backward-compatible with applications built against the same or earlier patch release (meaning, an application built against cuDNN 7.x is binary compatible with cuDNN library 7.y, where y>=x).
        //     Applications compiled with a cuDNN version 7.y are not guaranteed to work with 7.x release when y > x.
        auto cudnn_bversion = cudnnGetVersion();
        auto cudnn_major...",1,src\cuda4dnn\init.hpp,cv,16,cv,1
364642,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    void checkVersions()
    {
        // https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#programming-model
        // cuDNN API Compatibility
        // Beginning in cuDNN 7, the binary compatibility of a patch and minor releases is maintained as follows:
        //     Any patch release x.y.z is forward or backward-compatible with applications built against another cuDNN patch release x.y.w (meaning, of the same major and minor version number, but having w!=z).
        //     cuDNN minor releases beginning with cuDNN 7 are binary backward-compatible with applications built against the same or earlier patch release (meaning, an application built against cuDNN 7.x is binary compatible with cuDNN library 7.y, where y>=x).
        //     Applications compiled with a cuDNN version 7.y are not guaranteed to work with 7.x release when y > x.
        auto cudnn_bversion = cudnnGetVersion();
        auto cudnn_major_bversion = cud...",16,src\cuda4dnn\init.hpp,cv.dnn,16,dnn,1
364643,NAMESPACE_BLOCK,"namespace cuda4dnn {

    void checkVersions()
    {
        // https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#programming-model
        // cuDNN API Compatibility
        // Beginning in cuDNN 7, the binary compatibility of a patch and minor releases is maintained as follows:
        //     Any patch release x.y.z is forward or backward-compatible with applications built against another cuDNN patch release x.y.w (meaning, of the same major and minor version number, but having w!=z).
        //     cuDNN minor releases beginning with cuDNN 7 are binary backward-compatible with applications built against the same or earlier patch release (meaning, an application built against cuDNN 7.x is binary compatible with cuDNN library 7.y, where y>=x).
        //     Applications compiled with a cuDNN version 7.y are not guaranteed to work with 7.x release when y > x.
        auto cudnn_bversion = cudnnGetVersion();
        auto cudnn_major_bversion = cudnn_bversion / 10...",32,src\cuda4dnn\init.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
364946,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\activation_eltwise.hpp,src\cuda4dnn\kernels\activation_eltwise.hpp:<global>,,<global>,1
364950,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output) + eltwise */

    template <class T>
    void relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise, T slope);

    template <class T>
    void clipped_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void tanh_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void swish_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void mish_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void sigmoid_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::Vie...",1,src\cuda4dnn\kernels\activation_eltwise.hpp,cv,13,cv,1
364951,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output) + eltwise */

    template <class T>
    void relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise, T slope);

    template <class T>
    void clipped_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void tanh_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void swish_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void mish_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void sigmoid_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);
...",16,src\cuda4dnn\kernels\activation_eltwise.hpp,cv.dnn,13,dnn,1
364952,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output) + eltwise */

    template <class T>
    void relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise, T slope);

    template <class T>
    void clipped_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void tanh_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void swish_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void mish_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void sigmoid_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <c...",32,src\cuda4dnn\kernels\activation_eltwise.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
364953,NAMESPACE_BLOCK,"namespace kernels {

    /* inplace_output = activation(inplace_output) + eltwise */

    template <class T>
    void relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise, T slope);

    template <class T>
    void clipped_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void tanh_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void swish_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void mish_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void sigmoid_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, csl::View<T> eltwise);

    template <class T>
    void powe...",53,src\cuda4dnn\kernels\activation_eltwise.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
365016,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\activations.hpp,src\cuda4dnn\kernels\activations.hpp:<global>,,<global>,1
365020,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T slope);

    template <class T>
    void clipped_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T floor, T ceiling);

    template <class T>
    void axiswise_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, std::size_t inner_size, csl::View<T> slope);

    template <class T>
    void tanh(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void swish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void mish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void sigmoid(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void elu(const csl::Stream& stream, csl::Span<T> outpu...",1,src\cuda4dnn\kernels\activations.hpp,cv,13,cv,1
365021,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T slope);

    template <class T>
    void clipped_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T floor, T ceiling);

    template <class T>
    void axiswise_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, std::size_t inner_size, csl::View<T> slope);

    template <class T>
    void tanh(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void swish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void mish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void sigmoid(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void elu(const csl::Stream& stream, csl::Span<T> output, csl::View<T>...",16,src\cuda4dnn\kernels\activations.hpp,cv.dnn,13,dnn,1
365022,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T slope);

    template <class T>
    void clipped_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T floor, T ceiling);

    template <class T>
    void axiswise_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, std::size_t inner_size, csl::View<T> slope);

    template <class T>
    void tanh(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void swish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void mish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void sigmoid(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void elu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T alpha)...",32,src\cuda4dnn\kernels\activations.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
365023,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T slope);

    template <class T>
    void clipped_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T floor, T ceiling);

    template <class T>
    void axiswise_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, std::size_t inner_size, csl::View<T> slope);

    template <class T>
    void tanh(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void swish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void mish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void sigmoid(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

    template <class T>
    void elu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T alpha);

    template <clas...",53,src\cuda4dnn\kernels\activations.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
365337,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\bias_activation.hpp,src\cuda4dnn\kernels\bias_activation.hpp:<global>,,<global>,1
365341,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void biasN_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, T slope);

    template <class T>
    void biasN_clipped_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, T floor, T ceiling);

    template <class T>
    void biasN_tanh_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_swish_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_mish_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_sigmoid_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inne...",1,src\cuda4dnn\kernels\bias_activation.hpp,cv,13,cv,1
365342,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void biasN_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, T slope);

    template <class T>
    void biasN_clipped_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, T floor, T ceiling);

    template <class T>
    void biasN_tanh_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_swish_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_mish_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_sigmoid_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::Vi...",16,src\cuda4dnn\kernels\bias_activation.hpp,cv.dnn,13,dnn,1
365343,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void biasN_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, T slope);

    template <class T>
    void biasN_clipped_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, T floor, T ceiling);

    template <class T>
    void biasN_tanh_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_swish_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_mish_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_sigmoid_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

  ...",32,src\cuda4dnn\kernels\bias_activation.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
365344,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void biasN_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, T slope);

    template <class T>
    void biasN_clipped_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, T floor, T ceiling);

    template <class T>
    void biasN_tanh_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_swish_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_mish_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
    void biasN_sigmoid_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias);

    template <class T>
...",53,src\cuda4dnn\kernels\bias_activation.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
365414,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\bias_activation_eltwise.hpp,src\cuda4dnn\kernels\bias_activation_eltwise.hpp:<global>,,<global>,1
365418,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output + bias) + eltwise
     * broadcasting on `bias` is allowed but not on `eltwise`
     */

    template <class T>
    void biasN_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T slope);

    template <class T>
    void biasN_clipped_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void biasN_tanh_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_sigmoid_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltw...",1,src\cuda4dnn\kernels\bias_activation_eltwise.hpp,cv,13,cv,1
365419,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output + bias) + eltwise
     * broadcasting on `bias` is allowed but not on `eltwise`
     */

    template <class T>
    void biasN_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T slope);

    template <class T>
    void biasN_clipped_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void biasN_tanh_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_sigmoid_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    temp...",16,src\cuda4dnn\kernels\bias_activation_eltwise.hpp,cv.dnn,13,dnn,1
365420,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output + bias) + eltwise
     * broadcasting on `bias` is allowed but not on `eltwise`
     */

    template <class T>
    void biasN_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T slope);

    template <class T>
    void biasN_clipped_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void biasN_tanh_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_sigmoid_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
 ...",32,src\cuda4dnn\kernels\bias_activation_eltwise.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
365421,NAMESPACE_BLOCK,"namespace kernels {

    /* inplace_output = activation(inplace_output + bias) + eltwise
     * broadcasting on `bias` is allowed but not on `eltwise`
     */

    template <class T>
    void biasN_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T slope);

    template <class T>
    void biasN_clipped_relu_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void biasN_tanh_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_sigmoid_eltwise_sum_2_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_swish_e...",53,src\cuda4dnn\kernels\bias_activation_eltwise.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
365498,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\bias_eltwise_activation.hpp,src\cuda4dnn\kernels\bias_eltwise_activation.hpp:<global>,,<global>,1
365502,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output + bias + eltwise)
     * broadcasting on `bias` is allowed but not on `eltwise`
     */

    template <class T>
    void biasN_eltwise_sum_2_identity_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_eltwise_sum_2_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T slope);

    template <class T>
    void biasN_eltwise_sum_2_clipped_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void biasN_eltwise_sum_2_tanh_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> elt...",1,src\cuda4dnn\kernels\bias_eltwise_activation.hpp,cv,13,cv,1
365503,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output + bias + eltwise)
     * broadcasting on `bias` is allowed but not on `eltwise`
     */

    template <class T>
    void biasN_eltwise_sum_2_identity_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_eltwise_sum_2_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T slope);

    template <class T>
    void biasN_eltwise_sum_2_clipped_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void biasN_eltwise_sum_2_tanh_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    tem...",16,src\cuda4dnn\kernels\bias_eltwise_activation.hpp,cv.dnn,13,dnn,1
365504,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    /* inplace_output = activation(inplace_output + bias + eltwise)
     * broadcasting on `bias` is allowed but not on `eltwise`
     */

    template <class T>
    void biasN_eltwise_sum_2_identity_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_eltwise_sum_2_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T slope);

    template <class T>
    void biasN_eltwise_sum_2_clipped_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void biasN_eltwise_sum_2_tanh_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
...",32,src\cuda4dnn\kernels\bias_eltwise_activation.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
365505,NAMESPACE_BLOCK,"namespace kernels {

    /* inplace_output = activation(inplace_output + bias + eltwise)
     * broadcasting on `bias` is allowed but not on `eltwise`
     */

    template <class T>
    void biasN_eltwise_sum_2_identity_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_eltwise_sum_2_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T slope);

    template <class T>
    void biasN_eltwise_sum_2_clipped_relu_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise, T floor, T ceiling);

    template <class T>
    void biasN_eltwise_sum_2_tanh_inplace(const csl::Stream& stream, csl::Span<T> inplace_output, std::size_t inner_size, csl::View<T> bias, csl::View<T> eltwise);

    template <class T>
    void biasN_eltwis...",53,src\cuda4dnn\kernels\bias_eltwise_activation.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
365593,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\concat.hpp,src\cuda4dnn\kernels\concat.hpp:<global>,,<global>,1
365597,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void concat(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, std::size_t output_axis_offset,
        csl::TensorView<T> input, std::size_t axis);

    template <class T>
    void concat_with_offsets(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, std::vector<std::size_t> axis_offsets);

}}}}",1,src\cuda4dnn\kernels\concat.hpp,cv,14,cv,1
365598,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void concat(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, std::size_t output_axis_offset,
        csl::TensorView<T> input, std::size_t axis);

    template <class T>
    void concat_with_offsets(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, std::vector<std::size_t> axis_offsets);

}}}",16,src\cuda4dnn\kernels\concat.hpp,cv.dnn,14,dnn,1
365599,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void concat(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, std::size_t output_axis_offset,
        csl::TensorView<T> input, std::size_t axis);

    template <class T>
    void concat_with_offsets(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, std::vector<std::size_t> axis_offsets);

}}",32,src\cuda4dnn\kernels\concat.hpp,cv.dnn.cuda4dnn,14,cuda4dnn,1
365600,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void concat(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, std::size_t output_axis_offset,
        csl::TensorView<T> input, std::size_t axis);

    template <class T>
    void concat_with_offsets(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, std::vector<std::size_t> axis_offsets);

}",53,src\cuda4dnn\kernels\concat.hpp,cv.dnn.cuda4dnn.kernels,14,kernels,1
365625,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\crop_and_resize.hpp,src\cuda4dnn\kernels\crop_and_resize.hpp:<global>,,<global>,1
365629,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void crop_and_resize(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::View<T> boxes);

}}}}",1,src\cuda4dnn\kernels\crop_and_resize.hpp,cv,12,cv,1
365630,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void crop_and_resize(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::View<T> boxes);

}}}",16,src\cuda4dnn\kernels\crop_and_resize.hpp,cv.dnn,12,dnn,1
365631,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void crop_and_resize(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::View<T> boxes);

}}",32,src\cuda4dnn\kernels\crop_and_resize.hpp,cv.dnn.cuda4dnn,12,cuda4dnn,1
365632,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void crop_and_resize(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::View<T> boxes);

}",53,src\cuda4dnn\kernels\crop_and_resize.hpp,cv.dnn.cuda4dnn.kernels,12,kernels,1
365648,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\detection_output.hpp,src\cuda4dnn\kernels\detection_output.hpp:<global>,,<global>,1
365652,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void decode_bboxes(const csl::Stream& stream, csl::Span<T> output, csl::View<T> locations, csl::View<T> priors,
        std::size_t num_loc_classes, bool share_location, std::size_t background_label_id,
        bool transpose_location, bool variance_encoded_in_target,
        bool corner_true_or_center_false, bool normalized_bbox,
        bool clip_box, float clip_width, float clip_height);

    template <class T>
    void findTopK(const csl::Stream& stream, csl::TensorSpan<int> indices, csl::TensorSpan<int> count, csl::TensorView<T> scores, std::size_t background_label_id, float threshold);

    template <class T>
    void box_collect(const csl::Stream& stream, csl::TensorSpan<T> collected_bboxes, csl::TensorView<T> decoded_bboxes, csl::TensorView<int> indices, csl::TensorView<int> count, bool share_location, std::size_t background_label_id);

    template <class T>
    void blockwis...",1,src\cuda4dnn\kernels\detection_output.hpp,cv,12,cv,1
365653,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void decode_bboxes(const csl::Stream& stream, csl::Span<T> output, csl::View<T> locations, csl::View<T> priors,
        std::size_t num_loc_classes, bool share_location, std::size_t background_label_id,
        bool transpose_location, bool variance_encoded_in_target,
        bool corner_true_or_center_false, bool normalized_bbox,
        bool clip_box, float clip_width, float clip_height);

    template <class T>
    void findTopK(const csl::Stream& stream, csl::TensorSpan<int> indices, csl::TensorSpan<int> count, csl::TensorView<T> scores, std::size_t background_label_id, float threshold);

    template <class T>
    void box_collect(const csl::Stream& stream, csl::TensorSpan<T> collected_bboxes, csl::TensorView<T> decoded_bboxes, csl::TensorView<int> indices, csl::TensorView<int> count, bool share_location, std::size_t background_label_id);

    template <class T>
    void blockwise_class_nms(con...",16,src\cuda4dnn\kernels\detection_output.hpp,cv.dnn,12,dnn,1
365654,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void decode_bboxes(const csl::Stream& stream, csl::Span<T> output, csl::View<T> locations, csl::View<T> priors,
        std::size_t num_loc_classes, bool share_location, std::size_t background_label_id,
        bool transpose_location, bool variance_encoded_in_target,
        bool corner_true_or_center_false, bool normalized_bbox,
        bool clip_box, float clip_width, float clip_height);

    template <class T>
    void findTopK(const csl::Stream& stream, csl::TensorSpan<int> indices, csl::TensorSpan<int> count, csl::TensorView<T> scores, std::size_t background_label_id, float threshold);

    template <class T>
    void box_collect(const csl::Stream& stream, csl::TensorSpan<T> collected_bboxes, csl::TensorView<T> decoded_bboxes, csl::TensorView<int> indices, csl::TensorView<int> count, bool share_location, std::size_t background_label_id);

    template <class T>
    void blockwise_class_nms(const csl::Stream& ...",32,src\cuda4dnn\kernels\detection_output.hpp,cv.dnn.cuda4dnn,12,cuda4dnn,1
365655,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void decode_bboxes(const csl::Stream& stream, csl::Span<T> output, csl::View<T> locations, csl::View<T> priors,
        std::size_t num_loc_classes, bool share_location, std::size_t background_label_id,
        bool transpose_location, bool variance_encoded_in_target,
        bool corner_true_or_center_false, bool normalized_bbox,
        bool clip_box, float clip_width, float clip_height);

    template <class T>
    void findTopK(const csl::Stream& stream, csl::TensorSpan<int> indices, csl::TensorSpan<int> count, csl::TensorView<T> scores, std::size_t background_label_id, float threshold);

    template <class T>
    void box_collect(const csl::Stream& stream, csl::TensorSpan<T> collected_bboxes, csl::TensorView<T> decoded_bboxes, csl::TensorView<int> indices, csl::TensorView<int> count, bool share_location, std::size_t background_label_id);

    template <class T>
    void blockwise_class_nms(const csl::Stream& stream, csl::TensorSp...",53,src\cuda4dnn\kernels\detection_output.hpp,cv.dnn.cuda4dnn.kernels,12,kernels,1
365737,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\eltwise_activation.hpp,src\cuda4dnn\kernels\eltwise_activation.hpp:<global>,,<global>,1
365741,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    /* output = activation(x + y) */

    template <class T>
    void eltwise_sum_2_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y, T slope);

    template <class T>
    void eltwise_sum_2_clipped_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y, T floor, T ceiling);

    template <class T>
    void eltwise_sum_2_tanh(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_swish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_mish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_sigmoid(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_p...",1,src\cuda4dnn\kernels\eltwise_activation.hpp,cv,13,cv,1
365742,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    /* output = activation(x + y) */

    template <class T>
    void eltwise_sum_2_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y, T slope);

    template <class T>
    void eltwise_sum_2_clipped_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y, T floor, T ceiling);

    template <class T>
    void eltwise_sum_2_tanh(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_swish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_mish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_sigmoid(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_power(const csl:...",16,src\cuda4dnn\kernels\eltwise_activation.hpp,cv.dnn,13,dnn,1
365743,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    /* output = activation(x + y) */

    template <class T>
    void eltwise_sum_2_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y, T slope);

    template <class T>
    void eltwise_sum_2_clipped_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y, T floor, T ceiling);

    template <class T>
    void eltwise_sum_2_tanh(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_swish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_mish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_sigmoid(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_power(const csl::Stream& stream,...",32,src\cuda4dnn\kernels\eltwise_activation.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
365744,NAMESPACE_BLOCK,"namespace kernels {

    /* output = activation(x + y) */

    template <class T>
    void eltwise_sum_2_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y, T slope);

    template <class T>
    void eltwise_sum_2_clipped_relu(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y, T floor, T ceiling);

    template <class T>
    void eltwise_sum_2_tanh(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_swish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_mish(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_sigmoid(const csl::Stream& stream, csl::Span<T> output, csl::View<T> x, csl::View<T> y);

    template <class T>
    void eltwise_sum_2_power(const csl::Stream& stream, csl::Span<T> output,...",53,src\cuda4dnn\kernels\eltwise_activation.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
365814,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\eltwise_ops.hpp,src\cuda4dnn\kernels\eltwise_ops.hpp:<global>,,<global>,1
365818,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void eltwise_max_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_min_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sum_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sum_coeff_2(const csl::Stream& stream, csl::TensorSpan<T> output, T coeff_x, csl::TensorView<T> x, T coeff_y, csl::TensorView<T> y);

    template <class T>
    void eltwise_prod_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_div_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class...",1,src\cuda4dnn\kernels\eltwise_ops.hpp,cv,13,cv,1
365819,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void eltwise_max_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_min_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sum_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sum_coeff_2(const csl::Stream& stream, csl::TensorSpan<T> output, T coeff_x, csl::TensorView<T> x, T coeff_y, csl::TensorView<T> y);

    template <class T>
    void eltwise_prod_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_div_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void el...",16,src\cuda4dnn\kernels\eltwise_ops.hpp,cv.dnn,13,dnn,1
365820,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void eltwise_max_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_min_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sum_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sum_coeff_2(const csl::Stream& stream, csl::TensorSpan<T> output, T coeff_x, csl::TensorView<T> x, T coeff_y, csl::TensorView<T> y);

    template <class T>
    void eltwise_prod_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_div_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sub_2(cons...",32,src\cuda4dnn\kernels\eltwise_ops.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
365821,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void eltwise_max_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_min_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sum_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sum_coeff_2(const csl::Stream& stream, csl::TensorSpan<T> output, T coeff_x, csl::TensorView<T> x, T coeff_y, csl::TensorView<T> y);

    template <class T>
    void eltwise_prod_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_div_2(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> x, csl::TensorView<T> y);

    template <class T>
    void eltwise_sub_2(const csl::Stream& stream...",53,src\cuda4dnn\kernels\eltwise_ops.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
365885,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\fill_copy.hpp,src\cuda4dnn\kernels\fill_copy.hpp:<global>,,<global>,1
365889,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void fill(const csl::Stream& stream, csl::Span<T> output, T value);

    template <class T>
    void copy(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

}}}}",1,src\cuda4dnn\kernels\fill_copy.hpp,cv,11,cv,1
365890,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void fill(const csl::Stream& stream, csl::Span<T> output, T value);

    template <class T>
    void copy(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

}}}",16,src\cuda4dnn\kernels\fill_copy.hpp,cv.dnn,11,dnn,1
365891,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void fill(const csl::Stream& stream, csl::Span<T> output, T value);

    template <class T>
    void copy(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

}}",32,src\cuda4dnn\kernels\fill_copy.hpp,cv.dnn.cuda4dnn,11,cuda4dnn,1
365892,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void fill(const csl::Stream& stream, csl::Span<T> output, T value);

    template <class T>
    void copy(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input);

}",53,src\cuda4dnn\kernels\fill_copy.hpp,cv.dnn.cuda4dnn.kernels,11,kernels,1
365912,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\fp_conversion.hpp,src\cuda4dnn\kernels\fp_conversion.hpp:<global>,,<global>,1
365916,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    void fp32_to_fp16(const csl::Stream& stream, csl::Span<half> output, csl::View<float> input);
    void fp16_to_fp32(const csl::Stream& stream, csl::Span<float> output, csl::View<half> input);

}}}}",1,src\cuda4dnn\kernels\fp_conversion.hpp,cv,11,cv,1
365917,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    void fp32_to_fp16(const csl::Stream& stream, csl::Span<half> output, csl::View<float> input);
    void fp16_to_fp32(const csl::Stream& stream, csl::Span<float> output, csl::View<half> input);

}}}",16,src\cuda4dnn\kernels\fp_conversion.hpp,cv.dnn,11,dnn,1
365918,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    void fp32_to_fp16(const csl::Stream& stream, csl::Span<half> output, csl::View<float> input);
    void fp16_to_fp32(const csl::Stream& stream, csl::Span<float> output, csl::View<half> input);

}}",32,src\cuda4dnn\kernels\fp_conversion.hpp,cv.dnn.cuda4dnn,11,cuda4dnn,1
365919,NAMESPACE_BLOCK,"namespace kernels {

    void fp32_to_fp16(const csl::Stream& stream, csl::Span<half> output, csl::View<float> input);
    void fp16_to_fp32(const csl::Stream& stream, csl::Span<float> output, csl::View<half> input);

}",53,src\cuda4dnn\kernels\fp_conversion.hpp,cv.dnn.cuda4dnn.kernels,11,kernels,1
365941,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\grid_nms.hpp,src\cuda4dnn\kernels\grid_nms.hpp:<global>,,<global>,1
365945,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

std::size_t getGridNMSWorkspaceSizePerBatchItem(std::size_t num_classes, std::size_t classwise_topK);

template <class T>
void grid_nms(const csl::Stream& stream, csl::Span<unsigned int> workspace, csl::TensorSpan<int> indices, csl::TensorSpan<int> count, csl::TensorView<T> bboxes, int background_class_id, bool normalized_bbox, float nms_threshold);

}}}}",1,src\cuda4dnn\kernels\grid_nms.hpp,cv,12,cv,1
365946,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

std::size_t getGridNMSWorkspaceSizePerBatchItem(std::size_t num_classes, std::size_t classwise_topK);

template <class T>
void grid_nms(const csl::Stream& stream, csl::Span<unsigned int> workspace, csl::TensorSpan<int> indices, csl::TensorSpan<int> count, csl::TensorView<T> bboxes, int background_class_id, bool normalized_bbox, float nms_threshold);

}}}",16,src\cuda4dnn\kernels\grid_nms.hpp,cv.dnn,12,dnn,1
365947,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

std::size_t getGridNMSWorkspaceSizePerBatchItem(std::size_t num_classes, std::size_t classwise_topK);

template <class T>
void grid_nms(const csl::Stream& stream, csl::Span<unsigned int> workspace, csl::TensorSpan<int> indices, csl::TensorSpan<int> count, csl::TensorView<T> bboxes, int background_class_id, bool normalized_bbox, float nms_threshold);

}}",32,src\cuda4dnn\kernels\grid_nms.hpp,cv.dnn.cuda4dnn,12,cuda4dnn,1
365948,NAMESPACE_BLOCK,"namespace kernels {

std::size_t getGridNMSWorkspaceSizePerBatchItem(std::size_t num_classes, std::size_t classwise_topK);

template <class T>
void grid_nms(const csl::Stream& stream, csl::Span<unsigned int> workspace, csl::TensorSpan<int> indices, csl::TensorSpan<int> count, csl::TensorView<T> bboxes, int background_class_id, bool normalized_bbox, float nms_threshold);

}",53,src\cuda4dnn\kernels\grid_nms.hpp,cv.dnn.cuda4dnn.kernels,12,kernels,1
365976,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\max_unpooling.hpp,src\cuda4dnn\kernels\max_unpooling.hpp:<global>,,<global>,1
365980,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void max_pooling_with_indices(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorSpan<T> indices, csl::TensorView<T> input,
        const std::vector<std::size_t>& kernel_size, const std::vector<std::size_t>& strides,
        const std::vector<std::size_t>& padding_left);

    template <class T>
    void max_unpooling(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input, csl::TensorView<T> indices,
        const std::vector<std::size_t>& window_size, const std::vector<std::size_t>& strides,
        const std::vector<std::size_t>& padding_left);

}}}}",1,src\cuda4dnn\kernels\max_unpooling.hpp,cv,14,cv,1
365981,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void max_pooling_with_indices(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorSpan<T> indices, csl::TensorView<T> input,
        const std::vector<std::size_t>& kernel_size, const std::vector<std::size_t>& strides,
        const std::vector<std::size_t>& padding_left);

    template <class T>
    void max_unpooling(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input, csl::TensorView<T> indices,
        const std::vector<std::size_t>& window_size, const std::vector<std::size_t>& strides,
        const std::vector<std::size_t>& padding_left);

}}}",16,src\cuda4dnn\kernels\max_unpooling.hpp,cv.dnn,14,dnn,1
365982,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void max_pooling_with_indices(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorSpan<T> indices, csl::TensorView<T> input,
        const std::vector<std::size_t>& kernel_size, const std::vector<std::size_t>& strides,
        const std::vector<std::size_t>& padding_left);

    template <class T>
    void max_unpooling(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input, csl::TensorView<T> indices,
        const std::vector<std::size_t>& window_size, const std::vector<std::size_t>& strides,
        const std::vector<std::size_t>& padding_left);

}}",32,src\cuda4dnn\kernels\max_unpooling.hpp,cv.dnn.cuda4dnn,14,cuda4dnn,1
365983,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void max_pooling_with_indices(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorSpan<T> indices, csl::TensorView<T> input,
        const std::vector<std::size_t>& kernel_size, const std::vector<std::size_t>& strides,
        const std::vector<std::size_t>& padding_left);

    template <class T>
    void max_unpooling(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input, csl::TensorView<T> indices,
        const std::vector<std::size_t>& window_size, const std::vector<std::size_t>& strides,
        const std::vector<std::size_t>& padding_left);

}",53,src\cuda4dnn\kernels\max_unpooling.hpp,cv.dnn.cuda4dnn.kernels,14,kernels,1
366013,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\mvn.hpp,src\cuda4dnn\kernels\mvn.hpp:<global>,,<global>,1
366017,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

template <class T>
void reduce_mean(const csl::Stream& stream, csl::Span<float> means, csl::View<T> input, std::size_t inner_size);

template <class T>
void reduce_mean_sqr_sum(const csl::Stream& stream, csl::Span<float> means, csl::Span<float> sum_sqrs, csl::View<T> input, std::size_t inner_size);

void compute_normalization_scale(const csl::Stream& stream, csl::Span<float> scale, csl::View<float> means, csl::View<float> sum_sqrs, std::size_t inner_size, float eps);

template <class T>
void normalize_mean(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<float> means, std::size_t inner_size);

template <class T>
void normalize_mean_variance(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<float> means, csl::View<float> scale, std::size_t inner_size);

template <class T>
void normalize_mean_variance_channelwise(const csl::Stream &stream, csl::Span<T> outp...",1,src\cuda4dnn\kernels\mvn.hpp,cv,13,cv,1
366018,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

template <class T>
void reduce_mean(const csl::Stream& stream, csl::Span<float> means, csl::View<T> input, std::size_t inner_size);

template <class T>
void reduce_mean_sqr_sum(const csl::Stream& stream, csl::Span<float> means, csl::Span<float> sum_sqrs, csl::View<T> input, std::size_t inner_size);

void compute_normalization_scale(const csl::Stream& stream, csl::Span<float> scale, csl::View<float> means, csl::View<float> sum_sqrs, std::size_t inner_size, float eps);

template <class T>
void normalize_mean(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<float> means, std::size_t inner_size);

template <class T>
void normalize_mean_variance(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<float> means, csl::View<float> scale, std::size_t inner_size);

template <class T>
void normalize_mean_variance_channelwise(const csl::Stream &stream, csl::Span<T> output, csl::View<T...",16,src\cuda4dnn\kernels\mvn.hpp,cv.dnn,13,dnn,1
366019,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

template <class T>
void reduce_mean(const csl::Stream& stream, csl::Span<float> means, csl::View<T> input, std::size_t inner_size);

template <class T>
void reduce_mean_sqr_sum(const csl::Stream& stream, csl::Span<float> means, csl::Span<float> sum_sqrs, csl::View<T> input, std::size_t inner_size);

void compute_normalization_scale(const csl::Stream& stream, csl::Span<float> scale, csl::View<float> means, csl::View<float> sum_sqrs, std::size_t inner_size, float eps);

template <class T>
void normalize_mean(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<float> means, std::size_t inner_size);

template <class T>
void normalize_mean_variance(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<float> means, csl::View<float> scale, std::size_t inner_size);

template <class T>
void normalize_mean_variance_channelwise(const csl::Stream &stream, csl::Span<T> output, csl::View<T> input, csl::Vi...",32,src\cuda4dnn\kernels\mvn.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
366020,NAMESPACE_BLOCK,"namespace kernels {

template <class T>
void reduce_mean(const csl::Stream& stream, csl::Span<float> means, csl::View<T> input, std::size_t inner_size);

template <class T>
void reduce_mean_sqr_sum(const csl::Stream& stream, csl::Span<float> means, csl::Span<float> sum_sqrs, csl::View<T> input, std::size_t inner_size);

void compute_normalization_scale(const csl::Stream& stream, csl::Span<float> scale, csl::View<float> means, csl::View<float> sum_sqrs, std::size_t inner_size, float eps);

template <class T>
void normalize_mean(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<float> means, std::size_t inner_size);

template <class T>
void normalize_mean_variance(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<float> means, csl::View<float> scale, std::size_t inner_size);

template <class T>
void normalize_mean_variance_channelwise(const csl::Stream &stream, csl::Span<T> output, csl::View<T> input, csl::View<T> scale, csl::Vie...",53,src\cuda4dnn\kernels\mvn.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
366110,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\normalize.hpp,src\cuda4dnn\kernels\normalize.hpp:<global>,,<global>,1
366114,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void normalize(
        const csl::Stream& stream,
        csl::Span<T> output, csl::View<T> input,
        std::size_t outer_size, std::size_t mid_size, std::size_t inner_size, std::size_t norm, T epsilon,
        csl::Span<T> workspace);

}}}}",1,src\cuda4dnn\kernels\normalize.hpp,cv,13,cv,1
366115,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void normalize(
        const csl::Stream& stream,
        csl::Span<T> output, csl::View<T> input,
        std::size_t outer_size, std::size_t mid_size, std::size_t inner_size, std::size_t norm, T epsilon,
        csl::Span<T> workspace);

}}}",16,src\cuda4dnn\kernels\normalize.hpp,cv.dnn,13,dnn,1
366116,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void normalize(
        const csl::Stream& stream,
        csl::Span<T> output, csl::View<T> input,
        std::size_t outer_size, std::size_t mid_size, std::size_t inner_size, std::size_t norm, T epsilon,
        csl::Span<T> workspace);

}}",32,src\cuda4dnn\kernels\normalize.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
366117,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void normalize(
        const csl::Stream& stream,
        csl::Span<T> output, csl::View<T> input,
        std::size_t outer_size, std::size_t mid_size, std::size_t inner_size, std::size_t norm, T epsilon,
        csl::Span<T> workspace);

}",53,src\cuda4dnn\kernels\normalize.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
366142,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\padding.hpp,src\cuda4dnn\kernels\padding.hpp:<global>,,<global>,1
366146,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void copy_with_reflection101(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input,
        std::vector<std::pair<std::size_t, std::size_t>> ranges);

}}}}",1,src\cuda4dnn\kernels\padding.hpp,cv,15,cv,1
366147,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void copy_with_reflection101(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input,
        std::vector<std::pair<std::size_t, std::size_t>> ranges);

}}}",16,src\cuda4dnn\kernels\padding.hpp,cv.dnn,15,dnn,1
366148,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void copy_with_reflection101(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input,
        std::vector<std::pair<std::size_t, std::size_t>> ranges);

}}",32,src\cuda4dnn\kernels\padding.hpp,cv.dnn.cuda4dnn,15,cuda4dnn,1
366149,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void copy_with_reflection101(
        const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input,
        std::vector<std::pair<std::size_t, std::size_t>> ranges);

}",53,src\cuda4dnn\kernels\padding.hpp,cv.dnn.cuda4dnn.kernels,15,kernels,1
366167,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\permute.hpp,src\cuda4dnn\kernels\permute.hpp:<global>,,<global>,1
366171,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void permute(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, std::vector<std::size_t> order);

    template <class T>
    void transpose(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, std::size_t in_width, std::size_t out_width);

}}}}",1,src\cuda4dnn\kernels\permute.hpp,cv,14,cv,1
366172,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void permute(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, std::vector<std::size_t> order);

    template <class T>
    void transpose(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, std::size_t in_width, std::size_t out_width);

}}}",16,src\cuda4dnn\kernels\permute.hpp,cv.dnn,14,dnn,1
366173,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void permute(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, std::vector<std::size_t> order);

    template <class T>
    void transpose(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, std::size_t in_width, std::size_t out_width);

}}",32,src\cuda4dnn\kernels\permute.hpp,cv.dnn.cuda4dnn,14,cuda4dnn,1
366174,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void permute(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, std::vector<std::size_t> order);

    template <class T>
    void transpose(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, std::size_t in_width, std::size_t out_width);

}",53,src\cuda4dnn\kernels\permute.hpp,cv.dnn.cuda4dnn.kernels,14,kernels,1
366199,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\prior_box.hpp,src\cuda4dnn\kernels\prior_box.hpp:<global>,,<global>,1
366203,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void generate_prior_boxes(
        const csl::Stream& stream,
        csl::Span<T> output,
        csl::View<float> boxWidth, csl::View<float> boxHeight, csl::View<float> offsetX, csl::View<float> offsetY, float stepX, float stepY,
        std::vector<float> variance,
        std::size_t numPriors,
        std::size_t layerWidth, std::size_t layerHeight,
        std::size_t imageWidth, std::size_t imageHeight,
        bool normalize, bool clip);

}}}}",1,src\cuda4dnn\kernels\prior_box.hpp,cv,13,cv,1
366204,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void generate_prior_boxes(
        const csl::Stream& stream,
        csl::Span<T> output,
        csl::View<float> boxWidth, csl::View<float> boxHeight, csl::View<float> offsetX, csl::View<float> offsetY, float stepX, float stepY,
        std::vector<float> variance,
        std::size_t numPriors,
        std::size_t layerWidth, std::size_t layerHeight,
        std::size_t imageWidth, std::size_t imageHeight,
        bool normalize, bool clip);

}}}",16,src\cuda4dnn\kernels\prior_box.hpp,cv.dnn,13,dnn,1
366205,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void generate_prior_boxes(
        const csl::Stream& stream,
        csl::Span<T> output,
        csl::View<float> boxWidth, csl::View<float> boxHeight, csl::View<float> offsetX, csl::View<float> offsetY, float stepX, float stepY,
        std::vector<float> variance,
        std::size_t numPriors,
        std::size_t layerWidth, std::size_t layerHeight,
        std::size_t imageWidth, std::size_t imageHeight,
        bool normalize, bool clip);

}}",32,src\cuda4dnn\kernels\prior_box.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
366206,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void generate_prior_boxes(
        const csl::Stream& stream,
        csl::Span<T> output,
        csl::View<float> boxWidth, csl::View<float> boxHeight, csl::View<float> offsetX, csl::View<float> offsetY, float stepX, float stepY,
        std::vector<float> variance,
        std::size_t numPriors,
        std::size_t layerWidth, std::size_t layerHeight,
        std::size_t imageWidth, std::size_t imageHeight,
        bool normalize, bool clip);

}",53,src\cuda4dnn\kernels\prior_box.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
366234,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\region.hpp,src\cuda4dnn\kernels\region.hpp:<global>,,<global>,1
366238,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void region(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<T> bias,
        T object_prob_cutoff, T class_prob_cutoff,
        std::size_t boxes_per_cell, std::size_t box_size,
        std::size_t rows, std::size_t cols, T scale_x_y,
        std::size_t height_norm, std::size_t width_norm,
        bool if_true_sigmoid_else_softmax, bool new_coords);

}}}}",1,src\cuda4dnn\kernels\region.hpp,cv,13,cv,1
366239,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void region(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<T> bias,
        T object_prob_cutoff, T class_prob_cutoff,
        std::size_t boxes_per_cell, std::size_t box_size,
        std::size_t rows, std::size_t cols, T scale_x_y,
        std::size_t height_norm, std::size_t width_norm,
        bool if_true_sigmoid_else_softmax, bool new_coords);

}}}",16,src\cuda4dnn\kernels\region.hpp,cv.dnn,13,dnn,1
366240,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void region(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<T> bias,
        T object_prob_cutoff, T class_prob_cutoff,
        std::size_t boxes_per_cell, std::size_t box_size,
        std::size_t rows, std::size_t cols, T scale_x_y,
        std::size_t height_norm, std::size_t width_norm,
        bool if_true_sigmoid_else_softmax, bool new_coords);

}}",32,src\cuda4dnn\kernels\region.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
366241,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void region(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, csl::View<T> bias,
        T object_prob_cutoff, T class_prob_cutoff,
        std::size_t boxes_per_cell, std::size_t box_size,
        std::size_t rows, std::size_t cols, T scale_x_y,
        std::size_t height_norm, std::size_t width_norm,
        bool if_true_sigmoid_else_softmax, bool new_coords);

}",53,src\cuda4dnn\kernels\region.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
366266,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\resize.hpp,src\cuda4dnn\kernels\resize.hpp:<global>,,<global>,1
366270,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void resize_nn(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, float scale_y, float scale_x, bool round, bool half_pixel_centers);

    template <class T>
    void resize_bilinear(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, float scale_y, float scale_x, bool half_pixel_centers);

}}}}",1,src\cuda4dnn\kernels\resize.hpp,cv,11,cv,1
366271,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void resize_nn(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, float scale_y, float scale_x, bool round, bool half_pixel_centers);

    template <class T>
    void resize_bilinear(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, float scale_y, float scale_x, bool half_pixel_centers);

}}}",16,src\cuda4dnn\kernels\resize.hpp,cv.dnn,11,dnn,1
366272,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void resize_nn(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, float scale_y, float scale_x, bool round, bool half_pixel_centers);

    template <class T>
    void resize_bilinear(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, float scale_y, float scale_x, bool half_pixel_centers);

}}",32,src\cuda4dnn\kernels\resize.hpp,cv.dnn.cuda4dnn,11,cuda4dnn,1
366273,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void resize_nn(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, float scale_y, float scale_x, bool round, bool half_pixel_centers);

    template <class T>
    void resize_bilinear(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, float scale_y, float scale_x, bool half_pixel_centers);

}",53,src\cuda4dnn\kernels\resize.hpp,cv.dnn.cuda4dnn.kernels,11,kernels,1
366302,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\roi_pooling.hpp,src\cuda4dnn\kernels\roi_pooling.hpp:<global>,,<global>,1
366306,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void roi_pooling(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::View<T> rois, float spatial_scale);

}}}}",1,src\cuda4dnn\kernels\roi_pooling.hpp,cv,12,cv,1
366307,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void roi_pooling(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::View<T> rois, float spatial_scale);

}}}",16,src\cuda4dnn\kernels\roi_pooling.hpp,cv.dnn,12,dnn,1
366308,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void roi_pooling(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::View<T> rois, float spatial_scale);

}}",32,src\cuda4dnn\kernels\roi_pooling.hpp,cv.dnn.cuda4dnn,12,cuda4dnn,1
366309,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void roi_pooling(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::View<T> rois, float spatial_scale);

}",53,src\cuda4dnn\kernels\roi_pooling.hpp,cv.dnn.cuda4dnn.kernels,12,kernels,1
366326,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\scale_shift.hpp,src\cuda4dnn\kernels\scale_shift.hpp:<global>,,<global>,1
366330,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void biasN(const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> bias);

    template <class T>
    void scaleN(const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> weights);

    template <class T>
    void scale1_with_bias1(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T alpha, T beta);

    template <class T>
    void scaleN_with_biasN(
        const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> weights, csl::TensorView<T> bias);

}}}}",1,src\cuda4dnn\kernels\scale_shift.hpp,cv,13,cv,1
366331,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void biasN(const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> bias);

    template <class T>
    void scaleN(const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> weights);

    template <class T>
    void scale1_with_bias1(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T alpha, T beta);

    template <class T>
    void scaleN_with_biasN(
        const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> weights, csl::TensorView<T> bias);

}}}",16,src\cuda4dnn\kernels\scale_shift.hpp,cv.dnn,13,dnn,1
366332,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void biasN(const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> bias);

    template <class T>
    void scaleN(const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> weights);

    template <class T>
    void scale1_with_bias1(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T alpha, T beta);

    template <class T>
    void scaleN_with_biasN(
        const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> weights, csl::TensorView<T> bias);

}}",32,src\cuda4dnn\kernels\scale_shift.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
366333,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void biasN(const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> bias);

    template <class T>
    void scaleN(const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> weights);

    template <class T>
    void scale1_with_bias1(const csl::Stream& stream, csl::Span<T> output, csl::View<T> input, T alpha, T beta);

    template <class T>
    void scaleN_with_biasN(
        const csl::Stream& stream,
        csl::TensorSpan<T> output,
        csl::TensorView<T> input, std::size_t inner_size,
        csl::TensorView<T> weights, csl::TensorView<T> bias);

}",53,src\cuda4dnn\kernels\scale_shift.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
366376,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\shortcut.hpp,src\cuda4dnn\kernels\shortcut.hpp:<global>,,<global>,1
366380,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void input_shortcut(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::TensorView<T> from);

}}}}",1,src\cuda4dnn\kernels\shortcut.hpp,cv,11,cv,1
366381,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void input_shortcut(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::TensorView<T> from);

}}}",16,src\cuda4dnn\kernels\shortcut.hpp,cv.dnn,11,dnn,1
366382,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void input_shortcut(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::TensorView<T> from);

}}",32,src\cuda4dnn\kernels\shortcut.hpp,cv.dnn.cuda4dnn,11,cuda4dnn,1
366383,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void input_shortcut(const csl::Stream& stream, csl::TensorSpan<T> output, csl::TensorView<T> input, csl::TensorView<T> from);

}",53,src\cuda4dnn\kernels\shortcut.hpp,cv.dnn.cuda4dnn.kernels,11,kernels,1
366399,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\kernels\slice.hpp,src\cuda4dnn\kernels\slice.hpp:<global>,,<global>,1
366403,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void slice(const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input,
        std::vector<std::size_t> offsets);

}}}}",1,src\cuda4dnn\kernels\slice.hpp,cv,13,cv,1
366404,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    template <class T>
    void slice(const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input,
        std::vector<std::size_t> offsets);

}}}",16,src\cuda4dnn\kernels\slice.hpp,cv.dnn,13,dnn,1
366405,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    template <class T>
    void slice(const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input,
        std::vector<std::size_t> offsets);

}}",32,src\cuda4dnn\kernels\slice.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
366406,NAMESPACE_BLOCK,"namespace kernels {

    template <class T>
    void slice(const csl::Stream& stream,
        csl::TensorSpan<T> output, csl::TensorView<T> input,
        std::vector<std::size_t> offsets);

}",53,src\cuda4dnn\kernels\slice.hpp,cv.dnn.cuda4dnn.kernels,13,kernels,1
366428,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\activation.hpp,src\cuda4dnn\primitives\activation.hpp:<global>,,<global>,1
366432,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <template<class> class Op, class T>
    struct BaseOp : public CUDABackendNode
    {
    protected:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                static_cast<const Op<T>*>(this)->calculate(output, input);
            }
        }
    };

    template <class T>
    class ReLUOp final : public BaseOp<ReLUOp, T> {
    public:
        ReLUOp(csl::Stream stream_...",1,src\cuda4dnn\primitives\activation.hpp,cv,19,cv,1
366433,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <template<class> class Op, class T>
    struct BaseOp : public CUDABackendNode
    {
    protected:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                static_cast<const Op<T>*>(this)->calculate(output, input);
            }
        }
    };

    template <class T>
    class ReLUOp final : public BaseOp<ReLUOp, T> {
    public:
        ReLUOp(csl::Stream stream_, T slope_)
   ...",16,src\cuda4dnn\primitives\activation.hpp,cv.dnn,19,dnn,1
366434,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <template<class> class Op, class T>
    struct BaseOp : public CUDABackendNode
    {
    protected:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                static_cast<const Op<T>*>(this)->calculate(output, input);
            }
        }
    };

    template <class T>
    class ReLUOp final : public BaseOp<ReLUOp, T> {
    public:
        ReLUOp(csl::Stream stream_, T slope_)
                : s...",32,src\cuda4dnn\primitives\activation.hpp,cv.dnn.cuda4dnn,19,cuda4dnn,1
367434,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\batch_norm.hpp,src\cuda4dnn\primitives\batch_norm.hpp:<global>,,<global>,1
367438,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class BatchNormOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        BatchNormOp(csl::Stream stream_, const cv::Mat& weights, const cv::Mat& bias)
            : stream(std::move(stream_))
        {
            biasTensor = csl::makeTensorHeader<T>(bias);
            csl::copyMatToTensor<T>(bias, biasTensor, stream);

            weightsTensor = csl::makeTensorHeader<T>(weights);
            csl::copyMatToTensor<T>(weights, weightsTensor, stream);
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getVi...",1,src\cuda4dnn\primitives\batch_norm.hpp,cv,17,cv,1
367439,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class BatchNormOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        BatchNormOp(csl::Stream stream_, const cv::Mat& weights, const cv::Mat& bias)
            : stream(std::move(stream_))
        {
            biasTensor = csl::makeTensorHeader<T>(bias);
            csl::copyMatToTensor<T>(bias, biasTensor, stream);

            weightsTensor = csl::makeTensorHeader<T>(weights);
            csl::copyMatToTensor<T>(weights, weightsTensor, stream);
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

        ...",16,src\cuda4dnn\primitives\batch_norm.hpp,cv.dnn,17,dnn,1
367440,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class BatchNormOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        BatchNormOp(csl::Stream stream_, const cv::Mat& weights, const cv::Mat& bias)
            : stream(std::move(stream_))
        {
            biasTensor = csl::makeTensorHeader<T>(bias);
            csl::copyMatToTensor<T>(bias, biasTensor, stream);

            weightsTensor = csl::makeTensorHeader<T>(weights);
            csl::copyMatToTensor<T>(weights, weightsTensor, stream);
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto output_...",32,src\cuda4dnn\primitives\batch_norm.hpp,cv.dnn.cuda4dnn,17,cuda4dnn,1
367587,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\concat.hpp,src\cuda4dnn\primitives\concat.hpp:<global>,,<global>,1
367591,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class ConcatOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ConcatOp(csl::Stream stream_, std::size_t concat_axis, bool zero_padding)
            : stream(std::move(stream_)), concat_axis{ concat_axis }, zero_padding{ zero_padding }
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            if(zero_padding)
            {
                auto output_shape = output_wrapper->getShape();

                kernels::fill<T>(stream, output, 0.0);

                std::size_t output_concat_axis_of...",1,src\cuda4dnn\primitives\concat.hpp,cv,22,cv,1
367592,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class ConcatOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ConcatOp(csl::Stream stream_, std::size_t concat_axis, bool zero_padding)
            : stream(std::move(stream_)), concat_axis{ concat_axis }, zero_padding{ zero_padding }
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            if(zero_padding)
            {
                auto output_shape = output_wrapper->getShape();

                kernels::fill<T>(stream, output, 0.0);

                std::size_t output_concat_axis_offset = 0;
     ...",16,src\cuda4dnn\primitives\concat.hpp,cv.dnn,22,dnn,1
367593,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class ConcatOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ConcatOp(csl::Stream stream_, std::size_t concat_axis, bool zero_padding)
            : stream(std::move(stream_)), concat_axis{ concat_axis }, zero_padding{ zero_padding }
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            if(zero_padding)
            {
                auto output_shape = output_wrapper->getShape();

                kernels::fill<T>(stream, output, 0.0);

                std::size_t output_concat_axis_offset = 0;
                for (...",32,src\cuda4dnn\primitives\concat.hpp,cv.dnn.cuda4dnn,22,cuda4dnn,1
367821,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\const.hpp,src\cuda4dnn\primitives\const.hpp:<global>,,<global>,1
367825,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class ConstOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ConstOp(csl::Stream stream_, const cv::Mat& data)
            : stream(std::move(stream_))
        {
            constTensor = csl::makeTensorHeader<T>(data);
            csl::copyMatToTensor<T>(data, constTensor, stream);
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1 && inputs.size() == 0);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();
            csl::tensor_ops::copy<T>(stream, output, constTensor);
        }

    private:
        csl::Stream stream;
        csl::Tensor<T> constTensor...",1,src\cuda4dnn\primitives\const.hpp,cv,18,cv,1
367826,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class ConstOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ConstOp(csl::Stream stream_, const cv::Mat& data)
            : stream(std::move(stream_))
        {
            constTensor = csl::makeTensorHeader<T>(data);
            csl::copyMatToTensor<T>(data, constTensor, stream);
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1 && inputs.size() == 0);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();
            csl::tensor_ops::copy<T>(stream, output, constTensor);
        }

    private:
        csl::Stream stream;
        csl::Tensor<T> constTensor;
    };

}}",16,src\cuda4dnn\primitives\const.hpp,cv.dnn,18,dnn,1
367827,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class ConstOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ConstOp(csl::Stream stream_, const cv::Mat& data)
            : stream(std::move(stream_))
        {
            constTensor = csl::makeTensorHeader<T>(data);
            csl::copyMatToTensor<T>(data, constTensor, stream);
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1 && inputs.size() == 0);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();
            csl::tensor_ops::copy<T>(stream, output, constTensor);
        }

    private:
        csl::Stream stream;
        csl::Tensor<T> constTensor;
    };

}",32,src\cuda4dnn\primitives\const.hpp,cv.dnn.cuda4dnn,18,cuda4dnn,1
367956,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\convolution.hpp,src\cuda4dnn\primitives\convolution.hpp:<global>,,<global>,1
367960,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    struct ConvolutionConfiguration {
        /* the size of the following vectors must be equal to the kernel size */
        std::vector<std::size_t> kernel_size;
        std::vector<std::size_t> dilations, strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        /* explicit paddings are used if and only if padMode is set to manual */
        PaddingMode padMode;
        std::vector<std::size_t> pads_begin, pads_end;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_shape;
        std::vector<std::size_t> output_shape;

        /* group count for grouped convolution */
        std::size_t groups;

        enum class FusionMode {
            NONE,
            ACTIVATION, /...",1,src\cuda4dnn\primitives\convolution.hpp,cv,33,cv,1
367961,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    struct ConvolutionConfiguration {
        /* the size of the following vectors must be equal to the kernel size */
        std::vector<std::size_t> kernel_size;
        std::vector<std::size_t> dilations, strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        /* explicit paddings are used if and only if padMode is set to manual */
        PaddingMode padMode;
        std::vector<std::size_t> pads_begin, pads_end;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_shape;
        std::vector<std::size_t> output_shape;

        /* group count for grouped convolution */
        std::size_t groups;

        enum class FusionMode {
            NONE,
            ACTIVATION, /* act(conv) */
...",16,src\cuda4dnn\primitives\convolution.hpp,cv.dnn,33,dnn,1
367962,NAMESPACE_BLOCK,"namespace cuda4dnn {

    struct ConvolutionConfiguration {
        /* the size of the following vectors must be equal to the kernel size */
        std::vector<std::size_t> kernel_size;
        std::vector<std::size_t> dilations, strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        /* explicit paddings are used if and only if padMode is set to manual */
        PaddingMode padMode;
        std::vector<std::size_t> pads_begin, pads_end;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_shape;
        std::vector<std::size_t> output_shape;

        /* group count for grouped convolution */
        std::size_t groups;

        enum class FusionMode {
            NONE,
            ACTIVATION, /* act(conv) */
            ELTW...",32,src\cuda4dnn\primitives\convolution.hpp,cv.dnn.cuda4dnn,33,cuda4dnn,1
370306,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\crop_and_resize.hpp,src\cuda4dnn\primitives\crop_and_resize.hpp:<global>,,<global>,1
370310,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class CropAndResizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        CropAndResizeOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 2 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto box_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto boxes = box_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            kernels::crop_and_resize(stream, output, input, sta...",1,src\cuda4dnn\primitives\crop_and_resize.hpp,cv,17,cv,1
370311,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class CropAndResizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        CropAndResizeOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 2 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto box_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto boxes = box_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            kernels::crop_and_resize(stream, output, input, static_cast<csl::V...",16,src\cuda4dnn\primitives\crop_and_resize.hpp,cv.dnn,17,dnn,1
370312,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class CropAndResizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        CropAndResizeOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 2 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto box_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto boxes = box_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            kernels::crop_and_resize(stream, output, input, static_cast<csl::View<T>>(boxes));...",32,src\cuda4dnn\primitives\crop_and_resize.hpp,cv.dnn.cuda4dnn,17,cuda4dnn,1
370419,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\detection_output.hpp,src\cuda4dnn\primitives\detection_output.hpp:<global>,,<global>,1
370423,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    struct DetectionOutputConfiguration {
        std::size_t batch_size;

        enum class CodeType {
            CORNER,
            CENTER_SIZE
        };
        CodeType code_type;

        bool share_location;
        std::size_t num_priors;
        std::size_t num_classes;
        std::size_t background_class_id;

        bool transpose_location;
        bool variance_encoded_in_target;
        bool normalized_bbox;
        bool clip_box;

        std::size_t classwise_topK;
        float confidence_threshold;
        float nms_threshold;

        int keepTopK;
    };

    template <class T>
    class DetectionOutputOp final : public CUDABackendNode {
    private:
        /* We have block level NMS kernel where each block handles one class of one batch item.
         * If the number of classes and batch size together is very low, the blockwise NMS kernel
         * won't able to fully saturate the GPU with work.
         ...",1,src\cuda4dnn\primitives\detection_output.hpp,cv,21,cv,1
370424,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    struct DetectionOutputConfiguration {
        std::size_t batch_size;

        enum class CodeType {
            CORNER,
            CENTER_SIZE
        };
        CodeType code_type;

        bool share_location;
        std::size_t num_priors;
        std::size_t num_classes;
        std::size_t background_class_id;

        bool transpose_location;
        bool variance_encoded_in_target;
        bool normalized_bbox;
        bool clip_box;

        std::size_t classwise_topK;
        float confidence_threshold;
        float nms_threshold;

        int keepTopK;
    };

    template <class T>
    class DetectionOutputOp final : public CUDABackendNode {
    private:
        /* We have block level NMS kernel where each block handles one class of one batch item.
         * If the number of classes and batch size together is very low, the blockwise NMS kernel
         * won't able to fully saturate the GPU with work.
         *
         * We...",16,src\cuda4dnn\primitives\detection_output.hpp,cv.dnn,21,dnn,1
370425,NAMESPACE_BLOCK,"namespace cuda4dnn {

    struct DetectionOutputConfiguration {
        std::size_t batch_size;

        enum class CodeType {
            CORNER,
            CENTER_SIZE
        };
        CodeType code_type;

        bool share_location;
        std::size_t num_priors;
        std::size_t num_classes;
        std::size_t background_class_id;

        bool transpose_location;
        bool variance_encoded_in_target;
        bool normalized_bbox;
        bool clip_box;

        std::size_t classwise_topK;
        float confidence_threshold;
        float nms_threshold;

        int keepTopK;
    };

    template <class T>
    class DetectionOutputOp final : public CUDABackendNode {
    private:
        /* We have block level NMS kernel where each block handles one class of one batch item.
         * If the number of classes and batch size together is very low, the blockwise NMS kernel
         * won't able to fully saturate the GPU with work.
         *
         * We also have a gri...",32,src\cuda4dnn\primitives\detection_output.hpp,cv.dnn.cuda4dnn,21,cuda4dnn,1
371259,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\eltwise.hpp,src\cuda4dnn\primitives\eltwise.hpp:<global>,,<global>,1
371263,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    enum class EltwiseOpType {
        MAX,
        SUM,
        PRODUCT,
        DIV,
        MIN,
        SUB,
    };

    class EltwiseOpBase : public CUDABackendNode {
    public:
        EltwiseOpBase(csl::Stream stream_, EltwiseOpType op_, std::vector<float> coeffs_)
            : stream(std::move(stream_)), op(op_), coeffs(std::move(coeffs_))
        {
        }

    protected:
        csl::Stream stream;

    public:
        EltwiseOpType op;
        std::vector<float> coeffs;
    };

    template <class T>
    class EltwiseOp final : public EltwiseOpBase {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        EltwiseOp(csl::Stream stream_, EltwiseOpType op_, std::vector<float> coeffs_)
            : EltwiseOpBase(std::move(stream_), op_, std::move(coeffs_))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::...",1,src\cuda4dnn\primitives\eltwise.hpp,cv,22,cv,1
371264,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    enum class EltwiseOpType {
        MAX,
        SUM,
        PRODUCT,
        DIV,
        MIN,
        SUB,
    };

    class EltwiseOpBase : public CUDABackendNode {
    public:
        EltwiseOpBase(csl::Stream stream_, EltwiseOpType op_, std::vector<float> coeffs_)
            : stream(std::move(stream_)), op(op_), coeffs(std::move(coeffs_))
        {
        }

    protected:
        csl::Stream stream;

    public:
        EltwiseOpType op;
        std::vector<float> coeffs;
    };

    template <class T>
    class EltwiseOp final : public EltwiseOpBase {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        EltwiseOp(csl::Stream stream_, EltwiseOpType op_, std::vector<float> coeffs_)
            : EltwiseOpBase(std::move(stream_), op_, std::move(coeffs_))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrap...",16,src\cuda4dnn\primitives\eltwise.hpp,cv.dnn,22,dnn,1
371265,NAMESPACE_BLOCK,"namespace cuda4dnn {

    enum class EltwiseOpType {
        MAX,
        SUM,
        PRODUCT,
        DIV,
        MIN,
        SUB,
    };

    class EltwiseOpBase : public CUDABackendNode {
    public:
        EltwiseOpBase(csl::Stream stream_, EltwiseOpType op_, std::vector<float> coeffs_)
            : stream(std::move(stream_)), op(op_), coeffs(std::move(coeffs_))
        {
        }

    protected:
        csl::Stream stream;

    public:
        EltwiseOpType op;
        std::vector<float> coeffs;
    };

    template <class T>
    class EltwiseOp final : public EltwiseOpBase {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        EltwiseOp(csl::Stream stream_, EltwiseOpType op_, std::vector<float> coeffs_)
            : EltwiseOpBase(std::move(stream_), op_, std::move(coeffs_))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
...",32,src\cuda4dnn\primitives\eltwise.hpp,cv.dnn.cuda4dnn,22,cuda4dnn,1
371712,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\inner_product.hpp,src\cuda4dnn\primitives\inner_product.hpp:<global>,,<global>,1
371716,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class InnerProductOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        InnerProductOp(csl::Stream stream_, csl::cublas::Handle handle, std::size_t axis, const Mat& weights, const Mat& bias)
            : stream(std::move(stream_)), cublasHandle(std::move(handle)), axis{ axis }
        {
            weightsTensor = csl::makeTensorHeader<T>(weights);
            CV_Assert(get_effective_rank(weightsTensor) <= 2);
            csl::copyMatToTensor<T>(weights, weightsTensor, stream);

            if (!bias.empty())
            {
                biasTensor = csl::makeTensorHeader<T>(bias);
                csl::copyMatToTensor<T>(bias, biasTensor, stream);
                CV_Assert(weightsTensor.get_axis_size(-2) == biasTensor.size());
            }
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
     ...",1,src\cuda4dnn\primitives\inner_product.hpp,cv,23,cv,1
371717,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class InnerProductOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        InnerProductOp(csl::Stream stream_, csl::cublas::Handle handle, std::size_t axis, const Mat& weights, const Mat& bias)
            : stream(std::move(stream_)), cublasHandle(std::move(handle)), axis{ axis }
        {
            weightsTensor = csl::makeTensorHeader<T>(weights);
            CV_Assert(get_effective_rank(weightsTensor) <= 2);
            csl::copyMatToTensor<T>(weights, weightsTensor, stream);

            if (!bias.empty())
            {
                biasTensor = csl::makeTensorHeader<T>(bias);
                csl::copyMatToTensor<T>(bias, biasTensor, stream);
                CV_Assert(weightsTensor.get_axis_size(-2) == biasTensor.size());
            }
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const st...",16,src\cuda4dnn\primitives\inner_product.hpp,cv.dnn,23,dnn,1
371718,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class InnerProductOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        InnerProductOp(csl::Stream stream_, csl::cublas::Handle handle, std::size_t axis, const Mat& weights, const Mat& bias)
            : stream(std::move(stream_)), cublasHandle(std::move(handle)), axis{ axis }
        {
            weightsTensor = csl::makeTensorHeader<T>(weights);
            CV_Assert(get_effective_rank(weightsTensor) <= 2);
            csl::copyMatToTensor<T>(weights, weightsTensor, stream);

            if (!bias.empty())
            {
                biasTensor = csl::makeTensorHeader<T>(bias);
                csl::copyMatToTensor<T>(bias, biasTensor, stream);
                CV_Assert(weightsTensor.get_axis_size(-2) == biasTensor.size());
            }
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Pt...",32,src\cuda4dnn\primitives\inner_product.hpp,cv.dnn.cuda4dnn,23,cuda4dnn,1
371963,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\instance_norm.hpp,src\cuda4dnn\primitives\instance_norm.hpp:<global>,,<global>,1
371967,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class InstanceNormOp final : public CUDABackendNode {
     public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        InstanceNormOp(csl::Stream stream_, float epsilon_, size_t loops)
            : stream(std::move(stream_)), epsilon(epsilon_) {
            csl::WorkspaceBuilder builder;
            builder.require<float>(loops);
            builder.require<float>(loops);
            scratch_mem_in_bytes = builder.required_workspace_size();
        }

        void forward(const std::vector<cv::Ptr<BackendWrapper>>& inputs,
                     const std::vector<cv::Ptr<BackendWrapper>>& outputs,
                     csl::Workspace& workspace) override {
            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto scale_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto bias_wrapper = inputs[2].dynamicCast<wrapper_type>();

            auto input...",1,src\cuda4dnn\primitives\instance_norm.hpp,cv,24,cv,1
371968,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class InstanceNormOp final : public CUDABackendNode {
     public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        InstanceNormOp(csl::Stream stream_, float epsilon_, size_t loops)
            : stream(std::move(stream_)), epsilon(epsilon_) {
            csl::WorkspaceBuilder builder;
            builder.require<float>(loops);
            builder.require<float>(loops);
            scratch_mem_in_bytes = builder.required_workspace_size();
        }

        void forward(const std::vector<cv::Ptr<BackendWrapper>>& inputs,
                     const std::vector<cv::Ptr<BackendWrapper>>& outputs,
                     csl::Workspace& workspace) override {
            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto scale_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto bias_wrapper = inputs[2].dynamicCast<wrapper_type>();

            auto input = input_wrappe...",16,src\cuda4dnn\primitives\instance_norm.hpp,cv.dnn,24,dnn,1
371969,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class InstanceNormOp final : public CUDABackendNode {
     public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        InstanceNormOp(csl::Stream stream_, float epsilon_, size_t loops)
            : stream(std::move(stream_)), epsilon(epsilon_) {
            csl::WorkspaceBuilder builder;
            builder.require<float>(loops);
            builder.require<float>(loops);
            scratch_mem_in_bytes = builder.required_workspace_size();
        }

        void forward(const std::vector<cv::Ptr<BackendWrapper>>& inputs,
                     const std::vector<cv::Ptr<BackendWrapper>>& outputs,
                     csl::Workspace& workspace) override {
            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto scale_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto bias_wrapper = inputs[2].dynamicCast<wrapper_type>();

            auto input = input_wrapper->getView();
  ...",32,src\cuda4dnn\primitives\instance_norm.hpp,cv.dnn.cuda4dnn,24,cuda4dnn,1
372213,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\layer_norm.hpp,src\cuda4dnn\primitives\layer_norm.hpp:<global>,,<global>,1
372217,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class LayerNormOp final : public CUDABackendNode {
     public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        LayerNormOp(csl::Stream stream_, int normalized_axis, float epsilon_, size_t loops)
            : stream(std::move(stream_)), epsilon(epsilon_) {
            CV_CheckGE(normalized_axis, 0, ""LayerNorm/CUDA: axis needs to be normalized"");
            axis = static_cast<size_t>(normalized_axis);

            csl::WorkspaceBuilder builder;
            builder.require<float>(loops);
            builder.require<float>(loops);
            scratch_mem_in_bytes = builder.required_workspace_size();
        }

        void forward(const std::vector<cv::Ptr<BackendWrapper>>& inputs,
                     const std::vector<cv::Ptr<BackendWrapper>>& outputs,
                     csl::Workspace& workspace) override {
            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
  ...",1,src\cuda4dnn\primitives\layer_norm.hpp,cv,24,cv,1
372218,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class LayerNormOp final : public CUDABackendNode {
     public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        LayerNormOp(csl::Stream stream_, int normalized_axis, float epsilon_, size_t loops)
            : stream(std::move(stream_)), epsilon(epsilon_) {
            CV_CheckGE(normalized_axis, 0, ""LayerNorm/CUDA: axis needs to be normalized"");
            axis = static_cast<size_t>(normalized_axis);

            csl::WorkspaceBuilder builder;
            builder.require<float>(loops);
            builder.require<float>(loops);
            scratch_mem_in_bytes = builder.required_workspace_size();
        }

        void forward(const std::vector<cv::Ptr<BackendWrapper>>& inputs,
                     const std::vector<cv::Ptr<BackendWrapper>>& outputs,
                     csl::Workspace& workspace) override {
            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto ...",16,src\cuda4dnn\primitives\layer_norm.hpp,cv.dnn,24,dnn,1
372219,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class LayerNormOp final : public CUDABackendNode {
     public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        LayerNormOp(csl::Stream stream_, int normalized_axis, float epsilon_, size_t loops)
            : stream(std::move(stream_)), epsilon(epsilon_) {
            CV_CheckGE(normalized_axis, 0, ""LayerNorm/CUDA: axis needs to be normalized"");
            axis = static_cast<size_t>(normalized_axis);

            csl::WorkspaceBuilder builder;
            builder.require<float>(loops);
            builder.require<float>(loops);
            scratch_mem_in_bytes = builder.required_workspace_size();
        }

        void forward(const std::vector<cv::Ptr<BackendWrapper>>& inputs,
                     const std::vector<cv::Ptr<BackendWrapper>>& outputs,
                     csl::Workspace& workspace) override {
            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto scale_wrapper = ...",32,src\cuda4dnn\primitives\layer_norm.hpp,cv.dnn.cuda4dnn,24,cuda4dnn,1
372474,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\lrn.hpp,src\cuda4dnn\primitives\lrn.hpp:<global>,,<global>,1
372478,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    enum class LRNType {
        ACROSS_CHANNELS,
        WITHIN_CHANNEL
    };

    template <class T>
    class LRNOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        LRNOp(csl::cudnn::Handle handle, LRNType type_, std::size_t local_size, T alpha, T beta, T bias, std::size_t largestInputSize)
            : scratch_mem_in_bytes { 0 }
        {
            typename csl::LRN<T>::LRNType type{};
            switch (type_) {
            case LRNType::ACROSS_CHANNELS: type = csl::LRN<T>::LRNType::ACROSS_CHANNELS; break;
            case LRNType::WITHIN_CHANNEL: type = csl::LRN<T>::LRNType::WITHIN_CHANNEL; break;
            }
            lrn = csl::LRN<T>(std::move(handle), local_size, alpha, beta, bias, type);

            csl::WorkspaceBuilder builder;
            if (type_ == LRNType::WITHIN_CHANNEL) {
                /* this is not a bug; we require two of these */
  ...",1,src\cuda4dnn\primitives\lrn.hpp,cv,16,cv,1
372479,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    enum class LRNType {
        ACROSS_CHANNELS,
        WITHIN_CHANNEL
    };

    template <class T>
    class LRNOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        LRNOp(csl::cudnn::Handle handle, LRNType type_, std::size_t local_size, T alpha, T beta, T bias, std::size_t largestInputSize)
            : scratch_mem_in_bytes { 0 }
        {
            typename csl::LRN<T>::LRNType type{};
            switch (type_) {
            case LRNType::ACROSS_CHANNELS: type = csl::LRN<T>::LRNType::ACROSS_CHANNELS; break;
            case LRNType::WITHIN_CHANNEL: type = csl::LRN<T>::LRNType::WITHIN_CHANNEL; break;
            }
            lrn = csl::LRN<T>(std::move(handle), local_size, alpha, beta, bias, type);

            csl::WorkspaceBuilder builder;
            if (type_ == LRNType::WITHIN_CHANNEL) {
                /* this is not a bug; we require two of these */
                b...",16,src\cuda4dnn\primitives\lrn.hpp,cv.dnn,16,dnn,1
372480,NAMESPACE_BLOCK,"namespace cuda4dnn {

    enum class LRNType {
        ACROSS_CHANNELS,
        WITHIN_CHANNEL
    };

    template <class T>
    class LRNOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        LRNOp(csl::cudnn::Handle handle, LRNType type_, std::size_t local_size, T alpha, T beta, T bias, std::size_t largestInputSize)
            : scratch_mem_in_bytes { 0 }
        {
            typename csl::LRN<T>::LRNType type{};
            switch (type_) {
            case LRNType::ACROSS_CHANNELS: type = csl::LRN<T>::LRNType::ACROSS_CHANNELS; break;
            case LRNType::WITHIN_CHANNEL: type = csl::LRN<T>::LRNType::WITHIN_CHANNEL; break;
            }
            lrn = csl::LRN<T>(std::move(handle), local_size, alpha, beta, bias, type);

            csl::WorkspaceBuilder builder;
            if (type_ == LRNType::WITHIN_CHANNEL) {
                /* this is not a bug; we require two of these */
                builder.require<T...",32,src\cuda4dnn\primitives\lrn.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
372664,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\matmul.hpp,src\cuda4dnn\primitives\matmul.hpp:<global>,,<global>,1
372668,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class MatMulOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MatMulOp(csl::Stream stream_, csl::cublas::Handle handle, const Mat& constInp, const Mat& bias, bool _transA, bool _transB)
            : stream(std::move(stream_)), cublasHandle(std::move(handle))
        {
            if (!constInp.empty())
            {
                constTensor = csl::makeTensorHeader<T>(constInp);
                csl::copyMatToTensor<T>(constInp, constTensor, stream);
            }

            if (!bias.empty())
            {
                biasTensor = csl::makeTensorHeader<T>(bias);
                csl::copyMatToTensor<T>(bias, biasTensor, stream);
            }

            transA = _transA;
            transB = _transB;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::...",1,src\cuda4dnn\primitives\matmul.hpp,cv,21,cv,1
372669,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class MatMulOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MatMulOp(csl::Stream stream_, csl::cublas::Handle handle, const Mat& constInp, const Mat& bias, bool _transA, bool _transB)
            : stream(std::move(stream_)), cublasHandle(std::move(handle))
        {
            if (!constInp.empty())
            {
                constTensor = csl::makeTensorHeader<T>(constInp);
                csl::copyMatToTensor<T>(constInp, constTensor, stream);
            }

            if (!bias.empty())
            {
                biasTensor = csl::makeTensorHeader<T>(bias);
                csl::copyMatToTensor<T>(bias, biasTensor, stream);
            }

            transA = _transA;
            transB = _transB;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrap...",16,src\cuda4dnn\primitives\matmul.hpp,cv.dnn,21,dnn,1
372670,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class MatMulOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MatMulOp(csl::Stream stream_, csl::cublas::Handle handle, const Mat& constInp, const Mat& bias, bool _transA, bool _transB)
            : stream(std::move(stream_)), cublasHandle(std::move(handle))
        {
            if (!constInp.empty())
            {
                constTensor = csl::makeTensorHeader<T>(constInp);
                csl::copyMatToTensor<T>(constInp, constTensor, stream);
            }

            if (!bias.empty())
            {
                biasTensor = csl::makeTensorHeader<T>(bias);
                csl::copyMatToTensor<T>(bias, biasTensor, stream);
            }

            transA = _transA;
            transB = _transB;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
...",32,src\cuda4dnn\primitives\matmul.hpp,cv.dnn.cuda4dnn,21,cuda4dnn,1
373185,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\matmul_broadcast.hpp,src\cuda4dnn\primitives\matmul_broadcast.hpp:<global>,,<global>,1
373189,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class MatMulBroadcastOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MatMulBroadcastOp(csl::Stream stream_, csl::cublas::Handle handle, const Mat &B, bool _transA, bool _transB,
                 const std::vector<size_t> &A_offsets_, const std::vector<size_t> &B_offsets_, std::vector<size_t> &C_offsets_,
                 size_t batch_)
            : stream(std::move(stream_)), cublasHandle(std::move(handle)), A_offsets(A_offsets_), B_offsets(B_offsets_), C_offsets(C_offsets_), batch(batch_)
        {
            if (!B.empty()) {
                input_B_tensor = csl::makeTensorHeader<T>(B);
                csl::copyMatToTensor<T>(B, input_B_tensor, stream);
            }

            transA = _transA;
            transB = _transB;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
           ...",1,src\cuda4dnn\primitives\matmul_broadcast.hpp,cv,19,cv,1
373190,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class MatMulBroadcastOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MatMulBroadcastOp(csl::Stream stream_, csl::cublas::Handle handle, const Mat &B, bool _transA, bool _transB,
                 const std::vector<size_t> &A_offsets_, const std::vector<size_t> &B_offsets_, std::vector<size_t> &C_offsets_,
                 size_t batch_)
            : stream(std::move(stream_)), cublasHandle(std::move(handle)), A_offsets(A_offsets_), B_offsets(B_offsets_), C_offsets(C_offsets_), batch(batch_)
        {
            if (!B.empty()) {
                input_B_tensor = csl::makeTensorHeader<T>(B);
                csl::copyMatToTensor<T>(B, input_B_tensor, stream);
            }

            transA = _transA;
            transB = _transB;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vec...",16,src\cuda4dnn\primitives\matmul_broadcast.hpp,cv.dnn,19,dnn,1
373191,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class MatMulBroadcastOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MatMulBroadcastOp(csl::Stream stream_, csl::cublas::Handle handle, const Mat &B, bool _transA, bool _transB,
                 const std::vector<size_t> &A_offsets_, const std::vector<size_t> &B_offsets_, std::vector<size_t> &C_offsets_,
                 size_t batch_)
            : stream(std::move(stream_)), cublasHandle(std::move(handle)), A_offsets(A_offsets_), B_offsets(B_offsets_), C_offsets(C_offsets_), batch(batch_)
        {
            if (!B.empty()) {
                input_B_tensor = csl::makeTensorHeader<T>(B);
                csl::copyMatToTensor<T>(B, input_B_tensor, stream);
            }

            transA = _transA;
            transB = _transB;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<Back...",32,src\cuda4dnn\primitives\matmul_broadcast.hpp,cv.dnn.cuda4dnn,19,cuda4dnn,1
373384,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\max_unpooling.hpp,src\cuda4dnn\primitives\max_unpooling.hpp:<global>,,<global>,1
373388,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    struct MaxPoolingConfiguration {
        /* the size of the following vectors must be equal to the pooling order */
        std::vector<std::size_t> window_size;
        std::vector<std::size_t> strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        PaddingMode padMode;

        /* explicit paddings are used if and only if padMode is set to manual */
        std::vector<std::size_t> pads_begin;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_shape;
    };

    template <class T>
    class MaxPoolingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MaxPoolingOp(csl::Stream stream_, const MaxPoolingConf...",1,src\cuda4dnn\primitives\max_unpooling.hpp,cv,20,cv,1
373389,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    struct MaxPoolingConfiguration {
        /* the size of the following vectors must be equal to the pooling order */
        std::vector<std::size_t> window_size;
        std::vector<std::size_t> strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        PaddingMode padMode;

        /* explicit paddings are used if and only if padMode is set to manual */
        std::vector<std::size_t> pads_begin;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_shape;
    };

    template <class T>
    class MaxPoolingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MaxPoolingOp(csl::Stream stream_, const MaxPoolingConfiguration& conf...",16,src\cuda4dnn\primitives\max_unpooling.hpp,cv.dnn,20,dnn,1
373390,NAMESPACE_BLOCK,"namespace cuda4dnn {

    struct MaxPoolingConfiguration {
        /* the size of the following vectors must be equal to the pooling order */
        std::vector<std::size_t> window_size;
        std::vector<std::size_t> strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        PaddingMode padMode;

        /* explicit paddings are used if and only if padMode is set to manual */
        std::vector<std::size_t> pads_begin;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_shape;
    };

    template <class T>
    class MaxPoolingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MaxPoolingOp(csl::Stream stream_, const MaxPoolingConfiguration& config)
            ...",32,src\cuda4dnn\primitives\max_unpooling.hpp,cv.dnn.cuda4dnn,20,cuda4dnn,1
373878,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\mvn.hpp,src\cuda4dnn\primitives\mvn.hpp:<global>,,<global>,1
373882,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    struct MVNConfiguration {
        std::vector<std::vector<std::size_t>> input_shapes;

        /*
         * [0, split_axis) = outer range
         * [split_axis, -1] = inner range
         *
         * for each location in the outer range, all the values in the inner range are normalized as a group
         */
        std::size_t split_axis;

        /* The group (described above) is centered always. The following parameter controls whether the variance
         * is also normalized.
         */
        bool normalize_variance;
        float epsilon;
    };

    template <class T>
    class MVNOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MVNOp(csl::Stream stream_, const MVNConfiguration& config)
            : stream(std::move(stream_))
        {
            split_axis = config.split_axis;
            normalize_variance = config.normalize_variance;
        ...",1,src\cuda4dnn\primitives\mvn.hpp,cv,24,cv,1
373883,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    struct MVNConfiguration {
        std::vector<std::vector<std::size_t>> input_shapes;

        /*
         * [0, split_axis) = outer range
         * [split_axis, -1] = inner range
         *
         * for each location in the outer range, all the values in the inner range are normalized as a group
         */
        std::size_t split_axis;

        /* The group (described above) is centered always. The following parameter controls whether the variance
         * is also normalized.
         */
        bool normalize_variance;
        float epsilon;
    };

    template <class T>
    class MVNOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MVNOp(csl::Stream stream_, const MVNConfiguration& config)
            : stream(std::move(stream_))
        {
            split_axis = config.split_axis;
            normalize_variance = config.normalize_variance;
            epsilon = c...",16,src\cuda4dnn\primitives\mvn.hpp,cv.dnn,24,dnn,1
373884,NAMESPACE_BLOCK,"namespace cuda4dnn {

    struct MVNConfiguration {
        std::vector<std::vector<std::size_t>> input_shapes;

        /*
         * [0, split_axis) = outer range
         * [split_axis, -1] = inner range
         *
         * for each location in the outer range, all the values in the inner range are normalized as a group
         */
        std::size_t split_axis;

        /* The group (described above) is centered always. The following parameter controls whether the variance
         * is also normalized.
         */
        bool normalize_variance;
        float epsilon;
    };

    template <class T>
    class MVNOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        MVNOp(csl::Stream stream_, const MVNConfiguration& config)
            : stream(std::move(stream_))
        {
            split_axis = config.split_axis;
            normalize_variance = config.normalize_variance;
            epsilon = config.epsilon;

...",32,src\cuda4dnn\primitives\mvn.hpp,cv.dnn.cuda4dnn,24,cuda4dnn,1
374211,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\normalize_bbox.hpp,src\cuda4dnn\primitives\normalize_bbox.hpp:<global>,,<global>,1
374215,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    struct NormalizeConfiguration {
        std::vector<std::size_t> input_shape;

        /* axis range across which values are normalized
         *
         * [0, axis_start) = outer range
         * [axis_start, axis_end) = mid range
         * [axis_end + 1, -1) = inner range
         *
         * for each location in the outer and inner range, all the values in the mid range are
         * normalized together
         */
        std::size_t axis_start, axis_end;

        /* 1 for L1 norm, 2 for L2 norm */
        std::size_t norm;

        /* epsilon to use to avoid division by zero */
        T eps;
    };

    template <class T>
    class NormalizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        template <class V>
        NormalizeOp(csl::Stream stream_, const Mat& weights, const NormalizeConfiguration<V>& config)
            : stream...",1,src\cuda4dnn\primitives\normalize_bbox.hpp,cv,24,cv,1
374216,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    struct NormalizeConfiguration {
        std::vector<std::size_t> input_shape;

        /* axis range across which values are normalized
         *
         * [0, axis_start) = outer range
         * [axis_start, axis_end) = mid range
         * [axis_end + 1, -1) = inner range
         *
         * for each location in the outer and inner range, all the values in the mid range are
         * normalized together
         */
        std::size_t axis_start, axis_end;

        /* 1 for L1 norm, 2 for L2 norm */
        std::size_t norm;

        /* epsilon to use to avoid division by zero */
        T eps;
    };

    template <class T>
    class NormalizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        template <class V>
        NormalizeOp(csl::Stream stream_, const Mat& weights, const NormalizeConfiguration<V>& config)
            : stream(std::move(stre...",16,src\cuda4dnn\primitives\normalize_bbox.hpp,cv.dnn,24,dnn,1
374217,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    struct NormalizeConfiguration {
        std::vector<std::size_t> input_shape;

        /* axis range across which values are normalized
         *
         * [0, axis_start) = outer range
         * [axis_start, axis_end) = mid range
         * [axis_end + 1, -1) = inner range
         *
         * for each location in the outer and inner range, all the values in the mid range are
         * normalized together
         */
        std::size_t axis_start, axis_end;

        /* 1 for L1 norm, 2 for L2 norm */
        std::size_t norm;

        /* epsilon to use to avoid division by zero */
        T eps;
    };

    template <class T>
    class NormalizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        template <class V>
        NormalizeOp(csl::Stream stream_, const Mat& weights, const NormalizeConfiguration<V>& config)
            : stream(std::move(stream_)), weight{ 1...",32,src\cuda4dnn\primitives\normalize_bbox.hpp,cv.dnn.cuda4dnn,24,cuda4dnn,1
374564,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\padding.hpp,src\cuda4dnn\primitives\padding.hpp:<global>,,<global>,1
374568,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    enum class PaddingType {
        CONSTANT,
        REFLECTION101
    };

    template <class T>
    class PaddingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        /* `ranges` is indexed by axis and contains the range in the output where the input is copied to */
        PaddingOp(csl::Stream stream_, PaddingType type_, T value_, std::vector<cv::Range> ranges)
            : stream(std::move(stream_)),  type{ type_ }, value{ value_ }, dstRanges(std::move(ranges))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper-...",1,src\cuda4dnn\primitives\padding.hpp,cv,24,cv,1
374569,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    enum class PaddingType {
        CONSTANT,
        REFLECTION101
    };

    template <class T>
    class PaddingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        /* `ranges` is indexed by axis and contains the range in the output where the input is copied to */
        PaddingOp(csl::Stream stream_, PaddingType type_, T value_, std::vector<cv::Range> ranges)
            : stream(std::move(stream_)),  type{ type_ }, value{ value_ }, dstRanges(std::move(ranges))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

  ...",16,src\cuda4dnn\primitives\padding.hpp,cv.dnn,24,dnn,1
374570,NAMESPACE_BLOCK,"namespace cuda4dnn {

    enum class PaddingType {
        CONSTANT,
        REFLECTION101
    };

    template <class T>
    class PaddingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        /* `ranges` is indexed by axis and contains the range in the output where the input is copied to */
        PaddingOp(csl::Stream stream_, PaddingType type_, T value_, std::vector<cv::Range> ranges)
            : stream(std::move(stream_)),  type{ type_ }, value{ value_ }, dstRanges(std::move(ranges))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto o...",32,src\cuda4dnn\primitives\padding.hpp,cv.dnn.cuda4dnn,24,cuda4dnn,1
374909,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\permute.hpp,src\cuda4dnn\primitives\permute.hpp:<global>,,<global>,1
374913,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class PermuteOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        PermuteOp(csl::Stream stream_, std::vector<std::size_t> order_)
            : stream(std::move(stream_)), order(std::move(order_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                auto needsPermute = [&] {
                    for (int i = 0; i < order.size(); i++)
        ...",1,src\cuda4dnn\primitives\permute.hpp,cv,21,cv,1
374914,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class PermuteOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        PermuteOp(csl::Stream stream_, std::vector<std::size_t> order_)
            : stream(std::move(stream_)), order(std::move(order_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                auto needsPermute = [&] {
                    for (int i = 0; i < order.size(); i++)
                       ...",16,src\cuda4dnn\primitives\permute.hpp,cv.dnn,21,dnn,1
374915,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class PermuteOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        PermuteOp(csl::Stream stream_, std::vector<std::size_t> order_)
            : stream(std::move(stream_)), order(std::move(order_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                auto needsPermute = [&] {
                    for (int i = 0; i < order.size(); i++)
                        if (order[i] !=...",32,src\cuda4dnn\primitives\permute.hpp,cv.dnn.cuda4dnn,21,cuda4dnn,1
375049,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\pooling.hpp,src\cuda4dnn\primitives\pooling.hpp:<global>,,<global>,1
375053,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    struct PoolingConfiguration {
        enum class PoolingMode {
            MAX,
            AVERAGE_INCLUDE_PADDING, /* include padding while calculating average */
            AVERAGE_EXCLUDE_PADDING /* exclude padding while calculating average */
        };

        PoolingMode poolMode;

        /* the size of the following vectors must be equal to the window size */
        std::vector<std::size_t> window_size;
        std::vector<std::size_t> strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        PaddingMode padMode;

        /* explicit paddings are used if and only if padMode is set to manual */
        std::vector<std::size_t> pads_begin, pads_end;

        /* the output shape is calculated using the following...",1,src\cuda4dnn\primitives\pooling.hpp,cv,22,cv,1
375054,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    struct PoolingConfiguration {
        enum class PoolingMode {
            MAX,
            AVERAGE_INCLUDE_PADDING, /* include padding while calculating average */
            AVERAGE_EXCLUDE_PADDING /* exclude padding while calculating average */
        };

        PoolingMode poolMode;

        /* the size of the following vectors must be equal to the window size */
        std::vector<std::size_t> window_size;
        std::vector<std::size_t> strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        PaddingMode padMode;

        /* explicit paddings are used if and only if padMode is set to manual */
        std::vector<std::size_t> pads_begin, pads_end;

        /* the output shape is calculated using the following formula:
     ...",16,src\cuda4dnn\primitives\pooling.hpp,cv.dnn,22,dnn,1
375055,NAMESPACE_BLOCK,"namespace cuda4dnn {

    struct PoolingConfiguration {
        enum class PoolingMode {
            MAX,
            AVERAGE_INCLUDE_PADDING, /* include padding while calculating average */
            AVERAGE_EXCLUDE_PADDING /* exclude padding while calculating average */
        };

        PoolingMode poolMode;

        /* the size of the following vectors must be equal to the window size */
        std::vector<std::size_t> window_size;
        std::vector<std::size_t> strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        PaddingMode padMode;

        /* explicit paddings are used if and only if padMode is set to manual */
        std::vector<std::size_t> pads_begin, pads_end;

        /* the output shape is calculated using the following formula:
         * output_dim...",32,src\cuda4dnn\primitives\pooling.hpp,cv.dnn.cuda4dnn,22,cuda4dnn,1
375943,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\prior_box.hpp,src\cuda4dnn\primitives\prior_box.hpp:<global>,,<global>,1
375947,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    struct PriorBoxConfiguration {
        std::size_t feature_map_width, feature_map_height;
        std::size_t image_width, image_height;

        /* parameters for prior boxes for each feature point */
        std::vector<float> box_widths, box_heights;
        std::vector<float> offsets_x, offsets_y;
        float stepX, stepY;

        std::vector<float> variance;

        /* number of priors per feature point */
        std::size_t num_priors;

        /* clamps the box coordinates to [0, 1] range */
        bool clip;

        /* normalizes the box coordinates using the image dimensions */
        bool normalize;
    };

    template <class T>
    class PriorBoxOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        PriorBoxOp(csl::Stream stream_, const PriorBoxConfiguration& config)
            : stream(std::move(stream_))
        {
            feature_map_width ...",1,src\cuda4dnn\primitives\prior_box.hpp,cv,20,cv,1
375948,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    struct PriorBoxConfiguration {
        std::size_t feature_map_width, feature_map_height;
        std::size_t image_width, image_height;

        /* parameters for prior boxes for each feature point */
        std::vector<float> box_widths, box_heights;
        std::vector<float> offsets_x, offsets_y;
        float stepX, stepY;

        std::vector<float> variance;

        /* number of priors per feature point */
        std::size_t num_priors;

        /* clamps the box coordinates to [0, 1] range */
        bool clip;

        /* normalizes the box coordinates using the image dimensions */
        bool normalize;
    };

    template <class T>
    class PriorBoxOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        PriorBoxOp(csl::Stream stream_, const PriorBoxConfiguration& config)
            : stream(std::move(stream_))
        {
            feature_map_width = config.featur...",16,src\cuda4dnn\primitives\prior_box.hpp,cv.dnn,20,dnn,1
375949,NAMESPACE_BLOCK,"namespace cuda4dnn {

    struct PriorBoxConfiguration {
        std::size_t feature_map_width, feature_map_height;
        std::size_t image_width, image_height;

        /* parameters for prior boxes for each feature point */
        std::vector<float> box_widths, box_heights;
        std::vector<float> offsets_x, offsets_y;
        float stepX, stepY;

        std::vector<float> variance;

        /* number of priors per feature point */
        std::size_t num_priors;

        /* clamps the box coordinates to [0, 1] range */
        bool clip;

        /* normalizes the box coordinates using the image dimensions */
        bool normalize;
    };

    template <class T>
    class PriorBoxOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        PriorBoxOp(csl::Stream stream_, const PriorBoxConfiguration& config)
            : stream(std::move(stream_))
        {
            feature_map_width = config.feature_map_width;
   ...",32,src\cuda4dnn\primitives\prior_box.hpp,cv.dnn.cuda4dnn,20,cuda4dnn,1
376353,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\recurrent_cells.hpp,src\cuda4dnn\primitives\recurrent_cells.hpp:<global>,,<global>,1
376357,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

struct RNNConfiguration
{
    int seqLength;
    int numLayers;
    int hiddenSize;
    int inputSize;
    int miniBatch;
    bool bidirectional;
};

template<class T>
class LSTMOp final : public CUDABackendNode
{
public:
    using wrapper_type = GetCUDABackendWrapperType<T>;

    LSTMOp(csl::Stream stream_, csl::cudnn::Handle handle, const Mat& filters, const Mat& h0,
           const Mat& c0, const RNNConfiguration& config)
            : stream(std::move(stream_))
    {
        typename csl::LSTM<T>::params_type params{
                {filters.total(), 1, 1}, // reshape
                config.seqLength,
                config.numLayers,
                config.hiddenSize,
                config.inputSize,
                config.miniBatch,
                config.bidirectional,
                0.0, /* dropout */
                csl::cudnn::RNNDescriptor<T>::RNNMode::LSTM
        };

        lstm = csl::LSTM<T>(handle, params);
   ...",1,src\cuda4dnn\primitives\recurrent_cells.hpp,cv,14,cv,1
376358,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

struct RNNConfiguration
{
    int seqLength;
    int numLayers;
    int hiddenSize;
    int inputSize;
    int miniBatch;
    bool bidirectional;
};

template<class T>
class LSTMOp final : public CUDABackendNode
{
public:
    using wrapper_type = GetCUDABackendWrapperType<T>;

    LSTMOp(csl::Stream stream_, csl::cudnn::Handle handle, const Mat& filters, const Mat& h0,
           const Mat& c0, const RNNConfiguration& config)
            : stream(std::move(stream_))
    {
        typename csl::LSTM<T>::params_type params{
                {filters.total(), 1, 1}, // reshape
                config.seqLength,
                config.numLayers,
                config.hiddenSize,
                config.inputSize,
                config.miniBatch,
                config.bidirectional,
                0.0, /* dropout */
                csl::cudnn::RNNDescriptor<T>::RNNMode::LSTM
        };

        lstm = csl::LSTM<T>(handle, params);
        auto corre...",16,src\cuda4dnn\primitives\recurrent_cells.hpp,cv.dnn,14,dnn,1
376359,NAMESPACE_BLOCK,"namespace cuda4dnn {

struct RNNConfiguration
{
    int seqLength;
    int numLayers;
    int hiddenSize;
    int inputSize;
    int miniBatch;
    bool bidirectional;
};

template<class T>
class LSTMOp final : public CUDABackendNode
{
public:
    using wrapper_type = GetCUDABackendWrapperType<T>;

    LSTMOp(csl::Stream stream_, csl::cudnn::Handle handle, const Mat& filters, const Mat& h0,
           const Mat& c0, const RNNConfiguration& config)
            : stream(std::move(stream_))
    {
        typename csl::LSTM<T>::params_type params{
                {filters.total(), 1, 1}, // reshape
                config.seqLength,
                config.numLayers,
                config.hiddenSize,
                config.inputSize,
                config.miniBatch,
                config.bidirectional,
                0.0, /* dropout */
                csl::cudnn::RNNDescriptor<T>::RNNMode::LSTM
        };

        lstm = csl::LSTM<T>(handle, params);
        auto correct_shape_filters...",32,src\cuda4dnn\primitives\recurrent_cells.hpp,cv.dnn.cuda4dnn,14,cuda4dnn,1
376649,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\region.hpp,src\cuda4dnn\primitives\region.hpp:<global>,,<global>,1
376653,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    enum class SquashMethod {
        SOFTMAX,
        SIGMOID
    };

    template <class T>
    struct RegionConfiguration {
        /* The image is divided into (H, W) cells.
         *
         * Each cell is interested in exactly one object and predicts `boxes_per_cell` bounding boxes
         * for that object.
         *
         * Each bounding box contains:
         * - 4 box coordinates
         * - objectness confidence score
         * - `classes` number of class scores
         *
         * The object score is reduced to a probability using sigmoid and the class scores are reduced to
         * probabilities by either applying sigmoid or softmax (which is a configuration option).
         *
         * object_prob = sigmoid(object_score)
         * conditional_class_prob = sigmoid, softmax across all classes
         *
         * actual class probability = conditional_class_prob * object_prob
         */
        std::s...",1,src\cuda4dnn\primitives\region.hpp,cv,24,cv,1
376654,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    enum class SquashMethod {
        SOFTMAX,
        SIGMOID
    };

    template <class T>
    struct RegionConfiguration {
        /* The image is divided into (H, W) cells.
         *
         * Each cell is interested in exactly one object and predicts `boxes_per_cell` bounding boxes
         * for that object.
         *
         * Each bounding box contains:
         * - 4 box coordinates
         * - objectness confidence score
         * - `classes` number of class scores
         *
         * The object score is reduced to a probability using sigmoid and the class scores are reduced to
         * probabilities by either applying sigmoid or softmax (which is a configuration option).
         *
         * object_prob = sigmoid(object_score)
         * conditional_class_prob = sigmoid, softmax across all classes
         *
         * actual class probability = conditional_class_prob * object_prob
         */
        std::size_t classes, ...",16,src\cuda4dnn\primitives\region.hpp,cv.dnn,24,dnn,1
376655,NAMESPACE_BLOCK,"namespace cuda4dnn {

    enum class SquashMethod {
        SOFTMAX,
        SIGMOID
    };

    template <class T>
    struct RegionConfiguration {
        /* The image is divided into (H, W) cells.
         *
         * Each cell is interested in exactly one object and predicts `boxes_per_cell` bounding boxes
         * for that object.
         *
         * Each bounding box contains:
         * - 4 box coordinates
         * - objectness confidence score
         * - `classes` number of class scores
         *
         * The object score is reduced to a probability using sigmoid and the class scores are reduced to
         * probabilities by either applying sigmoid or softmax (which is a configuration option).
         *
         * object_prob = sigmoid(object_score)
         * conditional_class_prob = sigmoid, softmax across all classes
         *
         * actual class probability = conditional_class_prob * object_prob
         */
        std::size_t classes, boxes_per_cell;
...",32,src\cuda4dnn\primitives\region.hpp,cv.dnn.cuda4dnn,24,cuda4dnn,1
377141,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\reorg.hpp,src\cuda4dnn\primitives\reorg.hpp:<global>,,<global>,1
377145,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class ReorgOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ReorgOp(csl::Stream stream_, std::size_t stride_)
            : stream(std::move(stream_)), stride{ stride_ } { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            const std::size_t permute_input_shape[] = {
               input.get_axis_size(0),
               input.get_axis_size(1) * input.get_ax...",1,src\cuda4dnn\primitives\reorg.hpp,cv,18,cv,1
377146,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class ReorgOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ReorgOp(csl::Stream stream_, std::size_t stride_)
            : stream(std::move(stream_)), stride{ stride_ } { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            const std::size_t permute_input_shape[] = {
               input.get_axis_size(0),
               input.get_axis_size(1) * input.get_axis_size(2) / (s...",16,src\cuda4dnn\primitives\reorg.hpp,cv.dnn,18,dnn,1
377147,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class ReorgOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ReorgOp(csl::Stream stream_, std::size_t stride_)
            : stream(std::move(stream_)), stride{ stride_ } { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            const std::size_t permute_input_shape[] = {
               input.get_axis_size(0),
               input.get_axis_size(1) * input.get_axis_size(2) / (stride * stride),...",32,src\cuda4dnn\primitives\reorg.hpp,cv.dnn.cuda4dnn,18,cuda4dnn,1
377345,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\reshape.hpp,src\cuda4dnn\primitives\reshape.hpp:<global>,,<global>,1
377349,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class ReshapeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ReshapeOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            /* sometimes the output shape is passed as extra inputs; hence, >= instead of == */
            CV_Assert(inputs.size() >= outputs.size());

            for (int i = 0; i < outputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                if (input....",1,src\cuda4dnn\primitives\reshape.hpp,cv,16,cv,1
377350,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class ReshapeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ReshapeOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            /* sometimes the output shape is passed as extra inputs; hence, >= instead of == */
            CV_Assert(inputs.size() >= outputs.size());

            for (int i = 0; i < outputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                if (input.get() != output...",16,src\cuda4dnn\primitives\reshape.hpp,cv.dnn,16,dnn,1
377351,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class ReshapeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ReshapeOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            /* sometimes the output shape is passed as extra inputs; hence, >= instead of == */
            CV_Assert(inputs.size() >= outputs.size());

            for (int i = 0; i < outputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                if (input.get() != output.get())
        ...",32,src\cuda4dnn\primitives\reshape.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
377515,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\resize.hpp,src\cuda4dnn\primitives\resize.hpp:<global>,,<global>,1
377519,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    enum class InterpolationType {
        NEAREST_NEIGHBOUR,
        BILINEAR
    };

    struct ResizeConfiguration {
        InterpolationType type;
        bool align_corners;
        bool half_pixel_centers;
    };

    template <class T>
    class ResizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ResizeOp(csl::Stream stream_, const ResizeConfiguration& config)
            : stream(std::move(stream_))
        {
            type = config.type;
            align_corners = config.align_corners;
            half_pixel_centers = config.half_pixel_centers;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            // sometimes the target shape is taken from the second input; we don't use it ...",1,src\cuda4dnn\primitives\resize.hpp,cv,16,cv,1
377520,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    enum class InterpolationType {
        NEAREST_NEIGHBOUR,
        BILINEAR
    };

    struct ResizeConfiguration {
        InterpolationType type;
        bool align_corners;
        bool half_pixel_centers;
    };

    template <class T>
    class ResizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ResizeOp(csl::Stream stream_, const ResizeConfiguration& config)
            : stream(std::move(stream_))
        {
            type = config.type;
            align_corners = config.align_corners;
            half_pixel_centers = config.half_pixel_centers;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            // sometimes the target shape is taken from the second input; we don't use it however
       ...",16,src\cuda4dnn\primitives\resize.hpp,cv.dnn,16,dnn,1
377521,NAMESPACE_BLOCK,"namespace cuda4dnn {

    enum class InterpolationType {
        NEAREST_NEIGHBOUR,
        BILINEAR
    };

    struct ResizeConfiguration {
        InterpolationType type;
        bool align_corners;
        bool half_pixel_centers;
    };

    template <class T>
    class ResizeOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ResizeOp(csl::Stream stream_, const ResizeConfiguration& config)
            : stream(std::move(stream_))
        {
            type = config.type;
            align_corners = config.align_corners;
            half_pixel_centers = config.half_pixel_centers;
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            // sometimes the target shape is taken from the second input; we don't use it however
            CV_Assert((...",32,src\cuda4dnn\primitives\resize.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
377713,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\roi_pooling.hpp,src\cuda4dnn\primitives\roi_pooling.hpp:<global>,,<global>,1
377717,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class ROIPoolingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ROIPoolingOp(csl::Stream stream_, float spatial_scale)
            : stream(std::move(stream_)), spatial_scale{spatial_scale} { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 2 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto rois_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto rois = rois_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

     ...",1,src\cuda4dnn\primitives\roi_pooling.hpp,cv,16,cv,1
377718,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class ROIPoolingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ROIPoolingOp(csl::Stream stream_, float spatial_scale)
            : stream(std::move(stream_)), spatial_scale{spatial_scale} { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 2 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto rois_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto rois = rois_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            kernels:...",16,src\cuda4dnn\primitives\roi_pooling.hpp,cv.dnn,16,dnn,1
377719,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class ROIPoolingOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ROIPoolingOp(csl::Stream stream_, float spatial_scale)
            : stream(std::move(stream_)), spatial_scale{spatial_scale} { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 2 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto rois_wrapper = inputs[1].dynamicCast<wrapper_type>();
            auto rois = rois_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            kernels::roi_pooling<T>(...",32,src\cuda4dnn\primitives\roi_pooling.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
377874,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\scale_shift.hpp,src\cuda4dnn\primitives\scale_shift.hpp:<global>,,<global>,1
377878,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    struct ScaleShiftConfiguration {
        enum class OpMode {
            NONE,
            TRAINABLE, /* use a pretrained blob */
            UNTRAINABLE /* use another input */
        };

        OpMode scaleMode;
        OpMode shiftMode;

        std::size_t axis;
    };

    template <class T>
    class ScaleShiftOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ScaleShiftOp(csl::Stream stream_, const ScaleShiftConfiguration& config, const cv::Mat& weights, const cv::Mat& bias)
            : stream(std::move(stream_)), axis{ config.axis }
        {
            scaleMode = config.scaleMode;
            if (scaleMode == ScaleShiftConfiguration::OpMode::TRAINABLE)
            {
                CV_Assert(!weights.empty());
                weightsTensor = csl::makeTensorHeader<T>(weights);
                csl::copyMatToTensor<T>(weights, weightsTensor, stream);
...",1,src\cuda4dnn\primitives\scale_shift.hpp,cv,20,cv,1
377879,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    struct ScaleShiftConfiguration {
        enum class OpMode {
            NONE,
            TRAINABLE, /* use a pretrained blob */
            UNTRAINABLE /* use another input */
        };

        OpMode scaleMode;
        OpMode shiftMode;

        std::size_t axis;
    };

    template <class T>
    class ScaleShiftOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ScaleShiftOp(csl::Stream stream_, const ScaleShiftConfiguration& config, const cv::Mat& weights, const cv::Mat& bias)
            : stream(std::move(stream_)), axis{ config.axis }
        {
            scaleMode = config.scaleMode;
            if (scaleMode == ScaleShiftConfiguration::OpMode::TRAINABLE)
            {
                CV_Assert(!weights.empty());
                weightsTensor = csl::makeTensorHeader<T>(weights);
                csl::copyMatToTensor<T>(weights, weightsTensor, stream);
            }

...",16,src\cuda4dnn\primitives\scale_shift.hpp,cv.dnn,20,dnn,1
377880,NAMESPACE_BLOCK,"namespace cuda4dnn {

    struct ScaleShiftConfiguration {
        enum class OpMode {
            NONE,
            TRAINABLE, /* use a pretrained blob */
            UNTRAINABLE /* use another input */
        };

        OpMode scaleMode;
        OpMode shiftMode;

        std::size_t axis;
    };

    template <class T>
    class ScaleShiftOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ScaleShiftOp(csl::Stream stream_, const ScaleShiftConfiguration& config, const cv::Mat& weights, const cv::Mat& bias)
            : stream(std::move(stream_)), axis{ config.axis }
        {
            scaleMode = config.scaleMode;
            if (scaleMode == ScaleShiftConfiguration::OpMode::TRAINABLE)
            {
                CV_Assert(!weights.empty());
                weightsTensor = csl::makeTensorHeader<T>(weights);
                csl::copyMatToTensor<T>(weights, weightsTensor, stream);
            }

            shif...",32,src\cuda4dnn\primitives\scale_shift.hpp,cv.dnn.cuda4dnn,20,cuda4dnn,1
378355,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\shortcut.hpp,src\cuda4dnn\primitives\shortcut.hpp:<global>,,<global>,1
378359,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class ShortcutOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ShortcutOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            /* output shape is determined by the input shape */
            CV_Assert(is_shape_same(output, input));

            for (int i = 1; i < inputs.size(); i++)
            {
                auto from_wrapp...",1,src\cuda4dnn\primitives\shortcut.hpp,cv,20,cv,1
378360,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class ShortcutOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ShortcutOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            /* output shape is determined by the input shape */
            CV_Assert(is_shape_same(output, input));

            for (int i = 1; i < inputs.size(); i++)
            {
                auto from_wrapper = inputs[i]....",16,src\cuda4dnn\primitives\shortcut.hpp,cv.dnn,20,dnn,1
378361,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class ShortcutOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ShortcutOp(csl::Stream stream_) : stream(std::move(stream_)) { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(outputs.size() == 1);

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            /* output shape is determined by the input shape */
            CV_Assert(is_shape_same(output, input));

            for (int i = 1; i < inputs.size(); i++)
            {
                auto from_wrapper = inputs[i].dynamicCast<wrap...",32,src\cuda4dnn\primitives\shortcut.hpp,cv.dnn.cuda4dnn,20,cuda4dnn,1
378531,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\shuffle_channel.hpp,src\cuda4dnn\primitives\shuffle_channel.hpp:<global>,,<global>,1
378535,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class ShuffleChannelOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ShuffleChannelOp(csl::Stream stream_, std::size_t group_)
            : stream(std::move(stream_)), group{ group_ } { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            if (group == 1) {
                /* permute is redundant; check else branch to know why */
                if (input.ge...",1,src\cuda4dnn\primitives\shuffle_channel.hpp,cv,20,cv,1
378536,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class ShuffleChannelOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ShuffleChannelOp(csl::Stream stream_, std::size_t group_)
            : stream(std::move(stream_)), group{ group_ } { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            if (group == 1) {
                /* permute is redundant; check else branch to know why */
                if (input.get() != output.g...",16,src\cuda4dnn\primitives\shuffle_channel.hpp,cv.dnn,20,dnn,1
378537,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class ShuffleChannelOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        ShuffleChannelOp(csl::Stream stream_, std::size_t group_)
            : stream(std::move(stream_)), group{ group_ } { }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1 && outputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            auto output_wrapper = outputs[0].dynamicCast<wrapper_type>();
            auto output = output_wrapper->getSpan();

            if (group == 1) {
                /* permute is redundant; check else branch to know why */
                if (input.get() != output.get()) {
        ...",32,src\cuda4dnn\primitives\shuffle_channel.hpp,cv.dnn.cuda4dnn,20,cuda4dnn,1
378758,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\slice.hpp,src\cuda4dnn\primitives\slice.hpp:<global>,,<global>,1
378762,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class SliceOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        /* offsets is indexed by output number and each subvector is indexed by axis number */
        SliceOp(csl::Stream stream_, std::vector<std::vector<std::size_t>> offsets)
            : stream(std::move(stream_)), offsets(std::move(offsets))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            /* sometimes the output shape is passed in the form of a second input tensor
             * it's only required for initialization and not here
             */
            CV_Assert(inputs.size() == 1 || inputs.size() == 2);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
          ...",1,src\cuda4dnn\primitives\slice.hpp,cv,22,cv,1
378763,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class SliceOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        /* offsets is indexed by output number and each subvector is indexed by axis number */
        SliceOp(csl::Stream stream_, std::vector<std::vector<std::size_t>> offsets)
            : stream(std::move(stream_)), offsets(std::move(offsets))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            /* sometimes the output shape is passed in the form of a second input tensor
             * it's only required for initialization and not here
             */
            CV_Assert(inputs.size() == 1 || inputs.size() == 2);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = ...",16,src\cuda4dnn\primitives\slice.hpp,cv.dnn,22,dnn,1
378764,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class SliceOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        /* offsets is indexed by output number and each subvector is indexed by axis number */
        SliceOp(csl::Stream stream_, std::vector<std::vector<std::size_t>> offsets)
            : stream(std::move(stream_)), offsets(std::move(offsets))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            /* sometimes the output shape is passed in the form of a second input tensor
             * it's only required for initialization and not here
             */
            CV_Assert(inputs.size() == 1 || inputs.size() == 2);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->g...",32,src\cuda4dnn\primitives\slice.hpp,cv.dnn.cuda4dnn,22,cuda4dnn,1
378874,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\softmax.hpp,src\cuda4dnn\primitives\softmax.hpp:<global>,,<global>,1
378878,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class SoftmaxOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        SoftmaxOp(csl::cudnn::Handle handle, std::size_t axis_, bool log_)
            : cudnnHandle(std::move(handle)), channel_axis{ axis_ }, log{ log_ }
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                csl::tensor_ops::softmax<T>(cudnnHandle, output, input, chan...",1,src\cuda4dnn\primitives\softmax.hpp,cv,16,cv,1
378879,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class SoftmaxOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        SoftmaxOp(csl::cudnn::Handle handle, std::size_t axis_, bool log_)
            : cudnnHandle(std::move(handle)), channel_axis{ axis_ }, log{ log_ }
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                csl::tensor_ops::softmax<T>(cudnnHandle, output, input, channel_axis, log);...",16,src\cuda4dnn\primitives\softmax.hpp,cv.dnn,16,dnn,1
378880,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class SoftmaxOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        SoftmaxOp(csl::cudnn::Handle handle, std::size_t axis_, bool log_)
            : cudnnHandle(std::move(handle)), channel_axis{ axis_ }, log{ log_ }
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            for (int i = 0; i < inputs.size(); i++)
            {
                auto input_wrapper = inputs[i].dynamicCast<wrapper_type>();
                auto input = input_wrapper->getView();

                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                csl::tensor_ops::softmax<T>(cudnnHandle, output, input, channel_axis, log);
            }
 ...",32,src\cuda4dnn\primitives\softmax.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
378969,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\split.hpp,src\cuda4dnn\primitives\split.hpp:<global>,,<global>,1
378973,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    template <class T>
    class SplitOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        SplitOp(csl::Stream stream_)
            : stream(std::move(stream_))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            for (int i = 0; i < outputs.size(); i++)
            {
                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                csl::tensor_ops::copy<T>(stream, output, input);
            }
        }

    private:
        csl::Str...",1,src\cuda4dnn\primitives\split.hpp,cv,17,cv,1
378974,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    template <class T>
    class SplitOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        SplitOp(csl::Stream stream_)
            : stream(std::move(stream_))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            for (int i = 0; i < outputs.size(); i++)
            {
                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                csl::tensor_ops::copy<T>(stream, output, input);
            }
        }

    private:
        csl::Stream stream;
   ...",16,src\cuda4dnn\primitives\split.hpp,cv.dnn,17,dnn,1
378975,NAMESPACE_BLOCK,"namespace cuda4dnn {

    template <class T>
    class SplitOp final : public CUDABackendNode {
    public:
        using wrapper_type = GetCUDABackendWrapperType<T>;

        SplitOp(csl::Stream stream_)
            : stream(std::move(stream_))
        {
        }

        void forward(
            const std::vector<cv::Ptr<BackendWrapper>>& inputs,
            const std::vector<cv::Ptr<BackendWrapper>>& outputs,
            csl::Workspace& workspace) override
        {
            CV_Assert(inputs.size() == 1);

            auto input_wrapper = inputs[0].dynamicCast<wrapper_type>();
            auto input = input_wrapper->getView();

            for (int i = 0; i < outputs.size(); i++)
            {
                auto output_wrapper = outputs[i].dynamicCast<wrapper_type>();
                auto output = output_wrapper->getSpan();

                csl::tensor_ops::copy<T>(stream, output, input);
            }
        }

    private:
        csl::Stream stream;
    };

}",32,src\cuda4dnn\primitives\split.hpp,cv.dnn.cuda4dnn,17,cuda4dnn,1
379090,NAMESPACE_BLOCK,<empty>,,src\cuda4dnn\primitives\transpose_convolution.hpp,src\cuda4dnn\primitives\transpose_convolution.hpp:<global>,,<global>,1
379094,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn {

    struct TransposeConvolutionConfiguration {
        /* other than `input_shape` and `output_shape`, all the configuration values must be provided
         * for the corresponding convolution operation (not transpose convolution)
         */

        /* the size of the following vectors must be equal to the kernel size */
        std::vector<std::size_t> kernel_size;
        std::vector<std::size_t> dilations, strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        /* explicit paddings are used if and only if padMode is set to manual */
        PaddingMode padMode;
        std::vector<std::size_t> pads_begin, pads_end;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_sh...",1,src\cuda4dnn\primitives\transpose_convolution.hpp,cv,25,cv,1
379095,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn {

    struct TransposeConvolutionConfiguration {
        /* other than `input_shape` and `output_shape`, all the configuration values must be provided
         * for the corresponding convolution operation (not transpose convolution)
         */

        /* the size of the following vectors must be equal to the kernel size */
        std::vector<std::size_t> kernel_size;
        std::vector<std::size_t> dilations, strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        /* explicit paddings are used if and only if padMode is set to manual */
        PaddingMode padMode;
        std::vector<std::size_t> pads_begin, pads_end;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_shape;
        st...",16,src\cuda4dnn\primitives\transpose_convolution.hpp,cv.dnn,25,dnn,1
379096,NAMESPACE_BLOCK,"namespace cuda4dnn {

    struct TransposeConvolutionConfiguration {
        /* other than `input_shape` and `output_shape`, all the configuration values must be provided
         * for the corresponding convolution operation (not transpose convolution)
         */

        /* the size of the following vectors must be equal to the kernel size */
        std::vector<std::size_t> kernel_size;
        std::vector<std::size_t> dilations, strides;

        enum class PaddingMode {
            MANUAL, /* uses explicit padding values provided in `pads_begin` and `pads_end` */
            VALID, /* no padding is added */
            SAME /* TensorFlow logic is used for same padding */
        };

        /* explicit paddings are used if and only if padMode is set to manual */
        PaddingMode padMode;
        std::vector<std::size_t> pads_begin, pads_end;

        /* full shape inclusive of channel and batch axis */
        std::vector<std::size_t> input_shape;
        std::vector<std::s...",32,src\cuda4dnn\primitives\transpose_convolution.hpp,cv.dnn.cuda4dnn,25,cuda4dnn,1
379912,NAMESPACE_BLOCK,<empty>,,src\cuda\array.hpp,src\cuda\array.hpp:<global>,,<global>,1
379916,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    template <class T, std::size_t N>
    struct array {
        using value_type        = T;
        using size_type         = device::size_type;
        using difference_type   = std::ptrdiff_t;
        using reference         = typename std::add_lvalue_reference<value_type>::type;
        using const_reference   = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>::type;
        using pointer           = typename std::add_pointer<value_type>::type;
        using const_pointer     = typename std::add_pointer<typename std::add_const<value_type>::type>::type;
        using iterator          = pointer;
        using const_iterator    = const_pointer;
        using reverse_iterator  = std::reverse_iterator<iterator>;
        using const_reverse_iterator = std::reverse_iterator<const_iterator>;

        __host__ __device__ bool empty() const noexcept { return N == 0; }
   ...",1,src\cuda\array.hpp,cv,16,cv,1
379917,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    template <class T, std::size_t N>
    struct array {
        using value_type        = T;
        using size_type         = device::size_type;
        using difference_type   = std::ptrdiff_t;
        using reference         = typename std::add_lvalue_reference<value_type>::type;
        using const_reference   = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>::type;
        using pointer           = typename std::add_pointer<value_type>::type;
        using const_pointer     = typename std::add_pointer<typename std::add_const<value_type>::type>::type;
        using iterator          = pointer;
        using const_iterator    = const_pointer;
        using reverse_iterator  = std::reverse_iterator<iterator>;
        using const_reverse_iterator = std::reverse_iterator<const_iterator>;

        __host__ __device__ bool empty() const noexcept { return N == 0; }
        __host__ _...",16,src\cuda\array.hpp,cv.dnn,16,dnn,1
379918,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

    template <class T, std::size_t N>
    struct array {
        using value_type        = T;
        using size_type         = device::size_type;
        using difference_type   = std::ptrdiff_t;
        using reference         = typename std::add_lvalue_reference<value_type>::type;
        using const_reference   = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>::type;
        using pointer           = typename std::add_pointer<value_type>::type;
        using const_pointer     = typename std::add_pointer<typename std::add_const<value_type>::type>::type;
        using iterator          = pointer;
        using const_iterator    = const_pointer;
        using reverse_iterator  = std::reverse_iterator<iterator>;
        using const_reverse_iterator = std::reverse_iterator<const_iterator>;

        __host__ __device__ bool empty() const noexcept { return N == 0; }
        __host__ __device__ size_t...",32,src\cuda\array.hpp,cv.dnn.cuda4dnn,16,cuda4dnn,1
379919,NAMESPACE_BLOCK,"namespace csl { namespace device {

    template <class T, std::size_t N>
    struct array {
        using value_type        = T;
        using size_type         = device::size_type;
        using difference_type   = std::ptrdiff_t;
        using reference         = typename std::add_lvalue_reference<value_type>::type;
        using const_reference   = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>::type;
        using pointer           = typename std::add_pointer<value_type>::type;
        using const_pointer     = typename std::add_pointer<typename std::add_const<value_type>::type>::type;
        using iterator          = pointer;
        using const_iterator    = const_pointer;
        using reverse_iterator  = std::reverse_iterator<iterator>;
        using const_reverse_iterator = std::reverse_iterator<const_iterator>;

        __host__ __device__ bool empty() const noexcept { return N == 0; }
        __host__ __device__ size_type size() const noex...",53,src\cuda\array.hpp,cv.dnn.cuda4dnn.csl,16,csl,1
379920,NAMESPACE_BLOCK,"namespace device {

    template <class T, std::size_t N>
    struct array {
        using value_type        = T;
        using size_type         = device::size_type;
        using difference_type   = std::ptrdiff_t;
        using reference         = typename std::add_lvalue_reference<value_type>::type;
        using const_reference   = typename std::add_lvalue_reference<typename std::add_const<value_type>::type>::type;
        using pointer           = typename std::add_pointer<value_type>::type;
        using const_pointer     = typename std::add_pointer<typename std::add_const<value_type>::type>::type;
        using iterator          = pointer;
        using const_iterator    = const_pointer;
        using reverse_iterator  = std::reverse_iterator<iterator>;
        using const_reverse_iterator = std::reverse_iterator<const_iterator>;

        __host__ __device__ bool empty() const noexcept { return N == 0; }
        __host__ __device__ size_type size() const noexcept { return N;...",69,src\cuda\array.hpp,cv.dnn.cuda4dnn.csl.device,16,device,1
380132,NAMESPACE_BLOCK,<empty>,,src\cuda\atomics.hpp,src\cuda\atomics.hpp:<global>,,<global>,1
380141,NAMESPACE_BLOCK,<empty>,,src\cuda\bbox_utils.hpp,src\cuda\bbox_utils.hpp:<global>,,<global>,1
380145,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

    struct BoundingBox
    {
        float xmin, ymin, xmax, ymax;
    };

    template <bool NORMALIZED_BBOX>
    __device__ __forceinline__ float compute_bbox_size(BoundingBox bbox)
    {
        float width = bbox.xmax - bbox.xmin;
        float height = bbox.ymax - bbox.ymin;
        if (width < 0 || height < 0)
            return 0.0;

        if (!NORMALIZED_BBOX)
        {
            width += 1;
            height += 1;
        }

        using csl::device::mul_ftz;
        return mul_ftz(width, height);
    }

}}}}",1,src\cuda\bbox_utils.hpp,cv,12,cv,1
380146,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

    struct BoundingBox
    {
        float xmin, ymin, xmax, ymax;
    };

    template <bool NORMALIZED_BBOX>
    __device__ __forceinline__ float compute_bbox_size(BoundingBox bbox)
    {
        float width = bbox.xmax - bbox.xmin;
        float height = bbox.ymax - bbox.ymin;
        if (width < 0 || height < 0)
            return 0.0;

        if (!NORMALIZED_BBOX)
        {
            width += 1;
            height += 1;
        }

        using csl::device::mul_ftz;
        return mul_ftz(width, height);
    }

}}}",16,src\cuda\bbox_utils.hpp,cv.dnn,12,dnn,1
380147,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

    struct BoundingBox
    {
        float xmin, ymin, xmax, ymax;
    };

    template <bool NORMALIZED_BBOX>
    __device__ __forceinline__ float compute_bbox_size(BoundingBox bbox)
    {
        float width = bbox.xmax - bbox.xmin;
        float height = bbox.ymax - bbox.ymin;
        if (width < 0 || height < 0)
            return 0.0;

        if (!NORMALIZED_BBOX)
        {
            width += 1;
            height += 1;
        }

        using csl::device::mul_ftz;
        return mul_ftz(width, height);
    }

}}",32,src\cuda\bbox_utils.hpp,cv.dnn.cuda4dnn,12,cuda4dnn,1
380148,NAMESPACE_BLOCK,"namespace kernels {

    struct BoundingBox
    {
        float xmin, ymin, xmax, ymax;
    };

    template <bool NORMALIZED_BBOX>
    __device__ __forceinline__ float compute_bbox_size(BoundingBox bbox)
    {
        float width = bbox.xmax - bbox.xmin;
        float height = bbox.ymax - bbox.ymin;
        if (width < 0 || height < 0)
            return 0.0;

        if (!NORMALIZED_BBOX)
        {
            width += 1;
            height += 1;
        }

        using csl::device::mul_ftz;
        return mul_ftz(width, height);
    }

}",53,src\cuda\bbox_utils.hpp,cv.dnn.cuda4dnn.kernels,12,kernels,1
380162,NAMESPACE_BLOCK,<empty>,,src\cuda\block_stride_range.hpp,src\cuda\block_stride_range.hpp:<global>,,<global>,1
380166,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

template <int dim, int BLOCK_SIZE = 0, class index_type = device::index_type, class size_type = device::size_type>
class block_stride_range_generic {
public:
    __device__ block_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ block_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
         * through the indices using a range based for loop
         */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            const index_type block_size = BLOCK_SIZE == 0 ? getBlockDim<dim>() : BLOCK_SIZE;
            pos += block_size;
            return *this;
        }

        __device__ bool operator!=(const...",1,src\cuda\block_stride_range.hpp,cv,13,cv,1
380167,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

template <int dim, int BLOCK_SIZE = 0, class index_type = device::index_type, class size_type = device::size_type>
class block_stride_range_generic {
public:
    __device__ block_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ block_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
         * through the indices using a range based for loop
         */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            const index_type block_size = BLOCK_SIZE == 0 ? getBlockDim<dim>() : BLOCK_SIZE;
            pos += block_size;
            return *this;
        }

        __device__ bool operator!=(const iterator& othe...",16,src\cuda\block_stride_range.hpp,cv.dnn,13,dnn,1
380168,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

template <int dim, int BLOCK_SIZE = 0, class index_type = device::index_type, class size_type = device::size_type>
class block_stride_range_generic {
public:
    __device__ block_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ block_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
         * through the indices using a range based for loop
         */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            const index_type block_size = BLOCK_SIZE == 0 ? getBlockDim<dim>() : BLOCK_SIZE;
            pos += block_size;
            return *this;
        }

        __device__ bool operator!=(const iterator& other) const {
     ...",32,src\cuda\block_stride_range.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
380169,NAMESPACE_BLOCK,"namespace csl { namespace device {

template <int dim, int BLOCK_SIZE = 0, class index_type = device::index_type, class size_type = device::size_type>
class block_stride_range_generic {
public:
    __device__ block_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ block_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
         * through the indices using a range based for loop
         */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            const index_type block_size = BLOCK_SIZE == 0 ? getBlockDim<dim>() : BLOCK_SIZE;
            pos += block_size;
            return *this;
        }

        __device__ bool operator!=(const iterator& other) const {
            /* NOTE HACK
 ...",53,src\cuda\block_stride_range.hpp,cv.dnn.cuda4dnn.csl,13,csl,1
380170,NAMESPACE_BLOCK,"namespace device {

template <int dim, int BLOCK_SIZE = 0, class index_type = device::index_type, class size_type = device::size_type>
class block_stride_range_generic {
public:
    __device__ block_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ block_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
         * through the indices using a range based for loop
         */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            const index_type block_size = BLOCK_SIZE == 0 ? getBlockDim<dim>() : BLOCK_SIZE;
            pos += block_size;
            return *this;
        }

        __device__ bool operator!=(const iterator& other) const {
            /* NOTE HACK
             * 'p...",69,src\cuda\block_stride_range.hpp,cv.dnn.cuda4dnn.csl.device,13,device,1
380264,NAMESPACE_BLOCK,<empty>,,src\cuda\execution.hpp,src\cuda\execution.hpp:<global>,,<global>,1
380268,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl {

    struct execution_policy {
        execution_policy(dim3 grid_size, dim3 block_size)
            : grid{ grid_size }, block{ block_size }, sharedMem{ 0 }, stream{ 0 } { }

        execution_policy(dim3 grid_size, dim3 block_size, std::size_t shared_mem)
            : grid{ grid_size }, block{ block_size }, sharedMem{ shared_mem }, stream{ nullptr } { }

        execution_policy(dim3 grid_size, dim3 block_size, const Stream& strm)
            : grid{ grid_size }, block{ block_size }, sharedMem{ 0 }, stream{ strm.get() } { }

        execution_policy(dim3 grid_size, dim3 block_size, std::size_t shared_mem, const Stream& strm)
            : grid{ grid_size }, block{ block_size }, sharedMem{ shared_mem }, stream{ strm.get() } { }

        dim3 grid;
        dim3 block;
        std::size_t sharedMem;
        cudaStream_t stream;
    };

    /* this overload shouldn't be necessary; we should always provide a bound on ...",1,src\cuda\execution.hpp,cv,17,cv,1
380269,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl {

    struct execution_policy {
        execution_policy(dim3 grid_size, dim3 block_size)
            : grid{ grid_size }, block{ block_size }, sharedMem{ 0 }, stream{ 0 } { }

        execution_policy(dim3 grid_size, dim3 block_size, std::size_t shared_mem)
            : grid{ grid_size }, block{ block_size }, sharedMem{ shared_mem }, stream{ nullptr } { }

        execution_policy(dim3 grid_size, dim3 block_size, const Stream& strm)
            : grid{ grid_size }, block{ block_size }, sharedMem{ 0 }, stream{ strm.get() } { }

        execution_policy(dim3 grid_size, dim3 block_size, std::size_t shared_mem, const Stream& strm)
            : grid{ grid_size }, block{ block_size }, sharedMem{ shared_mem }, stream{ strm.get() } { }

        dim3 grid;
        dim3 block;
        std::size_t sharedMem;
        cudaStream_t stream;
    };

    /* this overload shouldn't be necessary; we should always provide a bound on the number of t...",16,src\cuda\execution.hpp,cv.dnn,17,dnn,1
380270,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl {

    struct execution_policy {
        execution_policy(dim3 grid_size, dim3 block_size)
            : grid{ grid_size }, block{ block_size }, sharedMem{ 0 }, stream{ 0 } { }

        execution_policy(dim3 grid_size, dim3 block_size, std::size_t shared_mem)
            : grid{ grid_size }, block{ block_size }, sharedMem{ shared_mem }, stream{ nullptr } { }

        execution_policy(dim3 grid_size, dim3 block_size, const Stream& strm)
            : grid{ grid_size }, block{ block_size }, sharedMem{ 0 }, stream{ strm.get() } { }

        execution_policy(dim3 grid_size, dim3 block_size, std::size_t shared_mem, const Stream& strm)
            : grid{ grid_size }, block{ block_size }, sharedMem{ shared_mem }, stream{ strm.get() } { }

        dim3 grid;
        dim3 block;
        std::size_t sharedMem;
        cudaStream_t stream;
    };

    /* this overload shouldn't be necessary; we should always provide a bound on the number of threads */
    /*...",32,src\cuda\execution.hpp,cv.dnn.cuda4dnn,17,cuda4dnn,1
380271,NAMESPACE_BLOCK,"namespace csl {

    struct execution_policy {
        execution_policy(dim3 grid_size, dim3 block_size)
            : grid{ grid_size }, block{ block_size }, sharedMem{ 0 }, stream{ 0 } { }

        execution_policy(dim3 grid_size, dim3 block_size, std::size_t shared_mem)
            : grid{ grid_size }, block{ block_size }, sharedMem{ shared_mem }, stream{ nullptr } { }

        execution_policy(dim3 grid_size, dim3 block_size, const Stream& strm)
            : grid{ grid_size }, block{ block_size }, sharedMem{ 0 }, stream{ strm.get() } { }

        execution_policy(dim3 grid_size, dim3 block_size, std::size_t shared_mem, const Stream& strm)
            : grid{ grid_size }, block{ block_size }, sharedMem{ shared_mem }, stream{ strm.get() } { }

        dim3 grid;
        dim3 block;
        std::size_t sharedMem;
        cudaStream_t stream;
    };

    /* this overload shouldn't be necessary; we should always provide a bound on the number of threads */
    /*
    template <class ...",53,src\cuda\execution.hpp,cv.dnn.cuda4dnn.csl,17,csl,1
380423,NAMESPACE_BLOCK,<empty>,,src\cuda\functors.hpp,src\cuda\functors.hpp:<global>,,<global>,1
380427,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace kernels {

template <class T>
struct IdentityFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() { }
    };

    CUDA4DNN_DEVICE IdentityFunctor() { }
    CUDA4DNN_DEVICE IdentityFunctor(const Params& params) { }

    CUDA4DNN_DEVICE T operator()(T value) {
        return value;
    };
};

template <class T>
struct ReLUFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() : slope(0) { }
        CUDA4DNN_HOST_DEVICE Params(T slope_) : slope(slope_) { }
        T slope;
    };

    CUDA4DNN_DEVICE ReLUFunctor() : ReLUFunctor(Params{}) { }
    CUDA4DNN_DEVICE ReLUFunctor(const Params& params) : slope(params.slope) { }

    CUDA4DNN_DEVICE T operator()(T value) {
        using csl::device::log1pexp;
        return value >= T(0) ? value : slope * value;
    }

    T slope;
};

template <class T>
struct ClippedReLUFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() : floor(0), ceili...",1,src\cuda\functors.hpp,cv,14,cv,1
380428,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace kernels {

template <class T>
struct IdentityFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() { }
    };

    CUDA4DNN_DEVICE IdentityFunctor() { }
    CUDA4DNN_DEVICE IdentityFunctor(const Params& params) { }

    CUDA4DNN_DEVICE T operator()(T value) {
        return value;
    };
};

template <class T>
struct ReLUFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() : slope(0) { }
        CUDA4DNN_HOST_DEVICE Params(T slope_) : slope(slope_) { }
        T slope;
    };

    CUDA4DNN_DEVICE ReLUFunctor() : ReLUFunctor(Params{}) { }
    CUDA4DNN_DEVICE ReLUFunctor(const Params& params) : slope(params.slope) { }

    CUDA4DNN_DEVICE T operator()(T value) {
        using csl::device::log1pexp;
        return value >= T(0) ? value : slope * value;
    }

    T slope;
};

template <class T>
struct ClippedReLUFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() : floor(0), ceiling(6) { }
     ...",16,src\cuda\functors.hpp,cv.dnn,14,dnn,1
380429,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace kernels {

template <class T>
struct IdentityFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() { }
    };

    CUDA4DNN_DEVICE IdentityFunctor() { }
    CUDA4DNN_DEVICE IdentityFunctor(const Params& params) { }

    CUDA4DNN_DEVICE T operator()(T value) {
        return value;
    };
};

template <class T>
struct ReLUFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() : slope(0) { }
        CUDA4DNN_HOST_DEVICE Params(T slope_) : slope(slope_) { }
        T slope;
    };

    CUDA4DNN_DEVICE ReLUFunctor() : ReLUFunctor(Params{}) { }
    CUDA4DNN_DEVICE ReLUFunctor(const Params& params) : slope(params.slope) { }

    CUDA4DNN_DEVICE T operator()(T value) {
        using csl::device::log1pexp;
        return value >= T(0) ? value : slope * value;
    }

    T slope;
};

template <class T>
struct ClippedReLUFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() : floor(0), ceiling(6) { }
        CUDA4DNN_HOST...",32,src\cuda\functors.hpp,cv.dnn.cuda4dnn,14,cuda4dnn,1
380430,NAMESPACE_BLOCK,"namespace kernels {

template <class T>
struct IdentityFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() { }
    };

    CUDA4DNN_DEVICE IdentityFunctor() { }
    CUDA4DNN_DEVICE IdentityFunctor(const Params& params) { }

    CUDA4DNN_DEVICE T operator()(T value) {
        return value;
    };
};

template <class T>
struct ReLUFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() : slope(0) { }
        CUDA4DNN_HOST_DEVICE Params(T slope_) : slope(slope_) { }
        T slope;
    };

    CUDA4DNN_DEVICE ReLUFunctor() : ReLUFunctor(Params{}) { }
    CUDA4DNN_DEVICE ReLUFunctor(const Params& params) : slope(params.slope) { }

    CUDA4DNN_DEVICE T operator()(T value) {
        using csl::device::log1pexp;
        return value >= T(0) ? value : slope * value;
    }

    T slope;
};

template <class T>
struct ClippedReLUFunctor {
    struct Params {
        CUDA4DNN_HOST_DEVICE Params() : floor(0), ceiling(6) { }
        CUDA4DNN_HOST_DEVICE Params(T floo...",53,src\cuda\functors.hpp,cv.dnn.cuda4dnn.kernels,14,kernels,1
381892,NAMESPACE_BLOCK,<empty>,,src\cuda\grid_stride_range.hpp,src\cuda\grid_stride_range.hpp:<global>,,<global>,1
381896,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

template <int dim, class index_type = device::index_type, class size_type = device::size_type>
class grid_stride_range_generic {
public:
    __device__ grid_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ grid_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
            * through the indices using a range based for loop
            */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            pos += getGridDim<dim>() * static_cast<index_type>(getBlockDim<dim>());
            return *this;
        }

        __device__ bool operator!=(const iterator& other) const {
            /* NOTE HACK
      ...",1,src\cuda\grid_stride_range.hpp,cv,13,cv,1
381897,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

template <int dim, class index_type = device::index_type, class size_type = device::size_type>
class grid_stride_range_generic {
public:
    __device__ grid_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ grid_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
            * through the indices using a range based for loop
            */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            pos += getGridDim<dim>() * static_cast<index_type>(getBlockDim<dim>());
            return *this;
        }

        __device__ bool operator!=(const iterator& other) const {
            /* NOTE HACK
                * 'po...",16,src\cuda\grid_stride_range.hpp,cv.dnn,13,dnn,1
381898,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

template <int dim, class index_type = device::index_type, class size_type = device::size_type>
class grid_stride_range_generic {
public:
    __device__ grid_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ grid_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
            * through the indices using a range based for loop
            */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            pos += getGridDim<dim>() * static_cast<index_type>(getBlockDim<dim>());
            return *this;
        }

        __device__ bool operator!=(const iterator& other) const {
            /* NOTE HACK
                * 'pos' can move in l...",32,src\cuda\grid_stride_range.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
381899,NAMESPACE_BLOCK,"namespace csl { namespace device {

template <int dim, class index_type = device::index_type, class size_type = device::size_type>
class grid_stride_range_generic {
public:
    __device__ grid_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ grid_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
            * through the indices using a range based for loop
            */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            pos += getGridDim<dim>() * static_cast<index_type>(getBlockDim<dim>());
            return *this;
        }

        __device__ bool operator!=(const iterator& other) const {
            /* NOTE HACK
                * 'pos' can move in large steps (see opera...",53,src\cuda\grid_stride_range.hpp,cv.dnn.cuda4dnn.csl,13,csl,1
381900,NAMESPACE_BLOCK,"namespace device {

template <int dim, class index_type = device::index_type, class size_type = device::size_type>
class grid_stride_range_generic {
public:
    __device__ grid_stride_range_generic(index_type to_) : from(0), to(to_) { }
    __device__ grid_stride_range_generic(index_type from_, index_type to_) : from(from_), to(to_) { }

    class iterator
    {
    public:
        __device__ iterator(index_type pos_) : pos(pos_) {}

        /* these iterators return the index when dereferenced; this allows us to loop
            * through the indices using a range based for loop
            */
        __device__ index_type operator*() const { return pos; }

        __device__ iterator& operator++() {
            pos += getGridDim<dim>() * static_cast<index_type>(getBlockDim<dim>());
            return *this;
        }

        __device__ bool operator!=(const iterator& other) const {
            /* NOTE HACK
                * 'pos' can move in large steps (see operator++)
         ...",69,src\cuda\grid_stride_range.hpp,cv.dnn.cuda4dnn.csl.device,13,device,1
381987,NAMESPACE_BLOCK,<empty>,,src\cuda\index_helpers.hpp,src\cuda\index_helpers.hpp:<global>,,<global>,1
381991,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

namespace detail {
    using dim3_member_type = decltype(dim3::x);
    using uint3_member_type = decltype(uint3::x);
}

template <int>  __device__ detail::dim3_member_type getGridDim();
template <> inline __device__ detail::dim3_member_type getGridDim<0>() { return gridDim.x; }
template <> inline __device__ detail::dim3_member_type getGridDim<1>() { return gridDim.y; }
template <> inline __device__ detail::dim3_member_type getGridDim<2>() { return gridDim.z; }

template <int> __device__ detail::dim3_member_type getBlockDim();
template <> inline __device__ detail::dim3_member_type getBlockDim<0>() { return blockDim.x; }
template <> inline __device__ detail::dim3_member_type getBlockDim<1>() { return blockDim.y; }
template <> inline __device__ detail::dim3_member_type getBlockDim<2>() { return blockDim.z; }

template <int> __device__ detail::uint3_member_type getBlockIdx();
template <> inline __dev...",1,src\cuda\index_helpers.hpp,cv,12,cv,1
381992,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

namespace detail {
    using dim3_member_type = decltype(dim3::x);
    using uint3_member_type = decltype(uint3::x);
}

template <int>  __device__ detail::dim3_member_type getGridDim();
template <> inline __device__ detail::dim3_member_type getGridDim<0>() { return gridDim.x; }
template <> inline __device__ detail::dim3_member_type getGridDim<1>() { return gridDim.y; }
template <> inline __device__ detail::dim3_member_type getGridDim<2>() { return gridDim.z; }

template <int> __device__ detail::dim3_member_type getBlockDim();
template <> inline __device__ detail::dim3_member_type getBlockDim<0>() { return blockDim.x; }
template <> inline __device__ detail::dim3_member_type getBlockDim<1>() { return blockDim.y; }
template <> inline __device__ detail::dim3_member_type getBlockDim<2>() { return blockDim.z; }

template <int> __device__ detail::uint3_member_type getBlockIdx();
template <> inline __device__ detail::u...",16,src\cuda\index_helpers.hpp,cv.dnn,12,dnn,1
381993,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

namespace detail {
    using dim3_member_type = decltype(dim3::x);
    using uint3_member_type = decltype(uint3::x);
}

template <int>  __device__ detail::dim3_member_type getGridDim();
template <> inline __device__ detail::dim3_member_type getGridDim<0>() { return gridDim.x; }
template <> inline __device__ detail::dim3_member_type getGridDim<1>() { return gridDim.y; }
template <> inline __device__ detail::dim3_member_type getGridDim<2>() { return gridDim.z; }

template <int> __device__ detail::dim3_member_type getBlockDim();
template <> inline __device__ detail::dim3_member_type getBlockDim<0>() { return blockDim.x; }
template <> inline __device__ detail::dim3_member_type getBlockDim<1>() { return blockDim.y; }
template <> inline __device__ detail::dim3_member_type getBlockDim<2>() { return blockDim.z; }

template <int> __device__ detail::uint3_member_type getBlockIdx();
template <> inline __device__ detail::uint3_member_type...",32,src\cuda\index_helpers.hpp,cv.dnn.cuda4dnn,12,cuda4dnn,1
381994,NAMESPACE_BLOCK,"namespace csl { namespace device {

namespace detail {
    using dim3_member_type = decltype(dim3::x);
    using uint3_member_type = decltype(uint3::x);
}

template <int>  __device__ detail::dim3_member_type getGridDim();
template <> inline __device__ detail::dim3_member_type getGridDim<0>() { return gridDim.x; }
template <> inline __device__ detail::dim3_member_type getGridDim<1>() { return gridDim.y; }
template <> inline __device__ detail::dim3_member_type getGridDim<2>() { return gridDim.z; }

template <int> __device__ detail::dim3_member_type getBlockDim();
template <> inline __device__ detail::dim3_member_type getBlockDim<0>() { return blockDim.x; }
template <> inline __device__ detail::dim3_member_type getBlockDim<1>() { return blockDim.y; }
template <> inline __device__ detail::dim3_member_type getBlockDim<2>() { return blockDim.z; }

template <int> __device__ detail::uint3_member_type getBlockIdx();
template <> inline __device__ detail::uint3_member_type getBlockIdx<0>() { r...",53,src\cuda\index_helpers.hpp,cv.dnn.cuda4dnn.csl,12,csl,1
381995,NAMESPACE_BLOCK,"namespace device {

namespace detail {
    using dim3_member_type = decltype(dim3::x);
    using uint3_member_type = decltype(uint3::x);
}

template <int>  __device__ detail::dim3_member_type getGridDim();
template <> inline __device__ detail::dim3_member_type getGridDim<0>() { return gridDim.x; }
template <> inline __device__ detail::dim3_member_type getGridDim<1>() { return gridDim.y; }
template <> inline __device__ detail::dim3_member_type getGridDim<2>() { return gridDim.z; }

template <int> __device__ detail::dim3_member_type getBlockDim();
template <> inline __device__ detail::dim3_member_type getBlockDim<0>() { return blockDim.x; }
template <> inline __device__ detail::dim3_member_type getBlockDim<1>() { return blockDim.y; }
template <> inline __device__ detail::dim3_member_type getBlockDim<2>() { return blockDim.z; }

template <int> __device__ detail::uint3_member_type getBlockIdx();
template <> inline __device__ detail::uint3_member_type getBlockIdx<0>() { return blockIdx.x...",69,src\cuda\index_helpers.hpp,cv.dnn.cuda4dnn.csl.device,12,device,1
381996,NAMESPACE_BLOCK,"namespace detail {
    using dim3_member_type = decltype(dim3::x);
    using uint3_member_type = decltype(uint3::x);
}",1,src\cuda\index_helpers.hpp,cv.dnn.cuda4dnn.csl.device.detail,14,detail,1
382020,NAMESPACE_BLOCK,<empty>,,src\cuda\kernel_dispatcher.hpp,src\cuda\kernel_dispatcher.hpp:<global>,,<global>,1
382031,NAMESPACE_BLOCK,<empty>,,src\cuda\limits.hpp,src\cuda\limits.hpp:<global>,,<global>,1
382035,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    template <class T>
    struct numeric_limits;

#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <>
    struct numeric_limits<__half> {
        __device__ static __half min() { return 0.0000610; }
        __device__ static __half max() { return 65504.0; }
        __device__ static __half lowest() { return -65504.0; }
    };
#endif

    template <>
    struct numeric_limits<float> {
        __device__ static float min() { return FLT_MIN; }
        __device__ static float max() { return FLT_MAX; }
        __device__ static float lowest() { return -FLT_MAX; }
    };

}}}}}",1,src\cuda\limits.hpp,cv,13,cv,1
382036,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    template <class T>
    struct numeric_limits;

#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <>
    struct numeric_limits<__half> {
        __device__ static __half min() { return 0.0000610; }
        __device__ static __half max() { return 65504.0; }
        __device__ static __half lowest() { return -65504.0; }
    };
#endif

    template <>
    struct numeric_limits<float> {
        __device__ static float min() { return FLT_MIN; }
        __device__ static float max() { return FLT_MAX; }
        __device__ static float lowest() { return -FLT_MAX; }
    };

}}}}",16,src\cuda\limits.hpp,cv.dnn,13,dnn,1
382037,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

    template <class T>
    struct numeric_limits;

#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <>
    struct numeric_limits<__half> {
        __device__ static __half min() { return 0.0000610; }
        __device__ static __half max() { return 65504.0; }
        __device__ static __half lowest() { return -65504.0; }
    };
#endif

    template <>
    struct numeric_limits<float> {
        __device__ static float min() { return FLT_MIN; }
        __device__ static float max() { return FLT_MAX; }
        __device__ static float lowest() { return -FLT_MAX; }
    };

}}}",32,src\cuda\limits.hpp,cv.dnn.cuda4dnn,13,cuda4dnn,1
382038,NAMESPACE_BLOCK,"namespace csl { namespace device {

    template <class T>
    struct numeric_limits;

#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <>
    struct numeric_limits<__half> {
        __device__ static __half min() { return 0.0000610; }
        __device__ static __half max() { return 65504.0; }
        __device__ static __half lowest() { return -65504.0; }
    };
#endif

    template <>
    struct numeric_limits<float> {
        __device__ static float min() { return FLT_MIN; }
        __device__ static float max() { return FLT_MAX; }
        __device__ static float lowest() { return -FLT_MAX; }
    };

}}",53,src\cuda\limits.hpp,cv.dnn.cuda4dnn.csl,13,csl,1
382039,NAMESPACE_BLOCK,"namespace device {

    template <class T>
    struct numeric_limits;

#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <>
    struct numeric_limits<__half> {
        __device__ static __half min() { return 0.0000610; }
        __device__ static __half max() { return 65504.0; }
        __device__ static __half lowest() { return -65504.0; }
    };
#endif

    template <>
    struct numeric_limits<float> {
        __device__ static float min() { return FLT_MIN; }
        __device__ static float max() { return FLT_MAX; }
        __device__ static float lowest() { return -FLT_MAX; }
    };

}",69,src\cuda\limits.hpp,cv.dnn.cuda4dnn.csl.device,13,device,1
382092,NAMESPACE_BLOCK,<empty>,,src\cuda\math.hpp,src\cuda\math.hpp:<global>,,<global>,1
382096,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    template <class T> __device__ T abs(T val) { return (val < T(0) ? -val : val); }
    template <> inline __device__ float abs(float val) { return fabsf(val); }
    template <> inline __device__ double abs(double val) { return fabs(val); }

    template <class T> __device__ T exp(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half exp(__half val) { return hexp(val); }
#endif
    template <> inline __device__ float exp(float val) { return expf(val); }
    template <> inline __device__ double exp(double val) { return ::exp(val); }

    template <class T> __device__ T expm1(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half expm1(__half val) { return hexp(val) - __half(1); }
#endif
    template <> inline __device__ float expm1(float val) { return expm1f(val); }
    template <> inline __device__ d...",1,src\cuda\math.hpp,cv,11,cv,1
382097,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    template <class T> __device__ T abs(T val) { return (val < T(0) ? -val : val); }
    template <> inline __device__ float abs(float val) { return fabsf(val); }
    template <> inline __device__ double abs(double val) { return fabs(val); }

    template <class T> __device__ T exp(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half exp(__half val) { return hexp(val); }
#endif
    template <> inline __device__ float exp(float val) { return expf(val); }
    template <> inline __device__ double exp(double val) { return ::exp(val); }

    template <class T> __device__ T expm1(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half expm1(__half val) { return hexp(val) - __half(1); }
#endif
    template <> inline __device__ float expm1(float val) { return expm1f(val); }
    template <> inline __device__ double expm1(dou...",16,src\cuda\math.hpp,cv.dnn,11,dnn,1
382098,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

    template <class T> __device__ T abs(T val) { return (val < T(0) ? -val : val); }
    template <> inline __device__ float abs(float val) { return fabsf(val); }
    template <> inline __device__ double abs(double val) { return fabs(val); }

    template <class T> __device__ T exp(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half exp(__half val) { return hexp(val); }
#endif
    template <> inline __device__ float exp(float val) { return expf(val); }
    template <> inline __device__ double exp(double val) { return ::exp(val); }

    template <class T> __device__ T expm1(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half expm1(__half val) { return hexp(val) - __half(1); }
#endif
    template <> inline __device__ float expm1(float val) { return expm1f(val); }
    template <> inline __device__ double expm1(double val) { retur...",32,src\cuda\math.hpp,cv.dnn.cuda4dnn,11,cuda4dnn,1
382099,NAMESPACE_BLOCK,"namespace csl { namespace device {

    template <class T> __device__ T abs(T val) { return (val < T(0) ? -val : val); }
    template <> inline __device__ float abs(float val) { return fabsf(val); }
    template <> inline __device__ double abs(double val) { return fabs(val); }

    template <class T> __device__ T exp(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half exp(__half val) { return hexp(val); }
#endif
    template <> inline __device__ float exp(float val) { return expf(val); }
    template <> inline __device__ double exp(double val) { return ::exp(val); }

    template <class T> __device__ T expm1(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half expm1(__half val) { return hexp(val) - __half(1); }
#endif
    template <> inline __device__ float expm1(float val) { return expm1f(val); }
    template <> inline __device__ double expm1(double val) { return ::expm1(val); }

  ...",53,src\cuda\math.hpp,cv.dnn.cuda4dnn.csl,11,csl,1
382100,NAMESPACE_BLOCK,"namespace device {

    template <class T> __device__ T abs(T val) { return (val < T(0) ? -val : val); }
    template <> inline __device__ float abs(float val) { return fabsf(val); }
    template <> inline __device__ double abs(double val) { return fabs(val); }

    template <class T> __device__ T exp(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half exp(__half val) { return hexp(val); }
#endif
    template <> inline __device__ float exp(float val) { return expf(val); }
    template <> inline __device__ double exp(double val) { return ::exp(val); }

    template <class T> __device__ T expm1(T val);
#if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__ >= 530)
    template <> inline __device__ __half expm1(__half val) { return hexp(val) - __half(1); }
#endif
    template <> inline __device__ float expm1(float val) { return expm1f(val); }
    template <> inline __device__ double expm1(double val) { return ::expm1(val); }

    template <clas...",69,src\cuda\math.hpp,cv.dnn.cuda4dnn.csl.device,11,device,1
382227,NAMESPACE_BLOCK,<empty>,,src\cuda\memory.hpp,src\cuda\memory.hpp:<global>,,<global>,1
382231,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

template <class T>
__device__ T load_ldg(const T& src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(&src);
#else
    return src;
#endif
}

template <class T>
__device__ T load_ldg(const T* src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(src);
#else
    return *src;
#endif
}

}}}}}",1,src\cuda\memory.hpp,cv,10,cv,1
382232,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

template <class T>
__device__ T load_ldg(const T& src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(&src);
#else
    return src;
#endif
}

template <class T>
__device__ T load_ldg(const T* src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(src);
#else
    return *src;
#endif
}

}}}}",16,src\cuda\memory.hpp,cv.dnn,10,dnn,1
382233,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

template <class T>
__device__ T load_ldg(const T& src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(&src);
#else
    return src;
#endif
}

template <class T>
__device__ T load_ldg(const T* src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(src);
#else
    return *src;
#endif
}

}}}",32,src\cuda\memory.hpp,cv.dnn.cuda4dnn,10,cuda4dnn,1
382234,NAMESPACE_BLOCK,"namespace csl { namespace device {

template <class T>
__device__ T load_ldg(const T& src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(&src);
#else
    return src;
#endif
}

template <class T>
__device__ T load_ldg(const T* src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(src);
#else
    return *src;
#endif
}

}}",53,src\cuda\memory.hpp,cv.dnn.cuda4dnn.csl,10,csl,1
382235,NAMESPACE_BLOCK,"namespace device {

template <class T>
__device__ T load_ldg(const T& src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(&src);
#else
    return src;
#endif
}

template <class T>
__device__ T load_ldg(const T* src) {
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 350)
    return __ldg(src);
#else
    return *src;
#endif
}

}",69,src\cuda\memory.hpp,cv.dnn.cuda4dnn.csl.device,10,device,1
382241,NAMESPACE_BLOCK,<empty>,,src\cuda\types.hpp,src\cuda\types.hpp:<global>,,<global>,1
382245,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    /* For indices, we can use 32bit variables or 64bit variables. The GPU registers are 32 bits in size.
     * Hence, a 64bit variable requires two registers and is significantly slower than the 32bit versions.
     *
     * If we do not need to handle huge tensors, we can use 32-bit indices and get better performance.
     */
#ifdef __CUDACC__
    using size_type = int;
    using index_type = int;
#else
    using size_type = std::int32_t;
    using index_type = std::int32_t;
#endif

}}}}}",1,src\cuda\types.hpp,cv,10,cv,1
382246,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    /* For indices, we can use 32bit variables or 64bit variables. The GPU registers are 32 bits in size.
     * Hence, a 64bit variable requires two registers and is significantly slower than the 32bit versions.
     *
     * If we do not need to handle huge tensors, we can use 32-bit indices and get better performance.
     */
#ifdef __CUDACC__
    using size_type = int;
    using index_type = int;
#else
    using size_type = std::int32_t;
    using index_type = std::int32_t;
#endif

}}}}",16,src\cuda\types.hpp,cv.dnn,10,dnn,1
382247,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

    /* For indices, we can use 32bit variables or 64bit variables. The GPU registers are 32 bits in size.
     * Hence, a 64bit variable requires two registers and is significantly slower than the 32bit versions.
     *
     * If we do not need to handle huge tensors, we can use 32-bit indices and get better performance.
     */
#ifdef __CUDACC__
    using size_type = int;
    using index_type = int;
#else
    using size_type = std::int32_t;
    using index_type = std::int32_t;
#endif

}}}",32,src\cuda\types.hpp,cv.dnn.cuda4dnn,10,cuda4dnn,1
382248,NAMESPACE_BLOCK,"namespace csl { namespace device {

    /* For indices, we can use 32bit variables or 64bit variables. The GPU registers are 32 bits in size.
     * Hence, a 64bit variable requires two registers and is significantly slower than the 32bit versions.
     *
     * If we do not need to handle huge tensors, we can use 32-bit indices and get better performance.
     */
#ifdef __CUDACC__
    using size_type = int;
    using index_type = int;
#else
    using size_type = std::int32_t;
    using index_type = std::int32_t;
#endif

}}",53,src\cuda\types.hpp,cv.dnn.cuda4dnn.csl,10,csl,1
382249,NAMESPACE_BLOCK,"namespace device {

    /* For indices, we can use 32bit variables or 64bit variables. The GPU registers are 32 bits in size.
     * Hence, a 64bit variable requires two registers and is significantly slower than the 32bit versions.
     *
     * If we do not need to handle huge tensors, we can use 32-bit indices and get better performance.
     */
#ifdef __CUDACC__
    using size_type = int;
    using index_type = int;
#else
    using size_type = std::int32_t;
    using index_type = std::int32_t;
#endif

}",69,src\cuda\types.hpp,cv.dnn.cuda4dnn.csl.device,10,device,1
382263,NAMESPACE_BLOCK,<empty>,,src\cuda\vector_traits.hpp,src\cuda\vector_traits.hpp:<global>,,<global>,1
382267,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    /** \file vector_traits.hpp
     *  \brief utility classes and functions for vectorized memory loads/stores
     *
     * Example:
     * using vector_type = get_vector_type_t<float, 4>;
     *
     * auto input_vPtr = type::get_pointer(iptr); // iptr is of type DevicePtr<const float>
     * auto output_vPtr = type::get_pointer(optr);  // optr is of type DevicePtr<float>
     *
     * vector_type vec;
     * v_load(vec, input_vPtr);
     *
     * for(int i = 0; i < vector_type::size(); i++)
     *      vec[i] = do_something(vec[i]);
     *
     * v_store(output_vPtr, vec);
     */

    namespace detail {
        template <size_type N> struct raw_type_ { };
        template <> struct raw_type_<256> { typedef ulonglong4 type; };
        template <> struct raw_type_<128> { typedef uint4 type; };
        template <> struct raw_type_<64> { typedef uint2 type; };
        template <> struct raw_type...",1,src\cuda\vector_traits.hpp,cv,17,cv,1
382268,NAMESPACE_BLOCK,"namespace dnn { namespace cuda4dnn { namespace csl { namespace device {

    /** \file vector_traits.hpp
     *  \brief utility classes and functions for vectorized memory loads/stores
     *
     * Example:
     * using vector_type = get_vector_type_t<float, 4>;
     *
     * auto input_vPtr = type::get_pointer(iptr); // iptr is of type DevicePtr<const float>
     * auto output_vPtr = type::get_pointer(optr);  // optr is of type DevicePtr<float>
     *
     * vector_type vec;
     * v_load(vec, input_vPtr);
     *
     * for(int i = 0; i < vector_type::size(); i++)
     *      vec[i] = do_something(vec[i]);
     *
     * v_store(output_vPtr, vec);
     */

    namespace detail {
        template <size_type N> struct raw_type_ { };
        template <> struct raw_type_<256> { typedef ulonglong4 type; };
        template <> struct raw_type_<128> { typedef uint4 type; };
        template <> struct raw_type_<64> { typedef uint2 type; };
        template <> struct raw_type_<32> { typedef...",16,src\cuda\vector_traits.hpp,cv.dnn,17,dnn,1
382269,NAMESPACE_BLOCK,"namespace cuda4dnn { namespace csl { namespace device {

    /** \file vector_traits.hpp
     *  \brief utility classes and functions for vectorized memory loads/stores
     *
     * Example:
     * using vector_type = get_vector_type_t<float, 4>;
     *
     * auto input_vPtr = type::get_pointer(iptr); // iptr is of type DevicePtr<const float>
     * auto output_vPtr = type::get_pointer(optr);  // optr is of type DevicePtr<float>
     *
     * vector_type vec;
     * v_load(vec, input_vPtr);
     *
     * for(int i = 0; i < vector_type::size(); i++)
     *      vec[i] = do_something(vec[i]);
     *
     * v_store(output_vPtr, vec);
     */

    namespace detail {
        template <size_type N> struct raw_type_ { };
        template <> struct raw_type_<256> { typedef ulonglong4 type; };
        template <> struct raw_type_<128> { typedef uint4 type; };
        template <> struct raw_type_<64> { typedef uint2 type; };
        template <> struct raw_type_<32> { typedef uint1 type; };
...",32,src\cuda\vector_traits.hpp,cv.dnn.cuda4dnn,17,cuda4dnn,1
382270,NAMESPACE_BLOCK,"namespace csl { namespace device {

    /** \file vector_traits.hpp
     *  \brief utility classes and functions for vectorized memory loads/stores
     *
     * Example:
     * using vector_type = get_vector_type_t<float, 4>;
     *
     * auto input_vPtr = type::get_pointer(iptr); // iptr is of type DevicePtr<const float>
     * auto output_vPtr = type::get_pointer(optr);  // optr is of type DevicePtr<float>
     *
     * vector_type vec;
     * v_load(vec, input_vPtr);
     *
     * for(int i = 0; i < vector_type::size(); i++)
     *      vec[i] = do_something(vec[i]);
     *
     * v_store(output_vPtr, vec);
     */

    namespace detail {
        template <size_type N> struct raw_type_ { };
        template <> struct raw_type_<256> { typedef ulonglong4 type; };
        template <> struct raw_type_<128> { typedef uint4 type; };
        template <> struct raw_type_<64> { typedef uint2 type; };
        template <> struct raw_type_<32> { typedef uint1 type; };
        template <> s...",53,src\cuda\vector_traits.hpp,cv.dnn.cuda4dnn.csl,17,csl,1
382271,NAMESPACE_BLOCK,"namespace device {

    /** \file vector_traits.hpp
     *  \brief utility classes and functions for vectorized memory loads/stores
     *
     * Example:
     * using vector_type = get_vector_type_t<float, 4>;
     *
     * auto input_vPtr = type::get_pointer(iptr); // iptr is of type DevicePtr<const float>
     * auto output_vPtr = type::get_pointer(optr);  // optr is of type DevicePtr<float>
     *
     * vector_type vec;
     * v_load(vec, input_vPtr);
     *
     * for(int i = 0; i < vector_type::size(); i++)
     *      vec[i] = do_something(vec[i]);
     *
     * v_store(output_vPtr, vec);
     */

    namespace detail {
        template <size_type N> struct raw_type_ { };
        template <> struct raw_type_<256> { typedef ulonglong4 type; };
        template <> struct raw_type_<128> { typedef uint4 type; };
        template <> struct raw_type_<64> { typedef uint2 type; };
        template <> struct raw_type_<32> { typedef uint1 type; };
        template <> struct raw_type_<...",69,src\cuda\vector_traits.hpp,cv.dnn.cuda4dnn.csl.device,17,device,1
382272,NAMESPACE_BLOCK,"namespace detail {
        template <size_type N> struct raw_type_ { };
        template <> struct raw_type_<256> { typedef ulonglong4 type; };
        template <> struct raw_type_<128> { typedef uint4 type; };
        template <> struct raw_type_<64> { typedef uint2 type; };
        template <> struct raw_type_<32> { typedef uint1 type; };
        template <> struct raw_type_<16> { typedef uchar2 type; };
        template <> struct raw_type_<8> { typedef uchar1 type; };

        template <size_type N> struct raw_type {
            using type = typename raw_type_<N>::type;
            static_assert(sizeof(type) * 8 == N, """");
        };
    }",5,src\cuda\vector_traits.hpp,cv.dnn.cuda4dnn.csl.device.detail,37,detail,1
382350,NAMESPACE_BLOCK,<empty>,,src\darknet\darknet_importer.cpp,src\darknet\darknet_importer.cpp:<global>,,<global>,1
382354,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

namespace
{

class DarknetImporter
{
    FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;

    darknet::NetParameter net;

public:

    DarknetImporter() {}

    DarknetImporter(std::istream &cfgStream, std::istream &darknetModelStream)
    {
        CV_TRACE_FUNCTION();

        ReadNetParamsFromCfgStreamOrDie(cfgStream, &net);
        ReadNetParamsFromBinaryStreamOrDie(darknetModelStream, &net);
    }

    DarknetImporter(std::istream &cfgStream)
    {
        CV_TRACE_FUNCTION();

        ReadNetParamsFromCfgStreamOrDie(cfgStream, &net);
    }

    struct BlobNote
    {
        BlobNote(const std::string &_name, int _layerId, int _outNum) :
            name(_name), layerId(_layerId), outNum(_outNum) {}

        std::string name;
        int layerId, outNum;
    };

    std::vector<BlobNote> addedBlobs;
    std::map<String, int> layerCounter;

    void populateNet(Net dstNet)
    {
        CV_TRACE_FUNCTION();

        i...",1,src\darknet\darknet_importer.cpp,cv,56,cv,1
382355,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

namespace
{

class DarknetImporter
{
    FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;

    darknet::NetParameter net;

public:

    DarknetImporter() {}

    DarknetImporter(std::istream &cfgStream, std::istream &darknetModelStream)
    {
        CV_TRACE_FUNCTION();

        ReadNetParamsFromCfgStreamOrDie(cfgStream, &net);
        ReadNetParamsFromBinaryStreamOrDie(darknetModelStream, &net);
    }

    DarknetImporter(std::istream &cfgStream)
    {
        CV_TRACE_FUNCTION();

        ReadNetParamsFromCfgStreamOrDie(cfgStream, &net);
    }

    struct BlobNote
    {
        BlobNote(const std::string &_name, int _layerId, int _outNum) :
            name(_name), layerId(_layerId), outNum(_outNum) {}

        std::string name;
        int layerId, outNum;
    };

    std::vector<BlobNote> addedBlobs;
    std::map<String, int> layerCounter;

    void populateNet(Net dstNet)
    {
        CV_TRACE_FUNCTION();

        int layersSize =...",1,src\darknet\darknet_importer.cpp,cv.dnn,57,dnn,1
382357,NAMESPACE_BLOCK,"namespace
{

class DarknetImporter
{
    FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;

    darknet::NetParameter net;

public:

    DarknetImporter() {}

    DarknetImporter(std::istream &cfgStream, std::istream &darknetModelStream)
    {
        CV_TRACE_FUNCTION();

        ReadNetParamsFromCfgStreamOrDie(cfgStream, &net);
        ReadNetParamsFromBinaryStreamOrDie(darknetModelStream, &net);
    }

    DarknetImporter(std::istream &cfgStream)
    {
        CV_TRACE_FUNCTION();

        ReadNetParamsFromCfgStreamOrDie(cfgStream, &net);
    }

    struct BlobNote
    {
        BlobNote(const std::string &_name, int _layerId, int _outNum) :
            name(_name), layerId(_layerId), outNum(_outNum) {}

        std::string name;
        int layerId, outNum;
    };

    std::vector<BlobNote> addedBlobs;
    std::map<String, int> layerCounter;

    void populateNet(Net dstNet)
    {
        CV_TRACE_FUNCTION();

        int layersSize = net.layer_size();
        layerCounter.c...",1,src\darknet\darknet_importer.cpp,cv.dnn.anonymous_namespace_0,60,,2
382981,NAMESPACE_BLOCK,<empty>,,src\darknet\darknet_io.cpp,src\darknet\darknet_io.cpp:<global>,,<global>,1
382985,NAMESPACE_BLOCK,"namespace cv {
    namespace dnn {
        namespace darknet {

            template<typename T>
            T getParam(const std::map<std::string, std::string> &params, const std::string param_name, T init_val)
            {
                std::map<std::string, std::string>::const_iterator it = params.find(param_name);
                if (it != params.end()) {
                    std::stringstream ss(it->second);
                    ss >> init_val;
                }
                return init_val;
            }

            static const std::string kFirstLayerName = ""data"";

            class setLayersParams {

                NetParameter *net;
                int layer_id;
                std::string last_layer;
                std::vector<std::string> fused_layer_names;

            public:
                setLayersParams(NetParameter *_net) :
                    net(_net), layer_id(0), last_layer(kFirstLayerName)
                {}

                void setLayerBlobs(int i, s...",1,src\darknet\darknet_io.cpp,cv,78,cv,1
382986,NAMESPACE_BLOCK,"namespace dnn {
        namespace darknet {

            template<typename T>
            T getParam(const std::map<std::string, std::string> &params, const std::string param_name, T init_val)
            {
                std::map<std::string, std::string>::const_iterator it = params.find(param_name);
                if (it != params.end()) {
                    std::stringstream ss(it->second);
                    ss >> init_val;
                }
                return init_val;
            }

            static const std::string kFirstLayerName = ""data"";

            class setLayersParams {

                NetParameter *net;
                int layer_id;
                std::string last_layer;
                std::vector<std::string> fused_layer_names;

            public:
                setLayersParams(NetParameter *_net) :
                    net(_net), layer_id(0), last_layer(kFirstLayerName)
                {}

                void setLayerBlobs(int i, std::vector<cv::Mat>...",5,src\darknet\darknet_io.cpp,cv.dnn,79,dnn,1
382987,NAMESPACE_BLOCK,"namespace darknet {

            template<typename T>
            T getParam(const std::map<std::string, std::string> &params, const std::string param_name, T init_val)
            {
                std::map<std::string, std::string>::const_iterator it = params.find(param_name);
                if (it != params.end()) {
                    std::stringstream ss(it->second);
                    ss >> init_val;
                }
                return init_val;
            }

            static const std::string kFirstLayerName = ""data"";

            class setLayersParams {

                NetParameter *net;
                int layer_id;
                std::string last_layer;
                std::vector<std::string> fused_layer_names;

            public:
                setLayersParams(NetParameter *_net) :
                    net(_net), layer_id(0), last_layer(kFirstLayerName)
                {}

                void setLayerBlobs(int i, std::vector<cv::Mat> blobs)
                ...",9,src\darknet\darknet_io.cpp,cv.dnn.darknet,80,darknet,1
387420,NAMESPACE_BLOCK,<empty>,,src\darknet\darknet_io.hpp,src\darknet\darknet_io.hpp:<global>,,<global>,1
387424,NAMESPACE_BLOCK,"namespace cv {
    namespace dnn {
        namespace darknet {

            class LayerParameter {
                std::string layer_name, layer_type;
                std::vector<std::string> bottom_indexes;
                cv::dnn::LayerParams layerParams;
            public:
                friend class setLayersParams;
                cv::dnn::LayerParams getLayerParams() const { return layerParams; }
                std::string name() const { return layer_name; }
                std::string type() const { return layer_type; }
                int bottom_size() const { return bottom_indexes.size(); }
                std::string bottom(const int index) const { return bottom_indexes.at(index); }
                int top_size() const { return 1; }
                std::string top(const int index) const { return layer_name; }
            };

            class NetParameter {
            public:
                int width, height, channels;
                std::vector<LayerParameter> layer...",1,src\darknet\darknet_io.hpp,cv,74,cv,1
387425,NAMESPACE_BLOCK,"namespace dnn {
        namespace darknet {

            class LayerParameter {
                std::string layer_name, layer_type;
                std::vector<std::string> bottom_indexes;
                cv::dnn::LayerParams layerParams;
            public:
                friend class setLayersParams;
                cv::dnn::LayerParams getLayerParams() const { return layerParams; }
                std::string name() const { return layer_name; }
                std::string type() const { return layer_type; }
                int bottom_size() const { return bottom_indexes.size(); }
                std::string bottom(const int index) const { return bottom_indexes.at(index); }
                int top_size() const { return 1; }
                std::string top(const int index) const { return layer_name; }
            };

            class NetParameter {
            public:
                int width, height, channels;
                std::vector<LayerParameter> layers;
                ...",5,src\darknet\darknet_io.hpp,cv.dnn,75,dnn,1
387426,NAMESPACE_BLOCK,"namespace darknet {

            class LayerParameter {
                std::string layer_name, layer_type;
                std::vector<std::string> bottom_indexes;
                cv::dnn::LayerParams layerParams;
            public:
                friend class setLayersParams;
                cv::dnn::LayerParams getLayerParams() const { return layerParams; }
                std::string name() const { return layer_name; }
                std::string type() const { return layer_type; }
                int bottom_size() const { return bottom_indexes.size(); }
                std::string bottom(const int index) const { return bottom_indexes.at(index); }
                int top_size() const { return 1; }
                std::string top(const int index) const { return layer_name; }
            };

            class NetParameter {
            public:
                int width, height, channels;
                std::vector<LayerParameter> layers;
                std::vector<int> out_cha...",9,src\darknet\darknet_io.hpp,cv.dnn.darknet,76,darknet,1
387552,NAMESPACE_BLOCK,<empty>,,src\debug_utils.cpp,src\debug_utils.cpp:<global>,,<global>,1
387556,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN

bool DNN_DIAGNOSTICS_RUN = false;
bool DNN_SKIP_REAL_IMPORT = false;

void enableModelDiagnostics(bool isDiagnosticsMode)
{
    DNN_DIAGNOSTICS_RUN = isDiagnosticsMode;

    if (DNN_DIAGNOSTICS_RUN)
    {
        detail::NotImplemented::Register();
    }
    else
    {
        detail::NotImplemented::unRegister();
    }
}

void skipModelImport(bool skip)
{
    DNN_SKIP_REAL_IMPORT = skip;
}

void detail::LayerHandler::addMissing(const std::string& name, const std::string& type)
{
    // If we didn't add it, but can create it, it's custom and not missing.
    if (!contains(type) && LayerFactory::isLayerRegistered(type))
    {
        return;
    }

    layers[type].insert(name);
}

bool detail::LayerHandler::contains(const std::string& type) const
{
    return layers.count(type) != 0;
}

void detail::LayerHandler::printMissing() const
{
    if (layers.empty())
    {
        return;
    }

    std::ostringstream ss;
    ss << ""DN...",1,src\debug_utils.cpp,cv,13,cv,1
387557,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

bool DNN_DIAGNOSTICS_RUN = false;
bool DNN_SKIP_REAL_IMPORT = false;

void enableModelDiagnostics(bool isDiagnosticsMode)
{
    DNN_DIAGNOSTICS_RUN = isDiagnosticsMode;

    if (DNN_DIAGNOSTICS_RUN)
    {
        detail::NotImplemented::Register();
    }
    else
    {
        detail::NotImplemented::unRegister();
    }
}

void skipModelImport(bool skip)
{
    DNN_SKIP_REAL_IMPORT = skip;
}

void detail::LayerHandler::addMissing(const std::string& name, const std::string& type)
{
    // If we didn't add it, but can create it, it's custom and not missing.
    if (!contains(type) && LayerFactory::isLayerRegistered(type))
    {
        return;
    }

    layers[type].insert(name);
}

bool detail::LayerHandler::contains(const std::string& type) const
{
    return layers.count(type) != 0;
}

void detail::LayerHandler::printMissing() const
{
    if (layers.empty())
    {
        return;
    }

    std::ostringstream ss;
    ss << ""DNN: Not supporte...",16,src\debug_utils.cpp,cv.dnn,13,dnn,1
387732,NAMESPACE_BLOCK,<empty>,,src\dnn.cpp,src\dnn.cpp:<global>,,<global>,1
387743,NAMESPACE_BLOCK,<empty>,,src\dnn_common.hpp,src\dnn_common.hpp:<global>,,<global>,1
387747,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN
#define IS_DNN_OPENCL_TARGET(id) (id == DNN_TARGET_OPENCL || id == DNN_TARGET_OPENCL_FP16)
#define IS_DNN_CPU_TARGET(id) (id == DNN_TARGET_CPU || id == DNN_TARGET_CPU_FP16)
#define IS_DNN_VULKAN_TARGET(id) (id == DNN_TARGET_VULKAN)
Mutex& getInitializationMutex();
void initializeLayerFactory();

extern bool DNN_DIAGNOSTICS_RUN;
extern bool DNN_SKIP_REAL_IMPORT;

//
// dnn_params.cpp
//

/// Network dump level
size_t getParam_DNN_NETWORK_DUMP();

/// This parameter is useful to run with valgrind memory errors detection
bool getParam_DNN_DISABLE_MEMORY_OPTIMIZATIONS();

#ifdef HAVE_OPENCL
bool getParam_DNN_OPENCL_ALLOW_ALL_DEVICES();
#endif

int getParam_DNN_BACKEND_DEFAULT();

// Additional checks (slowdowns execution!)
bool getParam_DNN_CHECK_NAN_INF();
bool getParam_DNN_CHECK_NAN_INF_DUMP();
bool getParam_DNN_CHECK_NAN_INF_RAISE_ERROR();


inline namespace detail {

typedef std::vector<MatShape> ShapesVec;

struct LayerShapes
{...",1,src\dnn_common.hpp,cv,13,cv,1
387748,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN
#define IS_DNN_OPENCL_TARGET(id) (id == DNN_TARGET_OPENCL || id == DNN_TARGET_OPENCL_FP16)
#define IS_DNN_CPU_TARGET(id) (id == DNN_TARGET_CPU || id == DNN_TARGET_CPU_FP16)
#define IS_DNN_VULKAN_TARGET(id) (id == DNN_TARGET_VULKAN)
Mutex& getInitializationMutex();
void initializeLayerFactory();

extern bool DNN_DIAGNOSTICS_RUN;
extern bool DNN_SKIP_REAL_IMPORT;

//
// dnn_params.cpp
//

/// Network dump level
size_t getParam_DNN_NETWORK_DUMP();

/// This parameter is useful to run with valgrind memory errors detection
bool getParam_DNN_DISABLE_MEMORY_OPTIMIZATIONS();

#ifdef HAVE_OPENCL
bool getParam_DNN_OPENCL_ALLOW_ALL_DEVICES();
#endif

int getParam_DNN_BACKEND_DEFAULT();

// Additional checks (slowdowns execution!)
bool getParam_DNN_CHECK_NAN_INF();
bool getParam_DNN_CHECK_NAN_INF_DUMP();
bool getParam_DNN_CHECK_NAN_INF_RAISE_ERROR();


inline namespace detail {

typedef std::vector<MatShape> ShapesVec;

struct LayerShapes
{
    ShapesVec ...",16,src\dnn_common.hpp,cv.dnn,13,dnn,1
387784,NAMESPACE_BLOCK,"inline namespace detail {

typedef std::vector<MatShape> ShapesVec;

struct LayerShapes
{
    ShapesVec in, out, internal;
    // No guarantees that layer which support in-place computations
    // will be computed in-place (input.data_ptr == output.data_ptr).
    // If layer said that it could work in-place and layers after it
    // no longer use input blob, we'll set output = input.
    bool supportInPlace;
    LayerShapes() {supportInPlace = false;}
};


#define CALL_MEMBER_FN(object, ptrToMemFn)  ((object).*(ptrToMemFn))

class NotImplemented : public Layer
{
public:
    static Ptr<Layer> create(const LayerParams &params);

    static void Register();
    static void unRegister();
};

template <typename Importer, typename ... Args>
Net readNet(Args&& ... args)
{
    Net net;
    Importer importer(net, std::forward<Args>(args)...);
    return net;
}

template <typename Importer, typename ... Args>
Net readNetDiagnostic(Args&& ... args)
{
    Net maybeDebugNet = readNet<Importer>...",1,src\dnn_common.hpp,cv.dnn.detail,46,detail,12
388077,NAMESPACE_BLOCK,"namespace accessor {
class DnnNetAccessor
{
public:
    static inline Ptr<Net::Impl>& getImplPtrRef(Net& net)
    {
        return net.impl;
    }
};
}",1,src\dnn_common.hpp,cv.dnn.accessor,176,accessor,17
388095,NAMESPACE_BLOCK,<empty>,,src\dnn_params.cpp,src\dnn_params.cpp:<global>,,<global>,1
388099,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


size_t getParam_DNN_NETWORK_DUMP()
{
    static size_t DNN_NETWORK_DUMP = utils::getConfigurationParameterSizeT(""OPENCV_DNN_NETWORK_DUMP"", 0);
    return DNN_NETWORK_DUMP;
}

// this option is useful to run with valgrind memory errors detection
bool getParam_DNN_DISABLE_MEMORY_OPTIMIZATIONS()
{
    static bool DNN_DISABLE_MEMORY_OPTIMIZATIONS = utils::getConfigurationParameterBool(""OPENCV_DNN_DISABLE_MEMORY_OPTIMIZATIONS"", false);
    return DNN_DISABLE_MEMORY_OPTIMIZATIONS;
}

#ifdef HAVE_OPENCL
bool getParam_DNN_OPENCL_ALLOW_ALL_DEVICES()
{
    static bool DNN_OPENCL_ALLOW_ALL_DEVICES = utils::getConfigurationParameterBool(""OPENCV_DNN_OPENCL_ALLOW_ALL_DEVICES"", false);
    return DNN_OPENCL_ALLOW_ALL_DEVICES;
}
#endif

int getParam_DNN_BACKEND_DEFAULT()
{
    static int PARAM_DNN_BACKEND_DEFAULT = (int)utils::getConfigurationParameterSizeT(""OPENCV_DNN_BACKEND_DEFAULT"",
#ifdef OPENCV_DNN_BACKEND_DEFAULT
            (size_t)OP...",1,src\dnn_params.cpp,cv,10,cv,1
388100,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


size_t getParam_DNN_NETWORK_DUMP()
{
    static size_t DNN_NETWORK_DUMP = utils::getConfigurationParameterSizeT(""OPENCV_DNN_NETWORK_DUMP"", 0);
    return DNN_NETWORK_DUMP;
}

// this option is useful to run with valgrind memory errors detection
bool getParam_DNN_DISABLE_MEMORY_OPTIMIZATIONS()
{
    static bool DNN_DISABLE_MEMORY_OPTIMIZATIONS = utils::getConfigurationParameterBool(""OPENCV_DNN_DISABLE_MEMORY_OPTIMIZATIONS"", false);
    return DNN_DISABLE_MEMORY_OPTIMIZATIONS;
}

#ifdef HAVE_OPENCL
bool getParam_DNN_OPENCL_ALLOW_ALL_DEVICES()
{
    static bool DNN_OPENCL_ALLOW_ALL_DEVICES = utils::getConfigurationParameterBool(""OPENCV_DNN_OPENCL_ALLOW_ALL_DEVICES"", false);
    return DNN_OPENCL_ALLOW_ALL_DEVICES;
}
#endif

int getParam_DNN_BACKEND_DEFAULT()
{
    static int PARAM_DNN_BACKEND_DEFAULT = (int)utils::getConfigurationParameterSizeT(""OPENCV_DNN_BACKEND_DEFAULT"",
#ifdef OPENCV_DNN_BACKEND_DEFAULT
            (size_t)OPENCV_DNN_BACKEN...",1,src\dnn_params.cpp,cv.dnn,11,dnn,1
388200,NAMESPACE_BLOCK,<empty>,,src\dnn_read.cpp,src\dnn_read.cpp:<global>,,<global>,1
388204,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


Net readNet(const String& _model, const String& _config, const String& _framework)
{
    String framework = toLowerCase(_framework);
    String model = _model;
    String config = _config;
    const std::string modelExt = model.substr(model.rfind('.') + 1);
    const std::string configExt = config.substr(config.rfind('.') + 1);
    if (framework == ""caffe"" || modelExt == ""caffemodel"" || configExt == ""caffemodel"" || modelExt == ""prototxt"" || configExt == ""prototxt"")
    {
        if (modelExt == ""prototxt"" || configExt == ""caffemodel"")
            std::swap(model, config);
        return readNetFromCaffe(config, model);
    }
    if (framework == ""tensorflow"" || modelExt == ""pb"" || configExt == ""pb"" || modelExt == ""pbtxt"" || configExt == ""pbtxt"")
    {
        if (modelExt == ""pbtxt"" || configExt == ""pb"")
            std::swap(model, config);
        return readNetFromTensorflow(model, config);
    }
    if (framework == ""tflit...",1,src\dnn_read.cpp,cv,8,cv,1
388205,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


Net readNet(const String& _model, const String& _config, const String& _framework)
{
    String framework = toLowerCase(_framework);
    String model = _model;
    String config = _config;
    const std::string modelExt = model.substr(model.rfind('.') + 1);
    const std::string configExt = config.substr(config.rfind('.') + 1);
    if (framework == ""caffe"" || modelExt == ""caffemodel"" || configExt == ""caffemodel"" || modelExt == ""prototxt"" || configExt == ""prototxt"")
    {
        if (modelExt == ""prototxt"" || configExt == ""caffemodel"")
            std::swap(model, config);
        return readNetFromCaffe(config, model);
    }
    if (framework == ""tensorflow"" || modelExt == ""pb"" || configExt == ""pb"" || modelExt == ""pbtxt"" || configExt == ""pbtxt"")
    {
        if (modelExt == ""pbtxt"" || configExt == ""pb"")
            std::swap(model, config);
        return readNetFromTensorflow(model, config);
    }
    if (framework == ""tflite"" || modelExt ...",1,src\dnn_read.cpp,cv.dnn,9,dnn,1
388642,NAMESPACE_BLOCK,<empty>,,src\dnn_utils.cpp,src\dnn_utils.cpp:<global>,,<global>,1
388646,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

Image2BlobParams::Image2BlobParams():scalefactor(Scalar::all(1.0)), size(Size()), mean(Scalar()), swapRB(false), ddepth(CV_32F),
                           datalayout(DNN_LAYOUT_NCHW), paddingmode(DNN_PMODE_NULL)
{}

Image2BlobParams::Image2BlobParams(const Scalar& scalefactor_, const Size& size_, const Scalar& mean_, bool swapRB_,
    int ddepth_, DataLayout datalayout_, ImagePaddingMode mode_, Scalar borderValue_):
    scalefactor(scalefactor_), size(size_), mean(mean_), swapRB(swapRB_), ddepth(ddepth_),
    datalayout(datalayout_), paddingmode(mode_), borderValue(borderValue_)
{}

void getVector(InputArrayOfArrays images_, std::vector<Mat>& images) {
    images_.getMatVector(images);
}

void getVector(InputArrayOfArrays images_, std::vector<UMat>& images) {
    images_.getUMatVector(images);
}

void getMat(UMat& blob, InputArray blob_, AccessFlag flag) {
    if(blob_.kind() == _InputArray::UMAT)
        blob = blob_.getUMat(...",1,src\dnn_utils.cpp,cv,11,cv,1
388647,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

Image2BlobParams::Image2BlobParams():scalefactor(Scalar::all(1.0)), size(Size()), mean(Scalar()), swapRB(false), ddepth(CV_32F),
                           datalayout(DNN_LAYOUT_NCHW), paddingmode(DNN_PMODE_NULL)
{}

Image2BlobParams::Image2BlobParams(const Scalar& scalefactor_, const Size& size_, const Scalar& mean_, bool swapRB_,
    int ddepth_, DataLayout datalayout_, ImagePaddingMode mode_, Scalar borderValue_):
    scalefactor(scalefactor_), size(size_), mean(mean_), swapRB(swapRB_), ddepth(ddepth_),
    datalayout(datalayout_), paddingmode(mode_), borderValue(borderValue_)
{}

void getVector(InputArrayOfArrays images_, std::vector<Mat>& images) {
    images_.getMatVector(images);
}

void getVector(InputArrayOfArrays images_, std::vector<UMat>& images) {
    images_.getUMatVector(images);
}

void getMat(UMat& blob, InputArray blob_, AccessFlag flag) {
    if(blob_.kind() == _InputArray::UMAT)
        blob = blob_.getUMat();
    else if(...",1,src\dnn_utils.cpp,cv.dnn,12,dnn,1
390834,NAMESPACE_BLOCK,<empty>,,src\factory.hpp,src\factory.hpp:<global>,,<global>,1
390838,NAMESPACE_BLOCK,"namespace cv { namespace dnn_backend {

class IDNNBackendFactory
{
public:
    virtual ~IDNNBackendFactory() {}
    virtual std::shared_ptr<cv::dnn_backend::NetworkBackend> createNetworkBackend() const = 0;
};

//
// PluginDNNBackendFactory is implemented in plugin_wrapper
//

std::shared_ptr<IDNNBackendFactory> createPluginDNNBackendFactory(const std::string& baseName);

/// @brief Returns createPluginDNNBackendFactory()->createNetworkBackend()
cv::dnn_backend::NetworkBackend& createPluginDNNNetworkBackend(const std::string& baseName);

}}",1,src\factory.hpp,cv,10,cv,1
390839,NAMESPACE_BLOCK,"namespace dnn_backend {

class IDNNBackendFactory
{
public:
    virtual ~IDNNBackendFactory() {}
    virtual std::shared_ptr<cv::dnn_backend::NetworkBackend> createNetworkBackend() const = 0;
};

//
// PluginDNNBackendFactory is implemented in plugin_wrapper
//

std::shared_ptr<IDNNBackendFactory> createPluginDNNBackendFactory(const std::string& baseName);

/// @brief Returns createPluginDNNBackendFactory()->createNetworkBackend()
cv::dnn_backend::NetworkBackend& createPluginDNNNetworkBackend(const std::string& baseName);

}",16,src\factory.hpp,cv.dnn_backend,10,dnn_backend,1
390878,NAMESPACE_BLOCK,<empty>,,src\graph_simplifier.cpp,src\graph_simplifier.cpp:<global>,,<global>,1
390881,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

Subgraph::~Subgraph() {}

int Subgraph::addNodeToMatch(const std::string& op, int input_0, int input_1,
                             int input_2, int input_3)
{
    int nodeInputs[] = {input_0, input_1, input_2, input_3};
    int numInputs = 0;
    for (int i = 0; i < 4; ++i)
    {
        numInputs += (int)(nodeInputs[i] != -1);
    }
    return addNodeToMatch(op, std::vector<int>(&nodeInputs[0], &nodeInputs[0] + numInputs));
}

int Subgraph::addNodeToMatch(const std::string& op, const std::vector<int>& inputs_)
{
    for (int i = 0; i < inputs_.size(); ++i)
    {
        CV_Assert(inputs_[i] < (int)nodes.size());
    }
    nodes.push_back(op);
    inputs.push_back(inputs_);
    return nodes.size() - 1;
}

void Subgraph::setFusedNode(const std::string& op, int input_0, int input_1,
                            int input_2, int input_3, int input_4, int input_5)
{
    int nodeInputs[] = {input_0, input_1, input_2, input_3, input_4, input_5};
    int nu...",1,src\graph_simplifier.cpp,cv,14,cv,1
390882,NAMESPACE_BLOCK,"namespace dnn {

Subgraph::~Subgraph() {}

int Subgraph::addNodeToMatch(const std::string& op, int input_0, int input_1,
                             int input_2, int input_3)
{
    int nodeInputs[] = {input_0, input_1, input_2, input_3};
    int numInputs = 0;
    for (int i = 0; i < 4; ++i)
    {
        numInputs += (int)(nodeInputs[i] != -1);
    }
    return addNodeToMatch(op, std::vector<int>(&nodeInputs[0], &nodeInputs[0] + numInputs));
}

int Subgraph::addNodeToMatch(const std::string& op, const std::vector<int>& inputs_)
{
    for (int i = 0; i < inputs_.size(); ++i)
    {
        CV_Assert(inputs_[i] < (int)nodes.size());
    }
    nodes.push_back(op);
    inputs.push_back(inputs_);
    return nodes.size() - 1;
}

void Subgraph::setFusedNode(const std::string& op, int input_0, int input_1,
                            int input_2, int input_3, int input_4, int input_5)
{
    int nodeInputs[] = {input_0, input_1, input_2, input_3, input_4, input_5};
    int numInputs = 0;
  ...",16,src\graph_simplifier.cpp,cv.dnn,14,dnn,1
392039,NAMESPACE_BLOCK,<empty>,,src\graph_simplifier.hpp,src\graph_simplifier.hpp:<global>,,<global>,1
392043,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ImportNodeWrapper
{
public:
    virtual ~ImportNodeWrapper() {}

    virtual int getNumInputs() const = 0;

    virtual std::string getInputName(int idx) const = 0;

    virtual std::string getType() const = 0;

    virtual void setType(const std::string& type) = 0;

    virtual void setInputNames(const std::vector<std::string>& inputs) = 0;
};

class ImportGraphWrapper
{
public:
    virtual ~ImportGraphWrapper() {}

    virtual Ptr<ImportNodeWrapper> getNode(int idx) const = 0;

    virtual int getNumNodes() const = 0;

    virtual int getNumOutputs(int nodeId) const = 0;

    virtual std::string getOutputName(int nodeId, int outId) const = 0;

    virtual void removeNode(int idx) = 0;

    virtual bool isCommutativeOp(const std::string& type) const = 0;
};

class Subgraph  // Interface to match and replace subgraphs.
{
public:
    virtual ~Subgraph();

    // Add a node to be matched in the origin graph. Specify ids of nodes that
    // are ex...",1,src\graph_simplifier.hpp,cv,15,cv,1
392044,NAMESPACE_BLOCK,"namespace dnn {

class ImportNodeWrapper
{
public:
    virtual ~ImportNodeWrapper() {}

    virtual int getNumInputs() const = 0;

    virtual std::string getInputName(int idx) const = 0;

    virtual std::string getType() const = 0;

    virtual void setType(const std::string& type) = 0;

    virtual void setInputNames(const std::vector<std::string>& inputs) = 0;
};

class ImportGraphWrapper
{
public:
    virtual ~ImportGraphWrapper() {}

    virtual Ptr<ImportNodeWrapper> getNode(int idx) const = 0;

    virtual int getNumNodes() const = 0;

    virtual int getNumOutputs(int nodeId) const = 0;

    virtual std::string getOutputName(int nodeId, int outId) const = 0;

    virtual void removeNode(int idx) = 0;

    virtual bool isCommutativeOp(const std::string& type) const = 0;
};

class Subgraph  // Interface to match and replace subgraphs.
{
public:
    virtual ~Subgraph();

    // Add a node to be matched in the origin graph. Specify ids of nodes that
    // are expected to be in...",16,src\graph_simplifier.hpp,cv.dnn,15,dnn,1
392189,NAMESPACE_BLOCK,<empty>,,src\halide_scheduler.cpp,src\halide_scheduler.cpp:<global>,,<global>,1
392193,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

#ifdef HAVE_HALIDE
static void applySplit(const FileNode& directive, Halide::Func& func,
                       const FileNode& params)
{
    for (const auto& varNode : directive)
    {
        const std::string varName = varNode.name();
        const std::string factorName = (std::string)varNode;
        Halide::Var var(varName);
        Halide::Var outerVar(varName + ""o"");
        Halide::Var innerVar(varName + ""i"");
        // If split factor is integer or parameters map has parameter value.
        CV_Assert(varNode.isString() && !params[factorName].empty() ||
                  varNode.isInt());
        int factor = (int)(varNode.isInt() ? varNode : params[factorName]);
        func.split(var, outerVar, innerVar, factor);
    }
}

static void applyReorder(const FileNode& directive, Halide::Func& func)
{
    std::string varName;
    const int numVars = directive.size();
    std::vector<Halide::VarOrRVar> reorderedVars;
    reorderedVars.reserve(num...",1,src\halide_scheduler.cpp,cv,12,cv,1
392194,NAMESPACE_BLOCK,"namespace dnn
{

#ifdef HAVE_HALIDE
static void applySplit(const FileNode& directive, Halide::Func& func,
                       const FileNode& params)
{
    for (const auto& varNode : directive)
    {
        const std::string varName = varNode.name();
        const std::string factorName = (std::string)varNode;
        Halide::Var var(varName);
        Halide::Var outerVar(varName + ""o"");
        Halide::Var innerVar(varName + ""i"");
        // If split factor is integer or parameters map has parameter value.
        CV_Assert(varNode.isString() && !params[factorName].empty() ||
                  varNode.isInt());
        int factor = (int)(varNode.isInt() ? varNode : params[factorName]);
        func.split(var, outerVar, innerVar, factor);
    }
}

static void applyReorder(const FileNode& directive, Halide::Func& func)
{
    std::string varName;
    const int numVars = directive.size();
    std::vector<Halide::VarOrRVar> reorderedVars;
    reorderedVars.reserve(numVars);
    for ...",1,src\halide_scheduler.cpp,cv.dnn,14,dnn,1
392238,NAMESPACE_BLOCK,<empty>,,src\halide_scheduler.hpp,src\halide_scheduler.hpp:<global>,,<global>,1
392242,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class HalideScheduler
{
public:
    HalideScheduler(const std::string& configFile);

    ~HalideScheduler();

    // Returns true if pipeline found in scheduling file.
    // If more than one function, returns true if the top function scheduled.
    // Other functions are optional to scheduling.
    bool process(Ptr<BackendNode>& node);

private:
    FileStorage fs;
};

}  // namespace dnn
}",1,src\halide_scheduler.hpp,cv,13,cv,1
392243,NAMESPACE_BLOCK,"namespace dnn
{

class HalideScheduler
{
public:
    HalideScheduler(const std::string& configFile);

    ~HalideScheduler();

    // Returns true if pipeline found in scheduling file.
    // If more than one function, returns true if the top function scheduled.
    // Other functions are optional to scheduling.
    bool process(Ptr<BackendNode>& node);

private:
    FileStorage fs;
};

}",1,src\halide_scheduler.hpp,cv.dnn,15,dnn,1
392279,NAMESPACE_BLOCK,<empty>,,src\ie_ngraph.cpp,src\ie_ngraph.cpp:<global>,,<global>,1
392283,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

#ifdef HAVE_DNN_NGRAPH

static bool DNN_IE_SERIALIZE = utils::getConfigurationParameterBool(""OPENCV_DNN_IE_SERIALIZE"", false);

// For networks with input layer which has an empty name, IE generates a name id[some_number].
// OpenCV lets users use an empty input name and to prevent unexpected naming,
// we can use some predefined name.
static std::string kDefaultInpLayerName = ""opencv_ngraph_empty_inp_layer_name"";
static constexpr const char* kOpenCVLayersType = ""opencv_ngraph_layer"";

#if INF_ENGINE_VER_MAJOR_LT(INF_ENGINE_RELEASE_2022_1)
static std::string shapesToStr(const std::vector<Mat>& mats)
{
    std::ostringstream shapes;
    shapes << mats.size() << "" "";
    for (const Mat& m : mats)
    {
        shapes << m.dims << "" "";
        for (int i = 0; i < m.dims; ++i)
            shapes << m.size[i] << "" "";
    }
    return shapes.str();
}

static void strToShapes(const std::string& str, std::vector<std::vector<size_t> >& shapes)
{
    std::istri...",1,src\ie_ngraph.cpp,cv,26,cv,1
392284,NAMESPACE_BLOCK,"namespace dnn {

#ifdef HAVE_DNN_NGRAPH

static bool DNN_IE_SERIALIZE = utils::getConfigurationParameterBool(""OPENCV_DNN_IE_SERIALIZE"", false);

// For networks with input layer which has an empty name, IE generates a name id[some_number].
// OpenCV lets users use an empty input name and to prevent unexpected naming,
// we can use some predefined name.
static std::string kDefaultInpLayerName = ""opencv_ngraph_empty_inp_layer_name"";
static constexpr const char* kOpenCVLayersType = ""opencv_ngraph_layer"";

#if INF_ENGINE_VER_MAJOR_LT(INF_ENGINE_RELEASE_2022_1)
static std::string shapesToStr(const std::vector<Mat>& mats)
{
    std::ostringstream shapes;
    shapes << mats.size() << "" "";
    for (const Mat& m : mats)
    {
        shapes << m.dims << "" "";
        for (int i = 0; i < m.dims; ++i)
            shapes << m.size[i] << "" "";
    }
    return shapes.str();
}

static void strToShapes(const std::string& str, std::vector<std::vector<size_t> >& shapes)
{
    std::istringstream ss(str...",16,src\ie_ngraph.cpp,cv.dnn,26,dnn,1
392290,NAMESPACE_BLOCK,<empty>,,src\ie_ngraph.hpp,src\ie_ngraph.hpp:<global>,,<global>,1
392294,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

#ifdef HAVE_DNN_NGRAPH

class InfEngineNgraphNode;


class InfEngineNgraphNet
{
public:
    InfEngineNgraphNet(detail::NetImplBase& netImpl);
    InfEngineNgraphNet(detail::NetImplBase& netImpl, InferenceEngine::CNNNetwork& net);

    void addOutput(const Ptr<InfEngineNgraphNode>& node);

    bool isInitialized();
    void init(Target targetId);

    void forward(const std::vector<Ptr<BackendWrapper> >& outBlobsWrappers, bool isAsync);

    void initPlugin(InferenceEngine::CNNNetwork& net);
    ngraph::ParameterVector setInputs(const std::vector<cv::Mat>& inputs, const std::vector<std::string>& names);

    void addBlobs(const std::vector<cv::Ptr<BackendWrapper> >& ptrs);

    void createNet(Target targetId);

    void reset();

//private:
    detail::NetImplBase& netImpl_;

    ngraph::ParameterVector inputs_vec;
    std::shared_ptr<ngraph::Function> ngraph_function;

    InferenceEngine::ExecutableNetwork netExec;
#if INF_ENGINE_VER_MAJOR_GE(INF_ENG...",1,src\ie_ngraph.hpp,cv,27,cv,1
392295,NAMESPACE_BLOCK,"namespace dnn {

#ifdef HAVE_DNN_NGRAPH

class InfEngineNgraphNode;


class InfEngineNgraphNet
{
public:
    InfEngineNgraphNet(detail::NetImplBase& netImpl);
    InfEngineNgraphNet(detail::NetImplBase& netImpl, InferenceEngine::CNNNetwork& net);

    void addOutput(const Ptr<InfEngineNgraphNode>& node);

    bool isInitialized();
    void init(Target targetId);

    void forward(const std::vector<Ptr<BackendWrapper> >& outBlobsWrappers, bool isAsync);

    void initPlugin(InferenceEngine::CNNNetwork& net);
    ngraph::ParameterVector setInputs(const std::vector<cv::Mat>& inputs, const std::vector<std::string>& names);

    void addBlobs(const std::vector<cv::Ptr<BackendWrapper> >& ptrs);

    void createNet(Target targetId);

    void reset();

//private:
    detail::NetImplBase& netImpl_;

    ngraph::ParameterVector inputs_vec;
    std::shared_ptr<ngraph::Function> ngraph_function;

    InferenceEngine::ExecutableNetwork netExec;
#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_202...",16,src\ie_ngraph.hpp,cv.dnn,27,dnn,1
392303,NAMESPACE_BLOCK,<empty>,,src\init.cpp,src\init.cpp:<global>,,<global>,1
392307,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

static Mutex* __initialization_mutex = NULL;
Mutex& getInitializationMutex()
{
    if (__initialization_mutex == NULL)
        __initialization_mutex = new Mutex();
    return *__initialization_mutex;
}
// force initialization (single-threaded environment)
Mutex* __initialization_mutex_initializer = &getInitializationMutex();

#if defined(HAVE_PROTOBUF) && !defined(BUILD_PLUGIN)
namespace {
using namespace google::protobuf;
class ProtobufShutdown {
public:
    bool initialized;
    ProtobufShutdown() : initialized(true) {}
    ~ProtobufShutdown()
    {
        initialized = false;
        google::protobuf::ShutdownProtobufLibrary();
    }
};
} // namespace
#endif

void initializeLayerFactory()
{
    CV_TRACE_FUNCTION();

#if defined(HAVE_PROTOBUF) && !defined(BUILD_PLUGIN)
    static ProtobufShutdown protobufShutdown; CV_UNUSED(protobufShutdown);
#endif

    CV_DNN_REGISTER_LAYER_CLASS(Slice,          SliceLayer);
    CV_DNN_RE...",1,src\init.cpp,cv,49,cv,1
392308,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

static Mutex* __initialization_mutex = NULL;
Mutex& getInitializationMutex()
{
    if (__initialization_mutex == NULL)
        __initialization_mutex = new Mutex();
    return *__initialization_mutex;
}
// force initialization (single-threaded environment)
Mutex* __initialization_mutex_initializer = &getInitializationMutex();

#if defined(HAVE_PROTOBUF) && !defined(BUILD_PLUGIN)
namespace {
using namespace google::protobuf;
class ProtobufShutdown {
public:
    bool initialized;
    ProtobufShutdown() : initialized(true) {}
    ~ProtobufShutdown()
    {
        initialized = false;
        google::protobuf::ShutdownProtobufLibrary();
    }
};
} // namespace
#endif

void initializeLayerFactory()
{
    CV_TRACE_FUNCTION();

#if defined(HAVE_PROTOBUF) && !defined(BUILD_PLUGIN)
    static ProtobufShutdown protobufShutdown; CV_UNUSED(protobufShutdown);
#endif

    CV_DNN_REGISTER_LAYER_CLASS(Slice,          SliceLayer);
    CV_DNN_REGISTER_LAYER_CL...",1,src\init.cpp,cv.dnn,50,dnn,1
392778,NAMESPACE_BLOCK,<empty>,,src\int8layers\batch_norm_layer.cpp,src\int8layers\batch_norm_layer.cpp:<global>,,<global>,1
392782,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class BatchNormLayerInt8Impl CV_FINAL : public BatchNormLayerInt8
{
public:
    Mat origin_weights, origin_bias;
    Mat weights_, bias_;
    mutable int dims;

    BatchNormLayerInt8Impl(const LayerParams& params)
        : dims(-1)
    {
        setParamsFrom(params);
        useGlobalStats = params.get<bool>(""use_global_stats"", true);
        input_sc = params.get<float>(""input_scale"");
        input_zp = params.get<int>(""input_zeropoint"");
        output_sc = params.get<float>(""scales"");
        output_zp = params.get<int>(""zeropoints"");

        CV_Assert(blobs.size() == 2);
        size_t n = blobs[0].total();
        CV_Assert(blobs[1].total() == n &&
                  blobs[0].isContinuous() && blobs[1].isContinuous() &&
                  blobs[0].type() == CV_32F && blobs[1].type() == CV_32F);

        origin_weights = blobs[0];
        origin_bias = blobs[1];
    }

    virtual void finalize(InputArrayOfArrays, OutputArrayOfArrays) CV_OVERRI...",1,src\int8layers\batch_norm_layer.cpp,cv,12,cv,1
392783,NAMESPACE_BLOCK,"namespace dnn
{

class BatchNormLayerInt8Impl CV_FINAL : public BatchNormLayerInt8
{
public:
    Mat origin_weights, origin_bias;
    Mat weights_, bias_;
    mutable int dims;

    BatchNormLayerInt8Impl(const LayerParams& params)
        : dims(-1)
    {
        setParamsFrom(params);
        useGlobalStats = params.get<bool>(""use_global_stats"", true);
        input_sc = params.get<float>(""input_scale"");
        input_zp = params.get<int>(""input_zeropoint"");
        output_sc = params.get<float>(""scales"");
        output_zp = params.get<int>(""zeropoints"");

        CV_Assert(blobs.size() == 2);
        size_t n = blobs[0].total();
        CV_Assert(blobs[1].total() == n &&
                  blobs[0].isContinuous() && blobs[1].isContinuous() &&
                  blobs[0].type() == CV_32F && blobs[1].type() == CV_32F);

        origin_weights = blobs[0];
        origin_bias = blobs[1];
    }

    virtual void finalize(InputArrayOfArrays, OutputArrayOfArrays) CV_OVERRIDE
    {
      ...",1,src\int8layers\batch_norm_layer.cpp,cv.dnn,14,dnn,1
392817,NAMESPACE_BLOCK,<empty>,,src\int8layers\convolution_layer.cpp,src\int8layers\convolution_layer.cpp:<global>,,<global>,1
392821,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

#if CV_SIMD128
static inline void v_expand_mul_add(const v_int8x16& a, const v_int8x16& b,
                                    v_int32x4& out0, v_int32x4& out1, v_int32x4& out2, v_int32x4& out3)
{
    v_int16x8 a0, a1, b0, b1;
    v_expand(a, a0, a1);
    v_expand(b, b0, b1);

    v_int32x4 t0, t1;
    v_mul_expand(a0, b0, t0, t1);
    out0 = v_add(out0, t0); out1 = v_add(out1, t1);

    v_mul_expand(a1, b1, t0, t1);
    out2 = v_add(out2, t0); out3 = v_add(out3, t1);
}
#endif

class BaseConvolutionLayerInt8Impl : public ConvolutionLayerInt8
{
public:
    BaseConvolutionLayerInt8Impl(const LayerParams &params)
    {
        setParamsFrom(params);
        getConvolutionKernelParams(params, kernel_size, pads_begin, pads_end, strides, dilations, padMode, adjust_pads, useWinograd);

        numOutput = params.get<int>(""num_output"");
        int ngroups = params.get<int>(""group"", 1);
        CV_Assert(numOutput % ngroups == 0);

        input_sc = params.g...",1,src\int8layers\convolution_layer.cpp,cv,17,cv,1
392822,NAMESPACE_BLOCK,"namespace dnn
{

#if CV_SIMD128
static inline void v_expand_mul_add(const v_int8x16& a, const v_int8x16& b,
                                    v_int32x4& out0, v_int32x4& out1, v_int32x4& out2, v_int32x4& out3)
{
    v_int16x8 a0, a1, b0, b1;
    v_expand(a, a0, a1);
    v_expand(b, b0, b1);

    v_int32x4 t0, t1;
    v_mul_expand(a0, b0, t0, t1);
    out0 = v_add(out0, t0); out1 = v_add(out1, t1);

    v_mul_expand(a1, b1, t0, t1);
    out2 = v_add(out2, t0); out3 = v_add(out3, t1);
}
#endif

class BaseConvolutionLayerInt8Impl : public ConvolutionLayerInt8
{
public:
    BaseConvolutionLayerInt8Impl(const LayerParams &params)
    {
        setParamsFrom(params);
        getConvolutionKernelParams(params, kernel_size, pads_begin, pads_end, strides, dilations, padMode, adjust_pads, useWinograd);

        numOutput = params.get<int>(""num_output"");
        int ngroups = params.get<int>(""group"", 1);
        CV_Assert(numOutput % ngroups == 0);

        input_sc = params.get<float>(""inpu...",1,src\int8layers\convolution_layer.cpp,cv.dnn,19,dnn,1
393104,NAMESPACE_BLOCK,<empty>,,src\int8layers\elementwise_layers.cpp,src\int8layers\elementwise_layers.cpp:<global>,,<global>,1
393108,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class ActivationLayerInt8Impl CV_FINAL : public ActivationLayerInt8
{
public:
    int input_zp, output_zp;
    float input_sc, output_sc;
    float slope = 0.0f;

#ifdef HAVE_TIMVX
    tvActivationType tvActType;
#endif
    ActivationLayerInt8Impl(const LayerParams &params)
    {
        setParamsFrom(params);
        activationLUT = !blobs.empty() ? blobs[0] : Mat();

        input_zp = params.get<int>(""input_zeropoint"");
        input_sc = params.get<float>(""input_scale"");
        output_zp = params.get<int>(""zeropoints"");
        output_sc = params.get<float>(""scales"");

        if (params.has(""slope""))
        {
            slope = params.get<float>(""slope"");
        }

#ifdef HAVE_TIMVX
        tvActType = getTimVXActType(type);
#endif

    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_TIMVX
        if (backendId == DNN_BACKEND_TIMVX)
        {
            // TODO!: Leaky ReLU will be supported in future.
       ...",1,src\int8layers\elementwise_layers.cpp,cv,13,cv,1
393109,NAMESPACE_BLOCK,"namespace dnn
{

class ActivationLayerInt8Impl CV_FINAL : public ActivationLayerInt8
{
public:
    int input_zp, output_zp;
    float input_sc, output_sc;
    float slope = 0.0f;

#ifdef HAVE_TIMVX
    tvActivationType tvActType;
#endif
    ActivationLayerInt8Impl(const LayerParams &params)
    {
        setParamsFrom(params);
        activationLUT = !blobs.empty() ? blobs[0] : Mat();

        input_zp = params.get<int>(""input_zeropoint"");
        input_sc = params.get<float>(""input_scale"");
        output_zp = params.get<int>(""zeropoints"");
        output_sc = params.get<float>(""scales"");

        if (params.has(""slope""))
        {
            slope = params.get<float>(""slope"");
        }

#ifdef HAVE_TIMVX
        tvActType = getTimVXActType(type);
#endif

    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_TIMVX
        if (backendId == DNN_BACKEND_TIMVX)
        {
            // TODO!: Leaky ReLU will be supported in future.
            if (tvActT...",1,src\int8layers\elementwise_layers.cpp,cv.dnn,15,dnn,1
393135,NAMESPACE_BLOCK,<empty>,,src\int8layers\eltwise_layer.cpp,src\int8layers\eltwise_layer.cpp:<global>,,<global>,1
393139,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class EltwiseLayerInt8Impl CV_FINAL : public EltwiseLayerInt8
{
public:
    enum EltwiseOp
    {
        PROD = 0,
        SUM = 1,
        MAX = 2
    } op;
    std::vector<float> coeffs;
    std::vector<int> zeropoints;
    std::vector<float> scales;

    int output_zp;
    float output_sc;

    enum OutputChannelsMode
    {
        ELTWISE_CHANNNELS_SAME = 0,              //!< number of channels from inputs must be the same and equal to output's number of channels
        ELTWISE_CHANNNELS_INPUT_0,               //!< number of channels from inputs may be different,
                                                 //!< output's number of channels is equal to number of channels of first input
                                                 //!< number of channels of other inputs should not be greater than number of channels of first input
        ELTWISE_CHANNNELS_INPUT_0_TRUNCATE,      //!< number of channels from inputs may be different,
         ...",1,src\int8layers\eltwise_layer.cpp,cv,11,cv,1
393140,NAMESPACE_BLOCK,"namespace dnn
{

class EltwiseLayerInt8Impl CV_FINAL : public EltwiseLayerInt8
{
public:
    enum EltwiseOp
    {
        PROD = 0,
        SUM = 1,
        MAX = 2
    } op;
    std::vector<float> coeffs;
    std::vector<int> zeropoints;
    std::vector<float> scales;

    int output_zp;
    float output_sc;

    enum OutputChannelsMode
    {
        ELTWISE_CHANNNELS_SAME = 0,              //!< number of channels from inputs must be the same and equal to output's number of channels
        ELTWISE_CHANNNELS_INPUT_0,               //!< number of channels from inputs may be different,
                                                 //!< output's number of channels is equal to number of channels of first input
                                                 //!< number of channels of other inputs should not be greater than number of channels of first input
        ELTWISE_CHANNNELS_INPUT_0_TRUNCATE,      //!< number of channels from inputs may be different,
                        ...",1,src\int8layers\eltwise_layer.cpp,cv.dnn,13,dnn,1
393166,NAMESPACE_BLOCK,<empty>,,src\int8layers\fully_connected_layer.cpp,src\int8layers\fully_connected_layer.cpp:<global>,,<global>,1
393170,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class FullyConnectedLayerInt8Impl CV_FINAL : public InnerProductLayerInt8
{
public:
    enum { VEC_ALIGN = 32 };
    FullyConnectedLayerInt8Impl(const LayerParams& params)
    {
        setParamsFrom(params);

        input_sc = params.get<float>(""input_scale"");
        input_zp = params.get<int>(""input_zeropoint"");
        output_zp = params.get<int>(""zeropoints"");
        output_sc = params.get<float>(""scales"");
        axis = params.get<int>(""axis"", 1);
        per_channel = params.get<bool>(""per_channel"", true);

        if (blobs.size() == 3)
        {
            // blobs[0] - Weights
            // blobs[1] - Bias fused with offset
            // blobs[2] - Multipliers for output stage
            int numOutput = params.get<int>(""num_output"");
            int innerSize = (int)blobs[0].total() / numOutput;

            CV_Assert(blobs[0].dims >= 2 && (size_t)(innerSize * numOutput) == blobs[0].total());
            CV_Assert((size_t)numOutput ==...",1,src\int8layers\fully_connected_layer.cpp,cv,12,cv,1
393171,NAMESPACE_BLOCK,"namespace dnn
{

class FullyConnectedLayerInt8Impl CV_FINAL : public InnerProductLayerInt8
{
public:
    enum { VEC_ALIGN = 32 };
    FullyConnectedLayerInt8Impl(const LayerParams& params)
    {
        setParamsFrom(params);

        input_sc = params.get<float>(""input_scale"");
        input_zp = params.get<int>(""input_zeropoint"");
        output_zp = params.get<int>(""zeropoints"");
        output_sc = params.get<float>(""scales"");
        axis = params.get<int>(""axis"", 1);
        per_channel = params.get<bool>(""per_channel"", true);

        if (blobs.size() == 3)
        {
            // blobs[0] - Weights
            // blobs[1] - Bias fused with offset
            // blobs[2] - Multipliers for output stage
            int numOutput = params.get<int>(""num_output"");
            int innerSize = (int)blobs[0].total() / numOutput;

            CV_Assert(blobs[0].dims >= 2 && (size_t)(innerSize * numOutput) == blobs[0].total());
            CV_Assert((size_t)numOutput == blobs[1].total...",1,src\int8layers\fully_connected_layer.cpp,cv.dnn,14,dnn,1
393197,NAMESPACE_BLOCK,<empty>,,src\int8layers\layers_common.hpp,src\int8layers\layers_common.hpp:<global>,,<global>,1
393201,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
void getConvolutionKernelParams(const LayerParams &params, std::vector<size_t>& kernel, std::vector<size_t>& pads_begin,
                                std::vector<size_t>& pads_end, std::vector<size_t>& strides, std::vector<size_t>& dilations,
                                cv::String &padMode, std::vector<size_t>& adjust_pads, bool& useWinograd);

void getPoolingKernelParams(const LayerParams &params, std::vector<size_t>& kernel, std::vector<bool>& globalPooling,
                            std::vector<size_t>& pads_begin, std::vector<size_t>& pads_end, std::vector<size_t>& strides, cv::String &padMode);

void getConvPoolOutParams(const std::vector<int>& inp, const std::vector<size_t>& kernel,
                          const std::vector<size_t>& stride, const String &padMode,
                          const std::vector<size_t>& dilation, std::vector<int>& out);

 void getConvPoolPaddings(const std::vector<int>& inp, const std::vector<size_t>& kerne...",1,src\int8layers\layers_common.hpp,cv,20,cv,1
393202,NAMESPACE_BLOCK,"namespace dnn
{
void getConvolutionKernelParams(const LayerParams &params, std::vector<size_t>& kernel, std::vector<size_t>& pads_begin,
                                std::vector<size_t>& pads_end, std::vector<size_t>& strides, std::vector<size_t>& dilations,
                                cv::String &padMode, std::vector<size_t>& adjust_pads, bool& useWinograd);

void getPoolingKernelParams(const LayerParams &params, std::vector<size_t>& kernel, std::vector<bool>& globalPooling,
                            std::vector<size_t>& pads_begin, std::vector<size_t>& pads_end, std::vector<size_t>& strides, cv::String &padMode);

void getConvPoolOutParams(const std::vector<int>& inp, const std::vector<size_t>& kernel,
                          const std::vector<size_t>& stride, const String &padMode,
                          const std::vector<size_t>& dilation, std::vector<int>& out);

 void getConvPoolPaddings(const std::vector<int>& inp, const std::vector<size_t>& kernel,
            ...",1,src\int8layers\layers_common.hpp,cv.dnn,22,dnn,1
393250,NAMESPACE_BLOCK,<empty>,,src\int8layers\layers_common.simd.hpp,src\int8layers\layers_common.simd.hpp:<global>,,<global>,1
393254,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

void fastConv( const int8_t* weights, size_t wstep, const int* bias,
               const int8_t* rowbuf, int* output, const int* outShape,
               int blockSize, int vecsize, int vecsize_aligned, int outZp,
               const float* multiplier, bool initOutput, bool finalOutput );
void fastDepthwiseConv( const int8_t* wptr,
                        int kernel_h, int kernel_w,
                        int stride_h, int stride_w,
                        int dilation_h, int dilation_w,
                        int pad_t, int pad_l,
                        const int* biasptr, const float* multptr,
                        const int8_t* inptr_,
                        int height, int width,
                        int* outptr_,
                        int out_d, int outH, int outW,
                        int inpZp, int outZp );
void fastGEMM1T( const int8_t* vec, const int8_t* weights,
                 size_t wste...",1,src\int8layers\layers_common.simd.hpp,cv,7,cv,1
393255,NAMESPACE_BLOCK,"namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

void fastConv( const int8_t* weights, size_t wstep, const int* bias,
               const int8_t* rowbuf, int* output, const int* outShape,
               int blockSize, int vecsize, int vecsize_aligned, int outZp,
               const float* multiplier, bool initOutput, bool finalOutput );
void fastDepthwiseConv( const int8_t* wptr,
                        int kernel_h, int kernel_w,
                        int stride_h, int stride_w,
                        int dilation_h, int dilation_w,
                        int pad_t, int pad_l,
                        const int* biasptr, const float* multptr,
                        const int8_t* inptr_,
                        int height, int width,
                        int* outptr_,
                        int out_d, int outH, int outW,
                        int inpZp, int outZp );
void fastGEMM1T( const int8_t* vec, const int8_t* weights,
                 size_t wstep, const int* b...",1,src\int8layers\layers_common.simd.hpp,cv.dnn,8,dnn,1
393329,NAMESPACE_BLOCK,<empty>,,src\int8layers\pooling_layer.cpp,src\int8layers\pooling_layer.cpp:<global>,,<global>,1
393333,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class PoolingLayerInt8Impl CV_FINAL : public PoolingLayerInt8
{
public:
    PoolingLayerInt8Impl(const LayerParams& params)
    {
        computeMaxIdx = false;
        globalPooling = false;
        isGlobalPooling = std::vector<bool>(3, false);
        output_zp = params.get<int>(""zeropoints"", 0);
        input_zp = params.get<int>(""input_zeropoint"", output_zp);
        multiplier = params.get<float>(""multiplier"", 1.f);

        output_sc = params.get<float>(""scales"", 1.f);
        input_sc =  multiplier * output_sc;

        hasDynamicShapes = params.get<bool>(""has_dynamic_shapes"", false);
        shapesInitialized = !hasDynamicShapes;

        if (params.has(""pool"") || params.has(""kernel_size"") ||
            params.has(""kernel_w"") || params.has(""kernel_h""))
        {
            String pool = toLowerCase(params.get<String>(""pool"", ""max""));
            if (pool == ""max"")
                type = MAX;
            else if (pool == ""ave"")
             ...",1,src\int8layers\pooling_layer.cpp,cv,17,cv,1
393334,NAMESPACE_BLOCK,"namespace dnn
{

class PoolingLayerInt8Impl CV_FINAL : public PoolingLayerInt8
{
public:
    PoolingLayerInt8Impl(const LayerParams& params)
    {
        computeMaxIdx = false;
        globalPooling = false;
        isGlobalPooling = std::vector<bool>(3, false);
        output_zp = params.get<int>(""zeropoints"", 0);
        input_zp = params.get<int>(""input_zeropoint"", output_zp);
        multiplier = params.get<float>(""multiplier"", 1.f);

        output_sc = params.get<float>(""scales"", 1.f);
        input_sc =  multiplier * output_sc;

        hasDynamicShapes = params.get<bool>(""has_dynamic_shapes"", false);
        shapesInitialized = !hasDynamicShapes;

        if (params.has(""pool"") || params.has(""kernel_size"") ||
            params.has(""kernel_w"") || params.has(""kernel_h""))
        {
            String pool = toLowerCase(params.get<String>(""pool"", ""max""));
            if (pool == ""max"")
                type = MAX;
            else if (pool == ""ave"")
                type = AVE;
...",1,src\int8layers\pooling_layer.cpp,cv.dnn,19,dnn,1
393358,NAMESPACE_BLOCK,<empty>,,src\int8layers\quantization_utils.cpp,src\int8layers\quantization_utils.cpp:<global>,,<global>,1
393362,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

static void broadcast1D2TargetMat(Mat& data, const MatShape& targetShape, int axis)
{
    // The data is the 1-D scales or zeropoints.
    CV_Assert(axis >= 0 && targetShape.size() > axis && data.total() == targetShape[axis]);
    std::vector<int> broadcast_axes;
    for (int i = 0; i < targetShape.size(); i++)
    {
        if (i != axis)
            broadcast_axes.push_back(i);
    }

    MatShape subTargetShape = shape(data);

    // convert std::vector to 1D Mat.
    for (auto broadcast_axis : broadcast_axes)
    {
        subTargetShape[broadcast_axis] = targetShape[broadcast_axis];
        data = data.reshape(0, total(data, 0, broadcast_axis));
        Mat tmp = cv::repeat(data, 1, subTargetShape[broadcast_axis]);
        data = tmp.reshape(0, subTargetShape);
    }
}

static void broadcastScaleAndZeropoint(Mat& scalesMat, Mat& zeropointsMat, const std::vector<float>& scales,
                                       const std::vector<int>& zeropoi...",1,src\int8layers\quantization_utils.cpp,cv,10,cv,1
393363,NAMESPACE_BLOCK,"namespace dnn
{

static void broadcast1D2TargetMat(Mat& data, const MatShape& targetShape, int axis)
{
    // The data is the 1-D scales or zeropoints.
    CV_Assert(axis >= 0 && targetShape.size() > axis && data.total() == targetShape[axis]);
    std::vector<int> broadcast_axes;
    for (int i = 0; i < targetShape.size(); i++)
    {
        if (i != axis)
            broadcast_axes.push_back(i);
    }

    MatShape subTargetShape = shape(data);

    // convert std::vector to 1D Mat.
    for (auto broadcast_axis : broadcast_axes)
    {
        subTargetShape[broadcast_axis] = targetShape[broadcast_axis];
        data = data.reshape(0, total(data, 0, broadcast_axis));
        Mat tmp = cv::repeat(data, 1, subTargetShape[broadcast_axis]);
        data = tmp.reshape(0, subTargetShape);
    }
}

static void broadcastScaleAndZeropoint(Mat& scalesMat, Mat& zeropointsMat, const std::vector<float>& scales,
                                       const std::vector<int>& zeropoints, const MatS...",1,src\int8layers\quantization_utils.cpp,cv.dnn,12,dnn,1
393637,NAMESPACE_BLOCK,<empty>,,src\int8layers\scale_layer.cpp,src\int8layers\scale_layer.cpp:<global>,,<global>,1
393641,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class ScaleLayerInt8Impl CV_FINAL : public ScaleLayerInt8
{
public:
    Mat weights, bias;
    ScaleLayerInt8Impl(const LayerParams& params)
    {
        setParamsFrom(params);
        hasBias = params.get<bool>(""bias_term"", false);
        axis = params.get<int>(""axis"", 1);
        hasWeights = false;

        output_sc = params.get<float>(""scales"");
        output_zp = params.get<int>(""zeropoints"");

        DictValue inpSc = params.get(""input_scales"");
        DictValue inpZp = params.get(""input_zeropoints"");

        for (int i = 0; i < inpSc.size(); i++)
        {
            inp_sc.push_back(inpSc.get<float>(i));
            inp_zp.push_back(inpZp.get<int>(i));
        }
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        outputs.a...",1,src\int8layers\scale_layer.cpp,cv,11,cv,1
393642,NAMESPACE_BLOCK,"namespace dnn
{

class ScaleLayerInt8Impl CV_FINAL : public ScaleLayerInt8
{
public:
    Mat weights, bias;
    ScaleLayerInt8Impl(const LayerParams& params)
    {
        setParamsFrom(params);
        hasBias = params.get<bool>(""bias_term"", false);
        axis = params.get<int>(""axis"", 1);
        hasWeights = false;

        output_sc = params.get<float>(""scales"");
        output_zp = params.get<int>(""zeropoints"");

        DictValue inpSc = params.get(""input_scales"");
        DictValue inpZp = params.get(""input_zeropoints"");

        for (int i = 0; i < inpSc.size(); i++)
        {
            inp_sc.push_back(inpSc.get<float>(i));
            inp_zp.push_back(inpZp.get<int>(i));
        }
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        outputs.assign(1, inputs...",1,src\int8layers\scale_layer.cpp,cv.dnn,13,dnn,1
393704,NAMESPACE_BLOCK,<empty>,,src\int8layers\softmax_layer.cpp,src\int8layers\softmax_layer.cpp:<global>,,<global>,1
393708,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class SoftMaxLayerInt8Impl CV_FINAL : public SoftmaxLayerInt8
{
public:
    float input_sc;
    int input_zp;

    SoftMaxLayerInt8Impl(const LayerParams& params)
    {
        setParamsFrom(params);

        axis = params.get<int>(""axis"", 1);
        logSoftMax = params.get<bool>(""log_softmax"", false);
        coerced_2d = params.get<bool>(""coerced_2d"", false);

        input_sc = params.get<float>(""input_scale"");
        input_zp = params.get<int>(""input_zeropoint"");

        output_sc = params.get<float>(""scales"");
        output_zp = params.get<int>(""zeropoints"");

        if (blobs.empty()) // if no lookUpTable is found
        {
            Mat lookUpTable(1, 256, CV_32F);
            float* table = lookUpTable.ptr<float>();
            for (int i = -128; i < 128; i++)
            {
                float x = input_sc * (i - 127); // ensures exp(x) is always between (0, 1)
                table[i + 128] = std::exp(x);
            }
            bl...",1,src\int8layers\softmax_layer.cpp,cv,13,cv,1
393709,NAMESPACE_BLOCK,"namespace dnn
{

class SoftMaxLayerInt8Impl CV_FINAL : public SoftmaxLayerInt8
{
public:
    float input_sc;
    int input_zp;

    SoftMaxLayerInt8Impl(const LayerParams& params)
    {
        setParamsFrom(params);

        axis = params.get<int>(""axis"", 1);
        logSoftMax = params.get<bool>(""log_softmax"", false);
        coerced_2d = params.get<bool>(""coerced_2d"", false);

        input_sc = params.get<float>(""input_scale"");
        input_zp = params.get<int>(""input_zeropoint"");

        output_sc = params.get<float>(""scales"");
        output_zp = params.get<int>(""zeropoints"");

        if (blobs.empty()) // if no lookUpTable is found
        {
            Mat lookUpTable(1, 256, CV_32F);
            float* table = lookUpTable.ptr<float>();
            for (int i = -128; i < 128; i++)
            {
                float x = input_sc * (i - 127); // ensures exp(x) is always between (0, 1)
                table[i + 128] = std::exp(x);
            }
            blobs.push_back(l...",1,src\int8layers\softmax_layer.cpp,cv.dnn,15,dnn,1
393727,NAMESPACE_BLOCK,<empty>,,src\layer.cpp,src\layer.cpp:<global>,,<global>,1
393731,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


Layer::Layer() { preferableTarget = DNN_TARGET_CPU; }

Layer::Layer(const LayerParams& params)
    : blobs(params.blobs)
    , name(params.name)
    , type(params.type)
{
    preferableTarget = DNN_TARGET_CPU;
}

void Layer::setParamsFrom(const LayerParams& params)
{
    blobs = params.blobs;
    name = params.name;
    type = params.type;
}

int Layer::inputNameToIndex(String)
{
    return -1;
}

int Layer::outputNameToIndex(const String&)
{
    return 0;
}

bool Layer::supportBackend(int backendId)
{
    return backendId == DNN_BACKEND_OPENCV;
}

Ptr<BackendNode> Layer::initCUDA(
        void*,
        const std::vector<Ptr<BackendWrapper>>&,
        const std::vector<Ptr<BackendWrapper>>&)
{
    CV_Error(Error::StsNotImplemented, ""CUDA pipeline of "" + type + "" layers is not defined."");
    return Ptr<BackendNode>();
}

Ptr<BackendNode> Layer::initVkCom(const std::vector<Ptr<BackendWrapper> > &inputs,
                       ...",1,src\layer.cpp,cv,7,cv,1
393732,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


Layer::Layer() { preferableTarget = DNN_TARGET_CPU; }

Layer::Layer(const LayerParams& params)
    : blobs(params.blobs)
    , name(params.name)
    , type(params.type)
{
    preferableTarget = DNN_TARGET_CPU;
}

void Layer::setParamsFrom(const LayerParams& params)
{
    blobs = params.blobs;
    name = params.name;
    type = params.type;
}

int Layer::inputNameToIndex(String)
{
    return -1;
}

int Layer::outputNameToIndex(const String&)
{
    return 0;
}

bool Layer::supportBackend(int backendId)
{
    return backendId == DNN_BACKEND_OPENCV;
}

Ptr<BackendNode> Layer::initCUDA(
        void*,
        const std::vector<Ptr<BackendWrapper>>&,
        const std::vector<Ptr<BackendWrapper>>&)
{
    CV_Error(Error::StsNotImplemented, ""CUDA pipeline of "" + type + "" layers is not defined."");
    return Ptr<BackendNode>();
}

Ptr<BackendNode> Layer::initVkCom(const std::vector<Ptr<BackendWrapper> > &inputs,
                                  std:...",1,src\layer.cpp,cv.dnn,8,dnn,1
394484,NAMESPACE_BLOCK,<empty>,,src\layer_factory.cpp,src\layer_factory.cpp:<global>,,<global>,1
394488,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

Mutex& getLayerFactoryMutex()
{
    static Mutex* volatile instance = NULL;
    if (instance == NULL)
    {
        cv::AutoLock lock(getInitializationMutex());
        if (instance == NULL)
            instance = new Mutex();
    }
    return *instance;
}

static LayerFactory_Impl& getLayerFactoryImpl_()
{
    static LayerFactory_Impl impl;
    return impl;
}

LayerFactory_Impl& getLayerFactoryImpl()
{
    static LayerFactory_Impl* volatile instance = NULL;
    if (instance == NULL)
    {
        cv::AutoLock lock(getLayerFactoryMutex());
        if (instance == NULL)
        {
            instance = &getLayerFactoryImpl_();
            initializeLayerFactory();
        }
    }
    return *instance;
}

void LayerFactory::registerLayer(const String& type, Constructor constructor)
{
    CV_TRACE_FUNCTION();
    CV_TRACE_ARG_VALUE(type, ""type"", type.c_str());

    cv::AutoLock lock(getLayerFactoryMutex());
    LayerFactory_Impl::...",1,src\layer_factory.cpp,cv,10,cv,1
394489,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

Mutex& getLayerFactoryMutex()
{
    static Mutex* volatile instance = NULL;
    if (instance == NULL)
    {
        cv::AutoLock lock(getInitializationMutex());
        if (instance == NULL)
            instance = new Mutex();
    }
    return *instance;
}

static LayerFactory_Impl& getLayerFactoryImpl_()
{
    static LayerFactory_Impl impl;
    return impl;
}

LayerFactory_Impl& getLayerFactoryImpl()
{
    static LayerFactory_Impl* volatile instance = NULL;
    if (instance == NULL)
    {
        cv::AutoLock lock(getLayerFactoryMutex());
        if (instance == NULL)
        {
            instance = &getLayerFactoryImpl_();
            initializeLayerFactory();
        }
    }
    return *instance;
}

void LayerFactory::registerLayer(const String& type, Constructor constructor)
{
    CV_TRACE_FUNCTION();
    CV_TRACE_ARG_VALUE(type, ""type"", type.c_str());

    cv::AutoLock lock(getLayerFactoryMutex());
    LayerFactory_Impl::iterator it = g...",1,src\layer_factory.cpp,cv.dnn,11,dnn,1
394769,NAMESPACE_BLOCK,<empty>,,src\layer_internals.hpp,src\layer_internals.hpp:<global>,,<global>,1
394773,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN
inline namespace detail {

struct LayerPin
{
    int lid;
    int oid;

    LayerPin(int layerId = -1, int outputId = -1)
        : lid(layerId)
        , oid(outputId)
    {}

    bool valid() const
    {
        return (lid >= 0 && oid >= 0);
    }

    bool equal(const LayerPin& r) const
    {
        return (lid == r.lid && oid == r.oid);
    }

    bool operator<(const LayerPin& r) const
    {
        return lid < r.lid || (lid == r.lid && oid < r.oid);
    }

    bool operator==(const LayerPin& r) const
    {
        return lid == r.lid && oid == r.oid;
    }
};

struct LayerData
{
    LayerData()
        : id(-1)
        , dtype(CV_32F)
        , skip(false)
        , flag(0)
    {}
    LayerData(int _id, const String& _name, const String& _type, const int& _dtype, LayerParams& _params)
        : id(_id)
        , name(_name)
        , type(_type)
        , dtype(_dtype)
        , params(_params)
        , skip(false)
   ...",1,src\layer_internals.hpp,cv,8,cv,1
394774,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN
inline namespace detail {

struct LayerPin
{
    int lid;
    int oid;

    LayerPin(int layerId = -1, int outputId = -1)
        : lid(layerId)
        , oid(outputId)
    {}

    bool valid() const
    {
        return (lid >= 0 && oid >= 0);
    }

    bool equal(const LayerPin& r) const
    {
        return (lid == r.lid && oid == r.oid);
    }

    bool operator<(const LayerPin& r) const
    {
        return lid < r.lid || (lid == r.lid && oid < r.oid);
    }

    bool operator==(const LayerPin& r) const
    {
        return lid == r.lid && oid == r.oid;
    }
};

struct LayerData
{
    LayerData()
        : id(-1)
        , dtype(CV_32F)
        , skip(false)
        , flag(0)
    {}
    LayerData(int _id, const String& _name, const String& _type, const int& _dtype, LayerParams& _params)
        : id(_id)
        , name(_name)
        , type(_type)
        , dtype(_dtype)
        , params(_params)
        , skip(false)
        , flag(0)
...",16,src\layer_internals.hpp,cv.dnn,8,dnn,1
394776,NAMESPACE_BLOCK,"inline namespace detail {

struct LayerPin
{
    int lid;
    int oid;

    LayerPin(int layerId = -1, int outputId = -1)
        : lid(layerId)
        , oid(outputId)
    {}

    bool valid() const
    {
        return (lid >= 0 && oid >= 0);
    }

    bool equal(const LayerPin& r) const
    {
        return (lid == r.lid && oid == r.oid);
    }

    bool operator<(const LayerPin& r) const
    {
        return lid < r.lid || (lid == r.lid && oid < r.oid);
    }

    bool operator==(const LayerPin& r) const
    {
        return lid == r.lid && oid == r.oid;
    }
};

struct LayerData
{
    LayerData()
        : id(-1)
        , dtype(CV_32F)
        , skip(false)
        , flag(0)
    {}
    LayerData(int _id, const String& _name, const String& _type, const int& _dtype, LayerParams& _params)
        : id(_id)
        , name(_name)
        , type(_type)
        , dtype(_dtype)
        , params(_params)
        , skip(false)
        , flag(0)
    {
        CV_TRACE_FUNCTION();

    ...",1,src\layer_internals.hpp,cv.dnn.detail,10,detail,2
395103,NAMESPACE_BLOCK,<empty>,,src\layers\accum_layer.cpp,src\layers\accum_layer.cpp:<global>,,<global>,1
395107,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class AccumLayerImpl CV_FINAL : public AccumLayer
{
public:
    AccumLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        top_height = params.get<int>(""top_height"", 0);
        top_width = params.get<int>(""top_width"", 0);
        divisor = params.get<int>(""size_divisible_by"", 0);
        have_reference = params.get<String>(""have_reference"", ""false"") == ""true"";
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        std::vector<int> outShape;
        int batch = inputs[0][0];
        outShape.push_back(batch);

        if (have_reference)
        {
            CV_Assert(inputs.size() >= 2);
            int totalchannels = 0;
            for (int i = 0; i < inputs.size() - 1; i++) {
 ...",1,src\layers\accum_layer.cpp,cv,12,cv,1
395108,NAMESPACE_BLOCK,"namespace dnn {

class AccumLayerImpl CV_FINAL : public AccumLayer
{
public:
    AccumLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        top_height = params.get<int>(""top_height"", 0);
        top_width = params.get<int>(""top_width"", 0);
        divisor = params.get<int>(""size_divisible_by"", 0);
        have_reference = params.get<String>(""have_reference"", ""false"") == ""true"";
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        std::vector<int> outShape;
        int batch = inputs[0][0];
        outShape.push_back(batch);

        if (have_reference)
        {
            CV_Assert(inputs.size() >= 2);
            int totalchannels = 0;
            for (int i = 0; i < inputs.size() - 1; i++) {
                ...",16,src\layers\accum_layer.cpp,cv.dnn,12,dnn,1
395128,NAMESPACE_BLOCK,<empty>,,src\layers\arg_layer.cpp,src\layers\arg_layer.cpp:<global>,,<global>,1
395132,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ArgLayerImpl CV_FINAL : public ArgLayer
{
public:
    enum class ArgOp
    {
        MIN = 0,
        MAX = 1,
    };

    ArgLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        axis = params.get<int>(""axis"", 0);
        keepdims = (params.get<int>(""keepdims"", 1) == 1);
        select_last_index = (params.get<int>(""select_last_index"", 0) == 1);

        const std::string& argOp = params.get<std::string>(""op"");

        if (argOp == ""max"")
        {
            op = ArgOp::MAX;
        }
        else if (argOp == ""min"")
        {
            op = ArgOp::MIN;
        }
        else
        {
            CV_Error(Error::StsBadArg, ""Unsupported operation"");
        }
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV && preferableTarget == DNN_TARGET_CPU;
    }

    void handleKeepDims(MatShape& shape, const int axis_) const
    {
        if (keepdims)...",1,src\layers\arg_layer.cpp,cv,9,cv,1
395133,NAMESPACE_BLOCK,"namespace dnn {

class ArgLayerImpl CV_FINAL : public ArgLayer
{
public:
    enum class ArgOp
    {
        MIN = 0,
        MAX = 1,
    };

    ArgLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        axis = params.get<int>(""axis"", 0);
        keepdims = (params.get<int>(""keepdims"", 1) == 1);
        select_last_index = (params.get<int>(""select_last_index"", 0) == 1);

        const std::string& argOp = params.get<std::string>(""op"");

        if (argOp == ""max"")
        {
            op = ArgOp::MAX;
        }
        else if (argOp == ""min"")
        {
            op = ArgOp::MIN;
        }
        else
        {
            CV_Error(Error::StsBadArg, ""Unsupported operation"");
        }
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV && preferableTarget == DNN_TARGET_CPU;
    }

    void handleKeepDims(MatShape& shape, const int axis_) const
    {
        if (keepdims)
        {
    ...",16,src\layers\arg_layer.cpp,cv.dnn,9,dnn,1
395157,NAMESPACE_BLOCK,<empty>,,src\layers\attention_layer.cpp,src\layers\attention_layer.cpp:<global>,,<global>,1
395161,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

static void packWeight(size_t num_heads, size_t head_size, size_t input_hidden_size,
                       const float *weight_data, size_t hidden_size, std::vector<float> &packed_weight, const FastGemmOpt &opt) {
    // num_heads * pack(head_size, input_hidden_size)
    size_t pack_size = fastGemmPackBSize(head_size, input_hidden_size, opt);
    size_t packed_weight_size = num_heads * pack_size;
    packed_weight.resize(packed_weight_size, 0.f);
    auto *packed_weight_data = packed_weight.data();
    for (size_t i = 0; i < num_heads; i++) {
        fastGemmPackB(false, head_size, input_hidden_size, weight_data, hidden_size, packed_weight_data, opt);
        packed_weight_data += pack_size;
        weight_data += head_size;
    }
}

// Operator spec: https://github.com/microsoft/onnxruntime/blob/v1.16.1/docs/ContribOperators.md#com.microsoft.Attention
class AttentionLayerImpl CV_FINAL : public AttentionLayer {
 public:
    AttentionLayerImpl(const L...",1,src\layers\attention_layer.cpp,cv,11,cv,1
395162,NAMESPACE_BLOCK,"namespace dnn {

static void packWeight(size_t num_heads, size_t head_size, size_t input_hidden_size,
                       const float *weight_data, size_t hidden_size, std::vector<float> &packed_weight, const FastGemmOpt &opt) {
    // num_heads * pack(head_size, input_hidden_size)
    size_t pack_size = fastGemmPackBSize(head_size, input_hidden_size, opt);
    size_t packed_weight_size = num_heads * pack_size;
    packed_weight.resize(packed_weight_size, 0.f);
    auto *packed_weight_data = packed_weight.data();
    for (size_t i = 0; i < num_heads; i++) {
        fastGemmPackB(false, head_size, input_hidden_size, weight_data, hidden_size, packed_weight_data, opt);
        packed_weight_data += pack_size;
        weight_data += head_size;
    }
}

// Operator spec: https://github.com/microsoft/onnxruntime/blob/v1.16.1/docs/ContribOperators.md#com.microsoft.Attention
class AttentionLayerImpl CV_FINAL : public AttentionLayer {
 public:
    AttentionLayerImpl(const LayerParams &par...",16,src\layers\attention_layer.cpp,cv.dnn,11,dnn,1
395261,NAMESPACE_BLOCK,<empty>,,src\layers\batch_norm_layer.cpp,src\layers\batch_norm_layer.cpp:<global>,,<global>,1
395265,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class BatchNormLayerImpl CV_FINAL : public BatchNormLayer
{
public:
    Mat origin_weights, origin_bias;
    Mat weights_, bias_;
    UMat umat_weight, umat_bias;
    mutable int dims;
    float momentum;


    BatchNormLayerImpl(const LayerParams& params)
        : dims(-1)
    {
        setParamsFrom(params);
        CV_Assert(blobs.size() >= 2);

        hasWeights = params.get<bool>(""has_weight"", false);
        hasBias = params.get<bool>(""has_bias"", false);
        useGlobalStats = params.get<bool>(""use_global_stats"", true);
        if(params.get<bool>(""scale_bias"", false))
            hasWeights = hasBias = true;
        epsilon = params.get<float>(""eps"", 1E-5);

        // std::cout << params.get<float>(""momentum"", 0.9) << std::endl;
        momentum = params.get<float>(""momentum"", 0.9);

        size_t n = blobs[0].total();
        CV_Assert(blobs[1].total() == n &&
                  blobs[0].isContinuous() && blobs[1].isContinuous() &&
      ...",1,src\layers\batch_norm_layer.cpp,cv,32,cv,1
395266,NAMESPACE_BLOCK,"namespace dnn
{

class BatchNormLayerImpl CV_FINAL : public BatchNormLayer
{
public:
    Mat origin_weights, origin_bias;
    Mat weights_, bias_;
    UMat umat_weight, umat_bias;
    mutable int dims;
    float momentum;


    BatchNormLayerImpl(const LayerParams& params)
        : dims(-1)
    {
        setParamsFrom(params);
        CV_Assert(blobs.size() >= 2);

        hasWeights = params.get<bool>(""has_weight"", false);
        hasBias = params.get<bool>(""has_bias"", false);
        useGlobalStats = params.get<bool>(""use_global_stats"", true);
        if(params.get<bool>(""scale_bias"", false))
            hasWeights = hasBias = true;
        epsilon = params.get<float>(""eps"", 1E-5);

        // std::cout << params.get<float>(""momentum"", 0.9) << std::endl;
        momentum = params.get<float>(""momentum"", 0.9);

        size_t n = blobs[0].total();
        CV_Assert(blobs[1].total() == n &&
                  blobs[0].isContinuous() && blobs[1].isContinuous() &&
                  blo...",1,src\layers\batch_norm_layer.cpp,cv.dnn,34,dnn,1
395294,NAMESPACE_BLOCK,<empty>,,src\layers\blank_layer.cpp,src\layers\blank_layer.cpp:<global>,,<global>,1
395298,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
class BlankLayerImpl CV_FINAL : public BlankLayer
{
public:
    BlankLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_CANN;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        Layer::getMemoryShapes(inputs, requiredOutputs, outputs, internals);
        return true;
    }

#ifdef HAVE_OPENCL
    bool forward_ocl(InputArrayOfArrays inputs_, OutputArrayOfArrays outputs_, OutputArrayOfArrays intern...",1,src\layers\blank_layer.cpp,cv,53,cv,1
395299,NAMESPACE_BLOCK,"namespace dnn
{
class BlankLayerImpl CV_FINAL : public BlankLayer
{
public:
    BlankLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_CANN;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        Layer::getMemoryShapes(inputs, requiredOutputs, outputs, internals);
        return true;
    }

#ifdef HAVE_OPENCL
    bool forward_ocl(InputArrayOfArrays inputs_, OutputArrayOfArrays outputs_, OutputArrayOfArrays internals_)
    {
   ...",1,src\layers\blank_layer.cpp,cv.dnn,55,dnn,1
395390,NAMESPACE_BLOCK,<empty>,,src\layers\concat_layer.cpp,src\layers\concat_layer.cpp:<global>,,<global>,1
395394,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class ConcatLayerImpl CV_FINAL : public ConcatLayer
{
public:
    ConcatLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        axis = params.get<int>(""axis"", 1);
        padding = params.get<bool>(""padding"", false);
        paddingValue = params.get<int>(""padding_value"", 0);

        zeropoint = params.get<int>(""zeropoints"", 0);
        scale = params.get<float>(""scales"", 1.0f);
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() > 0);
        outputs.resize(1, inputs[0]);
        int cAxis = normalize_axis(axis, inputs[0]);

        int axisSum = 0;
        for (size_t i = 0; i < inputs.size(); i++)
        {
            MatShape curShape = inputs[i];

   ...",1,src\layers\concat_layer.cpp,cv,63,cv,1
395395,NAMESPACE_BLOCK,"namespace dnn
{

class ConcatLayerImpl CV_FINAL : public ConcatLayer
{
public:
    ConcatLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        axis = params.get<int>(""axis"", 1);
        padding = params.get<bool>(""padding"", false);
        paddingValue = params.get<int>(""padding_value"", 0);

        zeropoint = params.get<int>(""zeropoints"", 0);
        scale = params.get<float>(""scales"", 1.0f);
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() > 0);
        outputs.resize(1, inputs[0]);
        int cAxis = normalize_axis(axis, inputs[0]);

        int axisSum = 0;
        for (size_t i = 0; i < inputs.size(); i++)
        {
            MatShape curShape = inputs[i];

            if (pa...",1,src\layers\concat_layer.cpp,cv.dnn,65,dnn,1
395431,NAMESPACE_BLOCK,<empty>,,src\layers\const_layer.cpp,src\layers\const_layer.cpp:<global>,,<global>,1
395435,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ConstLayerImpl CV_FINAL : public ConstLayer
{
public:
    ConstLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert(blobs.size() == 1);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_WEBNN ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_CANN;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.empty());
        outputs.assign(1, shape(blobs[0]));
        return false;
    }

#...",1,src\layers\const_layer.cpp,cv,27,cv,1
395436,NAMESPACE_BLOCK,"namespace dnn {

class ConstLayerImpl CV_FINAL : public ConstLayer
{
public:
    ConstLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert(blobs.size() == 1);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_WEBNN ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_CANN;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.empty());
        outputs.assign(1, shape(blobs[0]));
        return false;
    }

#ifdef HAVE_OPEN...",16,src\layers\const_layer.cpp,cv.dnn,27,dnn,1
395490,NAMESPACE_BLOCK,<empty>,,src\layers\convolution_layer.cpp,src\layers\convolution_layer.cpp:<global>,,<global>,1
395494,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class BaseConvolutionLayerImpl : public ConvolutionLayer
{
public:
    bool fusedWeights, fusedBias;
    std::vector<double> weightsMultipliers;
#ifdef HAVE_WEBNN
    int groups;
#endif
    BaseConvolutionLayerImpl(const LayerParams &params)
    {
        setParamsFrom(params);
        getConvolutionKernelParams(params, kernel_size, pads_begin, pads_end, strides, dilations,
                                   padMode, adjust_pads, useWinograd);

        numOutput = params.get<int>(""num_output"");
        int ngroups = params.get<int>(""group"", 1);
#ifdef HAVE_WEBNN
        groups = ngroups;
#endif
        CV_Assert(numOutput % ngroups == 0);

        if (kernel_size.size() == 2) {
            kernel = Size(kernel_size[1], kernel_size[0]);
            stride = Size(strides[1], strides[0]);
            pad = Size(pads_begin[1], pads_begin[0]);
            dilation = Size(dilations[1], dilations[0]);

            adjustPad.height = adjust_pads[0];
         ...",1,src\layers\convolution_layer.cpp,cv,74,cv,1
395495,NAMESPACE_BLOCK,"namespace dnn
{

class BaseConvolutionLayerImpl : public ConvolutionLayer
{
public:
    bool fusedWeights, fusedBias;
    std::vector<double> weightsMultipliers;
#ifdef HAVE_WEBNN
    int groups;
#endif
    BaseConvolutionLayerImpl(const LayerParams &params)
    {
        setParamsFrom(params);
        getConvolutionKernelParams(params, kernel_size, pads_begin, pads_end, strides, dilations,
                                   padMode, adjust_pads, useWinograd);

        numOutput = params.get<int>(""num_output"");
        int ngroups = params.get<int>(""group"", 1);
#ifdef HAVE_WEBNN
        groups = ngroups;
#endif
        CV_Assert(numOutput % ngroups == 0);

        if (kernel_size.size() == 2) {
            kernel = Size(kernel_size[1], kernel_size[0]);
            stride = Size(strides[1], strides[0]);
            pad = Size(pads_begin[1], pads_begin[0]);
            dilation = Size(dilations[1], dilations[0]);

            adjustPad.height = adjust_pads[0];
            adjustPad.wi...",1,src\layers\convolution_layer.cpp,cv.dnn,76,dnn,1
395750,NAMESPACE_BLOCK,<empty>,,src\layers\correlation_layer.cpp,src\layers\correlation_layer.cpp:<global>,,<global>,1
395754,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class CorrelationLayerImpl CV_FINAL : public CorrelationLayer
{
public:
    CorrelationLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        pad = params.get<int>(""pad"", 0);
        CV_Assert_N(params.has(""kernel_size""), params.has(""max_displacement""));
        max_displacement = params.get<int>(""max_displacement"");
        kernel = params.get<int>(""kernel_size"");
        if (kernel % 2 == 0)
            CV_Error(Error::StsNotImplemented, ""Odd kernel size required."");

        stride_1 = params.get<int>(""stride_1"", 1);
        stride_2 = params.get<int>(""stride_2"", 1);
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert_N(inputs.size() == 2, inputs[0].size() == 4, input...",1,src\layers\correlation_layer.cpp,cv,12,cv,1
395755,NAMESPACE_BLOCK,"namespace dnn {

class CorrelationLayerImpl CV_FINAL : public CorrelationLayer
{
public:
    CorrelationLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        pad = params.get<int>(""pad"", 0);
        CV_Assert_N(params.has(""kernel_size""), params.has(""max_displacement""));
        max_displacement = params.get<int>(""max_displacement"");
        kernel = params.get<int>(""kernel_size"");
        if (kernel % 2 == 0)
            CV_Error(Error::StsNotImplemented, ""Odd kernel size required."");

        stride_1 = params.get<int>(""stride_1"", 1);
        stride_2 = params.get<int>(""stride_2"", 1);
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert_N(inputs.size() == 2, inputs[0].size() == 4, inputs[1].size() == ...",16,src\layers\correlation_layer.cpp,cv.dnn,12,dnn,1
395773,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\conv_block.simd.hpp,src\layers\cpu_kernels\conv_block.simd.hpp:<global>,,<global>,1
395777,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

void convBlock_F32(int np, const float* a, const float* b, float* c, int ldc, bool init_c, int width, const int convMR, const int convNR);


// FP 16 branch.
void convBlock_F16(int np, const char * _a, const char * _b, char * _c, int ldc, bool init_c, int width,
                    const int convMR_fp16, const int convNR_fp16);

void convBlockMR1_F16(int np, const char* _a, const char* _b, float *c, const float _bias, bool init_c,
                       const float minval, const float maxval, bool ifMinMaxAct, const int width, const int convNR_FP16);

#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY)

#if CV_AVX

#if !CV_FMA3 // AVX workaround
#undef _mm256_fmadd_ps
#define _mm256_fmadd_ps(a, b, c) _mm256_add_ps(c, _mm256_mul_ps(a, b))
#endif

void convBlock_F32(int np, const float* a, const float* b, float* c, int ldc, bool init_c, int width, const int convMR, const int convNR)
{
    CV_Assert(convMR == 4 && conv...",1,src\layers\cpu_kernels\conv_block.simd.hpp,cv,7,cv,1
395778,NAMESPACE_BLOCK,"namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

void convBlock_F32(int np, const float* a, const float* b, float* c, int ldc, bool init_c, int width, const int convMR, const int convNR);


// FP 16 branch.
void convBlock_F16(int np, const char * _a, const char * _b, char * _c, int ldc, bool init_c, int width,
                    const int convMR_fp16, const int convNR_fp16);

void convBlockMR1_F16(int np, const char* _a, const char* _b, float *c, const float _bias, bool init_c,
                       const float minval, const float maxval, bool ifMinMaxAct, const int width, const int convNR_FP16);

#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY)

#if CV_AVX

#if !CV_FMA3 // AVX workaround
#undef _mm256_fmadd_ps
#define _mm256_fmadd_ps(a, b, c) _mm256_add_ps(c, _mm256_mul_ps(a, b))
#endif

void convBlock_F32(int np, const float* a, const float* b, float* c, int ldc, bool init_c, int width, const int convMR, const int convNR)
{
    CV_Assert(convMR == 4 && convNR == 24);
    ...",1,src\layers\cpu_kernels\conv_block.simd.hpp,cv.dnn,8,dnn,1
395966,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\conv_depthwise.cpp,src\layers\cpu_kernels\conv_depthwise.cpp:<global>,,<global>,1
395969,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

void depthWiseBlockConv2D(const float* wptr,
                                 int kernel_h, int kernel_w,
                                 int stride_h, int stride_w,
                                 int dilation_h, int dilation_w,
                                 int pad_t, int pad_l,
                                 const float* biasptr, const float* relu,
                                 const float* inptr_,
                                 int height, int width,
                                 float* outptr_,
                                 int out_d, int outH, int outW, bool fusedAdd);

void depthWiseBlockConv1D(const float* wptr,
                                 int kernel_w, int stride_w, int dilation_w, int pad_l,
                                 const float* biasptr, const float* relu,
                                 const float* inptr_, int width,
                                 float* outptr_,
                                 int out_d,...",1,src\layers\cpu_kernels\conv_depthwise.cpp,cv,11,cv,1
395970,NAMESPACE_BLOCK,"namespace dnn {

void depthWiseBlockConv2D(const float* wptr,
                                 int kernel_h, int kernel_w,
                                 int stride_h, int stride_w,
                                 int dilation_h, int dilation_w,
                                 int pad_t, int pad_l,
                                 const float* biasptr, const float* relu,
                                 const float* inptr_,
                                 int height, int width,
                                 float* outptr_,
                                 int out_d, int outH, int outW, bool fusedAdd);

void depthWiseBlockConv1D(const float* wptr,
                                 int kernel_w, int stride_w, int dilation_w, int pad_l,
                                 const float* biasptr, const float* relu,
                                 const float* inptr_, int width,
                                 float* outptr_,
                                 int out_d, int outW, bool...",16,src\layers\cpu_kernels\conv_depthwise.cpp,cv.dnn,11,dnn,1
397348,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\conv_depthwise.simd.hpp,src\layers\cpu_kernels\conv_depthwise.simd.hpp:<global>,,<global>,1
397352,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

void fastDepthwiseConv(const float* weights,
                        int kernel_h, int kernel_w,
                        int stride_h, int stride_w,
                        int dilation_h, int dilation_w,
                        int pad_t, int pad_l,
                        const float* bias, const float* relu,
                        const float* inptr,
                        int height, int width,
                        float* outptr,
                        int out_d, int outH, int outW);

#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY) && CV_AVX

#if !CV_FMA3 // AVX workaround
#undef _mm256_fmadd_ps
#define _mm256_fmadd_ps(a, b, c) _mm256_add_ps(c, _mm256_mul_ps(a, b))
#endif

static inline void _mm256_load_deinterleave(const float* ptr, __m256& a, __m256& b)
{
    __m256 t0 = _mm256_loadu_ps(ptr);
    __m256 t1 = _mm256_loadu_ps(ptr + 8);

    __m256 lo = _mm256_permute2f128_ps(t0, t1, 0+2*16);
    __m256...",1,src\layers\cpu_kernels\conv_depthwise.simd.hpp,cv,7,cv,1
397353,NAMESPACE_BLOCK,"namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

void fastDepthwiseConv(const float* weights,
                        int kernel_h, int kernel_w,
                        int stride_h, int stride_w,
                        int dilation_h, int dilation_w,
                        int pad_t, int pad_l,
                        const float* bias, const float* relu,
                        const float* inptr,
                        int height, int width,
                        float* outptr,
                        int out_d, int outH, int outW);

#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY) && CV_AVX

#if !CV_FMA3 // AVX workaround
#undef _mm256_fmadd_ps
#define _mm256_fmadd_ps(a, b, c) _mm256_add_ps(c, _mm256_mul_ps(a, b))
#endif

static inline void _mm256_load_deinterleave(const float* ptr, __m256& a, __m256& b)
{
    __m256 t0 = _mm256_loadu_ps(ptr);
    __m256 t1 = _mm256_loadu_ps(ptr + 8);

    __m256 lo = _mm256_permute2f128_ps(t0, t1, 0+2*16);
    __m256 hi = _mm256_pe...",1,src\layers\cpu_kernels\conv_depthwise.simd.hpp,cv.dnn,8,dnn,1
397387,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\conv_winograd_f63.cpp,src\layers\cpu_kernels\conv_winograd_f63.cpp:<global>,,<global>,1
397391,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

#if CV_NEON || CV_SIMD128 || CV_TRY_AVX2
enum { VEC_ALIGN = 32, DFT_TYPE = CV_32F }; // Memory alignment.

void winofunc_accum_F32(const float* inwptr, const float* wptr, float* outbuf, int Cg, int iblock,
                            const int winoIblock, const int winoKblock, const int winoAtomF32, const int winoNatomF32);

/*Input transform*/
void winofunc_BtXB_8x8_F32(const float* inptr, int inpstep,
                          float* outptr, int Cg, const int winoIblock, const int winoAtomF32);

/*Output transform*/
void winofunc_AtXA_8x8_F32(const float* inptr, int inpstep, float* bpptr, int bpstep, float* outptr, int outstep,
                          float bias, float minval, float maxval, bool ifMinMaxAct);

int runWinograd63(InputArray _input, InputArray _fusedAddMat, OutputArray _output, const Ptr<FastConv>& conv,
                  int ntasks, float minval, float maxval, ActivationLayer* activ, bool ifMinMaxAct)
{
    Mat input = _input.getMat...",1,src\layers\cpu_kernels\conv_winograd_f63.cpp,cv,18,cv,1
397392,NAMESPACE_BLOCK,"namespace dnn {

#if CV_NEON || CV_SIMD128 || CV_TRY_AVX2
enum { VEC_ALIGN = 32, DFT_TYPE = CV_32F }; // Memory alignment.

void winofunc_accum_F32(const float* inwptr, const float* wptr, float* outbuf, int Cg, int iblock,
                            const int winoIblock, const int winoKblock, const int winoAtomF32, const int winoNatomF32);

/*Input transform*/
void winofunc_BtXB_8x8_F32(const float* inptr, int inpstep,
                          float* outptr, int Cg, const int winoIblock, const int winoAtomF32);

/*Output transform*/
void winofunc_AtXA_8x8_F32(const float* inptr, int inpstep, float* bpptr, int bpstep, float* outptr, int outstep,
                          float bias, float minval, float maxval, bool ifMinMaxAct);

int runWinograd63(InputArray _input, InputArray _fusedAddMat, OutputArray _output, const Ptr<FastConv>& conv,
                  int ntasks, float minval, float maxval, ActivationLayer* activ, bool ifMinMaxAct)
{
    Mat input = _input.getMat();
    Mat out...",16,src\layers\cpu_kernels\conv_winograd_f63.cpp,cv.dnn,18,dnn,1
397415,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\conv_winograd_f63.neon.cpp,src\layers\cpu_kernels\conv_winograd_f63.neon.cpp:<global>,,<global>,1
397419,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {

// NEON code work around.
namespace opt_NEON
{

#if CV_NEON && CV_NEON_AARCH64

/* Accumulate */
void winofunc_accum_F32(const float* inwptr, const float* wptr, float* outbuf, int Cg, int iblock,
                            const int winoIblock, const int winoKblock, const int winoAtomF32, const int winoNatomF32)
{
    CV_Assert(winoIblock == 6 && winoKblock == 4 && winoAtomF32 == 4);
    if (iblock > 3)
    {
        for (int atom_id = 0; atom_id < winoNatomF32; atom_id++,
                outbuf += winoAtomF32)
        {
            float32x4_t s00 = vdupq_n_f32(0.f), s01 = s00, s02 = s00, s03 = s00, s04 = s00, s05 = s00;
            float32x4_t s10 = vdupq_n_f32(0.f), s11 = s00, s12 = s00, s13 = s00, s14 = s00, s15 = s00;
            float32x4_t s20 = vdupq_n_f32(0.f), s21 = s00, s22 = s00, s23 = s00, s24 = s00, s25 = s00;
            float32x4_t s30 = vdupq_n_f32(0.f), s31 = s00, s32 = s00, s33 = s00, s34 = s00, s35 = s00;
            for (int c = ...",1,src\layers\cpu_kernels\conv_winograd_f63.neon.cpp,cv,9,cv,1
397420,NAMESPACE_BLOCK,"namespace dnn {

// NEON code work around.
namespace opt_NEON
{

#if CV_NEON && CV_NEON_AARCH64

/* Accumulate */
void winofunc_accum_F32(const float* inwptr, const float* wptr, float* outbuf, int Cg, int iblock,
                            const int winoIblock, const int winoKblock, const int winoAtomF32, const int winoNatomF32)
{
    CV_Assert(winoIblock == 6 && winoKblock == 4 && winoAtomF32 == 4);
    if (iblock > 3)
    {
        for (int atom_id = 0; atom_id < winoNatomF32; atom_id++,
                outbuf += winoAtomF32)
        {
            float32x4_t s00 = vdupq_n_f32(0.f), s01 = s00, s02 = s00, s03 = s00, s04 = s00, s05 = s00;
            float32x4_t s10 = vdupq_n_f32(0.f), s11 = s00, s12 = s00, s13 = s00, s14 = s00, s15 = s00;
            float32x4_t s20 = vdupq_n_f32(0.f), s21 = s00, s22 = s00, s23 = s00, s24 = s00, s25 = s00;
            float32x4_t s30 = vdupq_n_f32(0.f), s31 = s00, s32 = s00, s33 = s00, s34 = s00, s35 = s00;
            for (int c = 0; c < Cg; c++,...",1,src\layers\cpu_kernels\conv_winograd_f63.neon.cpp,cv.dnn,10,dnn,1
397421,NAMESPACE_BLOCK,"namespace opt_NEON
{

#if CV_NEON && CV_NEON_AARCH64

/* Accumulate */
void winofunc_accum_F32(const float* inwptr, const float* wptr, float* outbuf, int Cg, int iblock,
                            const int winoIblock, const int winoKblock, const int winoAtomF32, const int winoNatomF32)
{
    CV_Assert(winoIblock == 6 && winoKblock == 4 && winoAtomF32 == 4);
    if (iblock > 3)
    {
        for (int atom_id = 0; atom_id < winoNatomF32; atom_id++,
                outbuf += winoAtomF32)
        {
            float32x4_t s00 = vdupq_n_f32(0.f), s01 = s00, s02 = s00, s03 = s00, s04 = s00, s05 = s00;
            float32x4_t s10 = vdupq_n_f32(0.f), s11 = s00, s12 = s00, s13 = s00, s14 = s00, s15 = s00;
            float32x4_t s20 = vdupq_n_f32(0.f), s21 = s00, s22 = s00, s23 = s00, s24 = s00, s25 = s00;
            float32x4_t s30 = vdupq_n_f32(0.f), s31 = s00, s32 = s00, s33 = s00, s34 = s00, s35 = s00;
            for (int c = 0; c < Cg; c++, inwptr += winoIblock*winoAtomF32,
        ...",1,src\layers\cpu_kernels\conv_winograd_f63.neon.cpp,cv.dnn.opt_NEON,13,opt_NEON,1
397425,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\conv_winograd_f63.simd.hpp,src\layers\cpu_kernels\conv_winograd_f63.simd.hpp:<global>,,<global>,1
397429,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

/* Accumulate */
void winofunc_accum_F32(const float* inwptr, const float* wptr, float* outbuf, int Cg, int iblock,
                            const int winoIblock, const int winoKblock, const int winoAtomF32, const int winoNatomF32);

/*Input transform*/
void winofunc_BtXB_8x8_F32(const float* inptr, int inpstep,
                               float* outptr, int Cg, const int winoIblock, const int winoAtomF32);

/*Output transform*/
void winofunc_AtXA_8x8_F32(const float* inptr, int inpstep,
                               float* bpptr, int bpstep, float* outptr, int outstep,
                               float bias, float minval, float maxval, bool ifMinMaxAct);

// FP 16 branch, only ARMv8 supports.
void winofunc_accum_F16(const char* _inwptr, const char* _wptr, char* _outbuf, int Cg, int iblock,
                        const int winoIblock, const int winoKblock, const int winoAtomF16, const int winoNatomF16);
v...",1,src\layers\cpu_kernels\conv_winograd_f63.simd.hpp,cv,7,cv,1
397430,NAMESPACE_BLOCK,"namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

/* Accumulate */
void winofunc_accum_F32(const float* inwptr, const float* wptr, float* outbuf, int Cg, int iblock,
                            const int winoIblock, const int winoKblock, const int winoAtomF32, const int winoNatomF32);

/*Input transform*/
void winofunc_BtXB_8x8_F32(const float* inptr, int inpstep,
                               float* outptr, int Cg, const int winoIblock, const int winoAtomF32);

/*Output transform*/
void winofunc_AtXA_8x8_F32(const float* inptr, int inpstep,
                               float* bpptr, int bpstep, float* outptr, int outstep,
                               float bias, float minval, float maxval, bool ifMinMaxAct);

// FP 16 branch, only ARMv8 supports.
void winofunc_accum_F16(const char* _inwptr, const char* _wptr, char* _outbuf, int Cg, int iblock,
                        const int winoIblock, const int winoKblock, const int winoAtomF16, const int winoNatomF16);
void winofunc_Bt...",1,src\layers\cpu_kernels\conv_winograd_f63.simd.hpp,cv.dnn,8,dnn,1
399424,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\convolution.cpp,src\layers\cpu_kernels\convolution.cpp:<global>,,<global>,1
399427,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
enum { VEC_ALIGN = 32}; // Memory alignment.

void convBlock_F32(int np, const float* a, const float* b, float* c, int ldc, bool init_c, const int outLen,
               const int convMR, const int convNR);
void convBlockMR1_F32(int np, const float* a, const float* b, float *c, const float bias, bool init_c,
                  const float minval, const float maxval, bool ifMinMaxAct, const int outLen, const int convNR);

#ifdef CONV_ARM_FP16
// Fast convert float 32 to float16
static inline void _cvt32f16f(const float* src, float16_t* dst, int len)
{
    int j = 0;
    const int VECSZ = 4;
    __fp16* dst_FP16 = (__fp16 *)dst;
    if (len > VECSZ * 4)
    {
        const int VECSZ4 = 4 * VECSZ;
        for( ; j + VECSZ4 < len; j += VECSZ4)
        {

            float32x4_t v0 = vld1q_f32(src + j);
            float32x4_t v1 = vld1q_f32(src + j + 4);
            float32x4_t v2 = vld1q_f32(src + j + 8);
            float32x4_t v3 = vld1q_f32(src + j + 12...",1,src\layers\cpu_kernels\convolution.cpp,cv,19,cv,1
399428,NAMESPACE_BLOCK,"namespace dnn {
enum { VEC_ALIGN = 32}; // Memory alignment.

void convBlock_F32(int np, const float* a, const float* b, float* c, int ldc, bool init_c, const int outLen,
               const int convMR, const int convNR);
void convBlockMR1_F32(int np, const float* a, const float* b, float *c, const float bias, bool init_c,
                  const float minval, const float maxval, bool ifMinMaxAct, const int outLen, const int convNR);

#ifdef CONV_ARM_FP16
// Fast convert float 32 to float16
static inline void _cvt32f16f(const float* src, float16_t* dst, int len)
{
    int j = 0;
    const int VECSZ = 4;
    __fp16* dst_FP16 = (__fp16 *)dst;
    if (len > VECSZ * 4)
    {
        const int VECSZ4 = 4 * VECSZ;
        for( ; j + VECSZ4 < len; j += VECSZ4)
        {

            float32x4_t v0 = vld1q_f32(src + j);
            float32x4_t v1 = vld1q_f32(src + j + 4);
            float32x4_t v2 = vld1q_f32(src + j + 8);
            float32x4_t v3 = vld1q_f32(src + j + 12);

           ...",16,src\layers\cpu_kernels\convolution.cpp,cv.dnn,19,dnn,1
404389,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\convolution.hpp,src\layers\cpu_kernels\convolution.hpp:<global>,,<global>,1
404458,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {

struct FastConv
{
    int ngroups;
    int K, C, Hk, Wk, Dk;
    int stride_h, stride_w, stride_d;
    int dilation_h, dilation_w, dilation_d;
    int pad_top, pad_bottom, pad_left, pad_right, pad_front, pad_behind;

    std::vector<float> weightsBuf;     // For generic Conv 2D
    std::vector<float> weightsWinoBuf; // For Winograd F(6x6, 3x3).
    std::vector<float> biasBuf;
    float* getWeights();
    float* getWeightsWino();

    std::vector<float16_t> weightsBuf_FP16;
    std::vector<float16_t> weightsWinoBuf_FP16;
    float16_t* getWeightsFP16();
    float16_t* getWeightsWinoFP16();

    int conv_type;
    int conv_dim;  // Flag for conv1d, conv2d, or conv3d.
    bool useFP16 = false; // Only ARMv8 is supported.
#if CV_SIMD128
    bool useSIMD128 = true;
#else
    bool useSIMD128 = false;
#endif

#if CV_NEON
    bool useNEON = checkHardwareSupport(CPU_NEON);
#else
    bool useNEON = false;
#endif

    bool useAVX   = checkHardwareSupport(CPU_AVX...",1,src\layers\cpu_kernels\convolution.hpp,cv,48,cv,4
404459,NAMESPACE_BLOCK,"namespace dnn {

struct FastConv
{
    int ngroups;
    int K, C, Hk, Wk, Dk;
    int stride_h, stride_w, stride_d;
    int dilation_h, dilation_w, dilation_d;
    int pad_top, pad_bottom, pad_left, pad_right, pad_front, pad_behind;

    std::vector<float> weightsBuf;     // For generic Conv 2D
    std::vector<float> weightsWinoBuf; // For Winograd F(6x6, 3x3).
    std::vector<float> biasBuf;
    float* getWeights();
    float* getWeightsWino();

    std::vector<float16_t> weightsBuf_FP16;
    std::vector<float16_t> weightsWinoBuf_FP16;
    float16_t* getWeightsFP16();
    float16_t* getWeightsWinoFP16();

    int conv_type;
    int conv_dim;  // Flag for conv1d, conv2d, or conv3d.
    bool useFP16 = false; // Only ARMv8 is supported.
#if CV_SIMD128
    bool useSIMD128 = true;
#else
    bool useSIMD128 = false;
#endif

#if CV_NEON
    bool useNEON = checkHardwareSupport(CPU_NEON);
#else
    bool useNEON = false;
#endif

    bool useAVX   = checkHardwareSupport(CPU_AVX);
    bool use...",1,src\layers\cpu_kernels\convolution.hpp,cv.dnn,49,dnn,1
404593,NAMESPACE_BLOCK,"namespace opt_NEON {
#if CV_NEON
void convBlock_F32(int np, const float* a, const float* b, float* c, int ldc, bool init_c, int width, const int convMR, const int convNR);

void convBlockMR1_F32(int np, const float* a, const float* b, float* c, const float bias, bool init_c,
                      const float minval, const float maxval, bool ifMinMaxAct, const int width, const int convNR);

#if CV_NEON_AARCH64
/* Accumulate */
void winofunc_accum_F32(const float* inwptr, const float* wptr, float* outbuf, int Cg, int iblock,
                    const int winoIblock, const int winoKblock, const int winoAtom, const int winoNatom);

/*Input transform*/
void winofunc_BtXB_8x8_F32(const float* inptr, int inpstep,
                       float* outptr, int Cg, const int winoIblock, const int winoAtom);

/*Output transform*/
void winofunc_AtXA_8x8_F32(const float* inptr, int inpstep,
                       float* bpptr, int bpstep, float* outptr, int outstep,
                       float bias...",1,src\layers\cpu_kernels\convolution.hpp,cv.dnn.opt_NEON,116,opt_NEON,6
404749,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\fast_gemm.cpp,src\layers\cpu_kernels\fast_gemm.cpp:<global>,,<global>,1
404752,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

size_t fastGemmPackBSize(size_t N, size_t K, const FastGemmOpt &opt) {
#if CV_TRY_NEON
    if (opt.use_neon) {
        return static_cast<size_t>(opt_NEON::fastGemmPackBSize(N, K));
    } else
#endif
#if CV_TRY_AVX2
    if (opt.use_avx2) {
        return static_cast<size_t>(opt_AVX2::fastGemmPackBSize(N, K));
    } else
#endif
#if CV_TRY_AVX
    if (opt.use_avx) {
        return static_cast<size_t>(opt_AVX::fastGemmPackBSize(N, K));
    } else
#endif
#if CV_TRY_LASX
    if (opt.use_lasx) {
        return static_cast<size_t>(opt_LASX::fastGemmPackBSize(N, K));
    } else
#endif
    {
        return static_cast<size_t>(cpu_baseline::fastGemmPackBSize(N, K));
    }
}

void fastGemmPackB(const Mat &B, std::vector<float> &packed_B, bool trans, FastGemmOpt &opt) {
    CV_CheckTypeEQ(B.type(), CV_32F, ""fastGemmPackB: only float32 is supported for now"");

    auto B_shape = shape(B);
    int batch = total(B_shape, 0, B_shape.size() - 2),
        K = B_shape[B...",1,src\layers\cpu_kernels\fast_gemm.cpp,cv,21,cv,1
404753,NAMESPACE_BLOCK,"namespace dnn {

size_t fastGemmPackBSize(size_t N, size_t K, const FastGemmOpt &opt) {
#if CV_TRY_NEON
    if (opt.use_neon) {
        return static_cast<size_t>(opt_NEON::fastGemmPackBSize(N, K));
    } else
#endif
#if CV_TRY_AVX2
    if (opt.use_avx2) {
        return static_cast<size_t>(opt_AVX2::fastGemmPackBSize(N, K));
    } else
#endif
#if CV_TRY_AVX
    if (opt.use_avx) {
        return static_cast<size_t>(opt_AVX::fastGemmPackBSize(N, K));
    } else
#endif
#if CV_TRY_LASX
    if (opt.use_lasx) {
        return static_cast<size_t>(opt_LASX::fastGemmPackBSize(N, K));
    } else
#endif
    {
        return static_cast<size_t>(cpu_baseline::fastGemmPackBSize(N, K));
    }
}

void fastGemmPackB(const Mat &B, std::vector<float> &packed_B, bool trans, FastGemmOpt &opt) {
    CV_CheckTypeEQ(B.type(), CV_32F, ""fastGemmPackB: only float32 is supported for now"");

    auto B_shape = shape(B);
    int batch = total(B_shape, 0, B_shape.size() - 2),
        K = B_shape[B_shape.size() -...",16,src\layers\cpu_kernels\fast_gemm.cpp,cv.dnn,21,dnn,1
405695,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\fast_gemm.hpp,src\layers\cpu_kernels\fast_gemm.hpp:<global>,,<global>,1
405699,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

struct FastGemmOpt {
    bool use_avx;
    bool use_avx2;
    bool use_neon;
    bool use_lasx;
    bool multi_thread;

    FastGemmOpt() {
        use_avx = false;
        use_avx2 = false;
        use_neon = false;
        use_lasx = false;
        multi_thread = false;
    }

    void init() {
        use_avx = checkHardwareSupport(CPU_AVX);
        use_avx2 = checkHardwareSupport(CPU_AVX2);
        use_neon = checkHardwareSupport(CPU_NEON);
        use_lasx = checkHardwareSupport(CPU_LASX);
        multi_thread = true;
    }

    bool all() {
        return use_avx || use_avx2 || use_neon || use_lasx;
    }
};

struct MatMulHelper {
    std::vector<size_t> A_offsets;
    std::vector<size_t> B_offsets;
    std::vector<size_t> packed_B_offsets;
    std::vector<size_t> C_offsets;
    std::vector<size_t> A_rows;
    std::vector<size_t> B_rows;
    std::vector<size_t> C_rows;
    size_t batch;

    int lda0, lda1;
    int ldb0, ldb1;
    int ldc;

    ...",1,src\layers\cpu_kernels\fast_gemm.hpp,cv,18,cv,1
405700,NAMESPACE_BLOCK,"namespace dnn {

struct FastGemmOpt {
    bool use_avx;
    bool use_avx2;
    bool use_neon;
    bool use_lasx;
    bool multi_thread;

    FastGemmOpt() {
        use_avx = false;
        use_avx2 = false;
        use_neon = false;
        use_lasx = false;
        multi_thread = false;
    }

    void init() {
        use_avx = checkHardwareSupport(CPU_AVX);
        use_avx2 = checkHardwareSupport(CPU_AVX2);
        use_neon = checkHardwareSupport(CPU_NEON);
        use_lasx = checkHardwareSupport(CPU_LASX);
        multi_thread = true;
    }

    bool all() {
        return use_avx || use_avx2 || use_neon || use_lasx;
    }
};

struct MatMulHelper {
    std::vector<size_t> A_offsets;
    std::vector<size_t> B_offsets;
    std::vector<size_t> packed_B_offsets;
    std::vector<size_t> C_offsets;
    std::vector<size_t> A_rows;
    std::vector<size_t> B_rows;
    std::vector<size_t> C_rows;
    size_t batch;

    int lda0, lda1;
    int ldb0, ldb1;
    int ldc;

    int M, N, K;

 ...",16,src\layers\cpu_kernels\fast_gemm.hpp,cv.dnn,18,dnn,1
407624,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\fast_gemm_kernels.default.hpp,src\layers\cpu_kernels\fast_gemm_kernels.default.hpp:<global>,,<global>,1
407627,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace cpu_baseline {

int fastGemmPackBSize(int N, int K);

void fastGemmPackBKernel(const char *B, char *packed_B, int N, int K, int ldb0, int ldb1, int esz);

void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *B, int ldb0, int ldb1,
                    float beta, char *C, int ldc, int esz, bool multi_thread);
void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *packed_B, float beta, char *C, int ldc, int esz, bool multi_thread);

void fastGemmBatchKernel(size_t batch, const size_t *A_offsets, const size_t *B_offsets, const size_t *C_offsets,
                         int M, int N, int K, float alpha, const char *A, int lda0, int lda1,
                         const char *B, int ldb0, int ldb1, float beta, char *C, int ldc, int esz);
void fastGemmBatchKernel(size_t batch, ...",1,src\layers\cpu_kernels\fast_gemm_kernels.default.hpp,cv,77,cv,1
407628,NAMESPACE_BLOCK,"namespace dnn { namespace cpu_baseline {

int fastGemmPackBSize(int N, int K);

void fastGemmPackBKernel(const char *B, char *packed_B, int N, int K, int ldb0, int ldb1, int esz);

void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *B, int ldb0, int ldb1,
                    float beta, char *C, int ldc, int esz, bool multi_thread);
void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *packed_B, float beta, char *C, int ldc, int esz, bool multi_thread);

void fastGemmBatchKernel(size_t batch, const size_t *A_offsets, const size_t *B_offsets, const size_t *C_offsets,
                         int M, int N, int K, float alpha, const char *A, int lda0, int lda1,
                         const char *B, int ldb0, int ldb1, float beta, char *C, int ldc, int esz);
void fastGemmBatchKernel(size_t batch, const size_t *A...",16,src\layers\cpu_kernels\fast_gemm_kernels.default.hpp,cv.dnn,77,dnn,1
407629,NAMESPACE_BLOCK,"namespace cpu_baseline {

int fastGemmPackBSize(int N, int K);

void fastGemmPackBKernel(const char *B, char *packed_B, int N, int K, int ldb0, int ldb1, int esz);

void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *B, int ldb0, int ldb1,
                    float beta, char *C, int ldc, int esz, bool multi_thread);
void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *packed_B, float beta, char *C, int ldc, int esz, bool multi_thread);

void fastGemmBatchKernel(size_t batch, const size_t *A_offsets, const size_t *B_offsets, const size_t *C_offsets,
                         int M, int N, int K, float alpha, const char *A, int lda0, int lda1,
                         const char *B, int ldb0, int ldb1, float beta, char *C, int ldc, int esz);
void fastGemmBatchKernel(size_t batch, const size_t *A_offsets, const ...",32,src\layers\cpu_kernels\fast_gemm_kernels.default.hpp,cv.dnn.cpu_baseline,77,cpu_baseline,1
410589,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\fast_gemm_kernels.simd.hpp,src\layers\cpu_kernels\fast_gemm_kernels.simd.hpp:<global>,,<global>,1
410592,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

int fastGemmPackBSize(int N, int K);

void fastGemmPackBKernel(const char *B, char *packed_B, int N, int K, int ldb0, int ldb1, int esz);

void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *B, int ldb0, int ldb1,
                    float beta, char *C, int ldc, int esz, bool multi_thread);
void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *packed_B, float beta, char *C, int ldc, int esz, bool multi_thread);

void fastGemmBatchKernel(size_t batch, const size_t *A_offsets, const size_t *B_offsets, const size_t *C_offsets,
                         int M, int N, int K, float alpha, const char *A, int lda0, int lda1,
                         const char *B, int ldb0, int ldb1, float beta, char *C, int ldc, int esz);
void fastGemmBatchKernel(si...",1,src\layers\cpu_kernels\fast_gemm_kernels.simd.hpp,cv,114,cv,1
410593,NAMESPACE_BLOCK,"namespace dnn {

CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

int fastGemmPackBSize(int N, int K);

void fastGemmPackBKernel(const char *B, char *packed_B, int N, int K, int ldb0, int ldb1, int esz);

void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *B, int ldb0, int ldb1,
                    float beta, char *C, int ldc, int esz, bool multi_thread);
void fastGemmKernel(int M, int N, int K,
                    float alpha, const char *A, int lda0, int lda1,
                    const char *packed_B, float beta, char *C, int ldc, int esz, bool multi_thread);

void fastGemmBatchKernel(size_t batch, const size_t *A_offsets, const size_t *B_offsets, const size_t *C_offsets,
                         int M, int N, int K, float alpha, const char *A, int lda0, int lda1,
                         const char *B, int ldb0, int ldb1, float beta, char *C, int ldc, int esz);
void fastGemmBatchKernel(size_t batch, con...",16,src\layers\cpu_kernels\fast_gemm_kernels.simd.hpp,cv.dnn,114,dnn,1
412398,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\fast_norm.cpp,src\layers\cpu_kernels\fast_norm.cpp:<global>,,<global>,1
412401,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

void fastNorm(const Mat &input, Mat &output, float epsilon, size_t normalized_axis, bool normalize_variance) {
    const auto input_shape = shape(input);
    CV_CheckLT(normalized_axis, input_shape.size(), ""fastNorm: axis out of range"");

    size_t loops = static_cast<size_t>(total(input_shape, 0, static_cast<int>(normalized_axis))),
           norm_size = static_cast<size_t>(total(input_shape, static_cast<int>(normalized_axis)));
    float inv_norm_size = 1.0 / norm_size;

    auto fn = [&](const Range &r) {
        const auto *input_data = input.ptr<const float>();
        auto *output_data = output.ptr<float>();
        for (int i = r.start; i < r.end; i++) {
            const auto *x = input_data + norm_size * i;
            auto *y = output_data + norm_size * i;

            float mean = 0.f, mean_square = 0.f;
            for (int j = 0; j < norm_size; j++) {
                float v = x[j];
                mean += v;
                mean_square...",1,src\layers\cpu_kernels\fast_norm.cpp,cv,8,cv,1
412402,NAMESPACE_BLOCK,"namespace dnn {

void fastNorm(const Mat &input, Mat &output, float epsilon, size_t normalized_axis, bool normalize_variance) {
    const auto input_shape = shape(input);
    CV_CheckLT(normalized_axis, input_shape.size(), ""fastNorm: axis out of range"");

    size_t loops = static_cast<size_t>(total(input_shape, 0, static_cast<int>(normalized_axis))),
           norm_size = static_cast<size_t>(total(input_shape, static_cast<int>(normalized_axis)));
    float inv_norm_size = 1.0 / norm_size;

    auto fn = [&](const Range &r) {
        const auto *input_data = input.ptr<const float>();
        auto *output_data = output.ptr<float>();
        for (int i = r.start; i < r.end; i++) {
            const auto *x = input_data + norm_size * i;
            auto *y = output_data + norm_size * i;

            float mean = 0.f, mean_square = 0.f;
            for (int j = 0; j < norm_size; j++) {
                float v = x[j];
                mean += v;
                mean_square += v * v;
    ...",16,src\layers\cpu_kernels\fast_norm.cpp,cv.dnn,8,dnn,1
412683,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\fast_norm.hpp,src\layers\cpu_kernels\fast_norm.hpp:<global>,,<global>,1
412687,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

// Normalization speedup by multi-threading, mainly for Caffe MVN layer which has normalize_variance parameter.
void fastNorm(const Mat &input, Mat &output, float epsilon, size_t normalized_axis = 0, bool normalize_variance = true);

// Normalization speedup by multi-threading with absent bias. Mainly for LayerNormalization.
void fastNorm(const Mat &input, const Mat &scale, Mat &output, float epsilon, size_t normalized_axis = 0);

// Normalization speedup by multi-threading with scale and bias. Mainly for LayerNormalization.
void fastNorm(const Mat &input, const Mat &scale, const Mat &bias, Mat &output, float epsilon, size_t normalized_axis = 0);

// Channel-wise Normalization speedup by multi-threading. Scale and bias should have the same shape (C). Input should have dimension >= 3.
void fastNormChannel(const Mat &input, const Mat &scale, const Mat &bias, Mat &output, float epsilon);

}}",1,src\layers\cpu_kernels\fast_norm.hpp,cv,10,cv,1
412688,NAMESPACE_BLOCK,"namespace dnn {

// Normalization speedup by multi-threading, mainly for Caffe MVN layer which has normalize_variance parameter.
void fastNorm(const Mat &input, Mat &output, float epsilon, size_t normalized_axis = 0, bool normalize_variance = true);

// Normalization speedup by multi-threading with absent bias. Mainly for LayerNormalization.
void fastNorm(const Mat &input, const Mat &scale, Mat &output, float epsilon, size_t normalized_axis = 0);

// Normalization speedup by multi-threading with scale and bias. Mainly for LayerNormalization.
void fastNorm(const Mat &input, const Mat &scale, const Mat &bias, Mat &output, float epsilon, size_t normalized_axis = 0);

// Channel-wise Normalization speedup by multi-threading. Scale and bias should have the same shape (C). Input should have dimension >= 3.
void fastNormChannel(const Mat &input, const Mat &scale, const Mat &bias, Mat &output, float epsilon);

}",16,src\layers\cpu_kernels\fast_norm.hpp,cv.dnn,10,dnn,1
412906,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\softmax.cpp,src\layers\cpu_kernels\softmax.cpp:<global>,,<global>,1
412909,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

void softmax(Mat &dst, const Mat &src, int axis, int axisBias, int axisStep){
    CV_Assert(src.type() == CV_32F);
    CV_Assert(src.isContinuous() && dst.isContinuous());
    CV_Assert(src.size == dst.size);
    axis = normalize_axis(axis, src.dims);

    size_t outerSize = src.total(0, axis),
           innerSize = src.total(axis + 1);

    const float *srcPtr = src.ptr<float>();
    float *dstPtr = dst.ptr<float>();

    size_t outerStep = src.total(axis);
    size_t cnStep = src.total(axis + 1);

    // multi-threads
    size_t totalTasks = outerSize * innerSize;
    double nstripes = (double) totalTasks / 1024.0;
    // make the channel axis to be multiple of 8
    size_t channelAxis = (axisStep + 7) & -8;

#if (CV_SIMD || CV_SIMD_SCALABLE)
    const int nlanes = VTraits<v_float32>::vlanes();
    // the number of redundant dimension
    size_t redundantDim = nlanes - axisStep % nlanes;
#endif

    parallel_for_(Range(0, (int) totalTasks), [&](con...",1,src\layers\cpu_kernels\softmax.cpp,cv,15,cv,1
412910,NAMESPACE_BLOCK,"namespace dnn {

void softmax(Mat &dst, const Mat &src, int axis, int axisBias, int axisStep){
    CV_Assert(src.type() == CV_32F);
    CV_Assert(src.isContinuous() && dst.isContinuous());
    CV_Assert(src.size == dst.size);
    axis = normalize_axis(axis, src.dims);

    size_t outerSize = src.total(0, axis),
           innerSize = src.total(axis + 1);

    const float *srcPtr = src.ptr<float>();
    float *dstPtr = dst.ptr<float>();

    size_t outerStep = src.total(axis);
    size_t cnStep = src.total(axis + 1);

    // multi-threads
    size_t totalTasks = outerSize * innerSize;
    double nstripes = (double) totalTasks / 1024.0;
    // make the channel axis to be multiple of 8
    size_t channelAxis = (axisStep + 7) & -8;

#if (CV_SIMD || CV_SIMD_SCALABLE)
    const int nlanes = VTraits<v_float32>::vlanes();
    // the number of redundant dimension
    size_t redundantDim = nlanes - axisStep % nlanes;
#endif

    parallel_for_(Range(0, (int) totalTasks), [&](const Range &range...",16,src\layers\cpu_kernels\softmax.cpp,cv.dnn,15,dnn,1
413062,NAMESPACE_BLOCK,<empty>,,src\layers\cpu_kernels\softmax.hpp,src\layers\cpu_kernels\softmax.hpp:<global>,,<global>,1
413066,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

void softmax(Mat &dst, const Mat &src, int axis, int axisBias, int axisStep);

void softmax(Mat &dst, const Mat &src, int axis);

void logSoftmax(Mat &dst, const Mat &src, int axis);

}}",1,src\layers\cpu_kernels\softmax.hpp,cv,18,cv,1
413067,NAMESPACE_BLOCK,"namespace dnn {

void softmax(Mat &dst, const Mat &src, int axis, int axisBias, int axisStep);

void softmax(Mat &dst, const Mat &src, int axis);

void logSoftmax(Mat &dst, const Mat &src, int axis);

}",16,src\layers\cpu_kernels\softmax.hpp,cv.dnn,18,dnn,1
413100,NAMESPACE_BLOCK,<empty>,,src\layers\crop_and_resize_layer.cpp,src\layers\crop_and_resize_layer.cpp:<global>,,<global>,1
413104,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class CropAndResizeLayerImpl CV_FINAL : public CropAndResizeLayer
{
public:
    CropAndResizeLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert_N(params.has(""width""), params.has(""height""));
        outWidth = params.get<float>(""width"");
        outHeight = params.get<float>(""height"");
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV
               || backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH
               || backendId == DNN_BACKEND_CUDA
        ;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert_N(inputs.size() == 2, inputs[0].size() == 4);
        if (inputs[0][0] != 1)
            CV_Error(Error::StsNotImpl...",1,src\layers\crop_and_resize_layer.cpp,cv,16,cv,1
413105,NAMESPACE_BLOCK,"namespace dnn {

class CropAndResizeLayerImpl CV_FINAL : public CropAndResizeLayer
{
public:
    CropAndResizeLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert_N(params.has(""width""), params.has(""height""));
        outWidth = params.get<float>(""width"");
        outHeight = params.get<float>(""height"");
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV
               || backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH
               || backendId == DNN_BACKEND_CUDA
        ;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert_N(inputs.size() == 2, inputs[0].size() == 4);
        if (inputs[0][0] != 1)
            CV_Error(Error::StsNotImplemented, """");
 ...",16,src\layers\crop_and_resize_layer.cpp,cv.dnn,16,dnn,1
413127,NAMESPACE_BLOCK,<empty>,,src\layers\cumsum_layer.cpp,src\layers\cumsum_layer.cpp:<global>,,<global>,1
413131,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class CumSumLayerImpl CV_FINAL : public CumSumLayer
{
public:
    CumSumLayerImpl(const LayerParams &params)
    {
        axis_raw = params.get<int>(""axis"", 0);
        exclusive_raw = params.get<int>(""exclusive"", 0);
        reverse_raw = params.get<int>(""reverse"", 0);
        setParamsFrom(params);
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        Layer::getMemoryShapes(inputs, requiredOutputs, outputs, internals);
        return exclusive_raw == 0;
    }

    void forward(InputArrayOfArrays inputs_arr, OutputArrayOfArrays outputs_arr, OutputArrayOfArrays internals_arr) CV_OVERRIDE
    {
        CV_TRACE_FUNCTION();
        CV_TRACE_ARG_VALUE(name, ""name"", name.c_str());

        if (inputs_arr.depth() == CV_16S)
        {
          ...",1,src\layers\cumsum_layer.cpp,cv,10,cv,1
413132,NAMESPACE_BLOCK,"namespace dnn
{

class CumSumLayerImpl CV_FINAL : public CumSumLayer
{
public:
    CumSumLayerImpl(const LayerParams &params)
    {
        axis_raw = params.get<int>(""axis"", 0);
        exclusive_raw = params.get<int>(""exclusive"", 0);
        reverse_raw = params.get<int>(""reverse"", 0);
        setParamsFrom(params);
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        Layer::getMemoryShapes(inputs, requiredOutputs, outputs, internals);
        return exclusive_raw == 0;
    }

    void forward(InputArrayOfArrays inputs_arr, OutputArrayOfArrays outputs_arr, OutputArrayOfArrays internals_arr) CV_OVERRIDE
    {
        CV_TRACE_FUNCTION();
        CV_TRACE_ARG_VALUE(name, ""name"", name.c_str());

        if (inputs_arr.depth() == CV_16S)
        {
            forward_fallb...",1,src\layers\cumsum_layer.cpp,cv.dnn,12,dnn,1
413172,NAMESPACE_BLOCK,<empty>,,src\layers\detection_output_layer.cpp,src\layers\detection_output_layer.cpp:<global>,,<global>,1
413176,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

namespace util
{

class NormalizedBBox
{
public:
    float xmin, ymin, xmax, ymax;

    NormalizedBBox()
        : xmin(0), ymin(0), xmax(0), ymax(0), has_size_(false), size_(0) {}

    float size() const { return size_; }

    bool has_size() const { return has_size_; }

    void set_size(float value) { size_ = value; has_size_ = true; }

    void clear_size() { size_ = 0; has_size_ = false; }

private:
    bool has_size_;
    float size_;
};

template <typename T>
static inline bool SortScorePairDescend(const std::pair<float, T>& pair1,
                          const std::pair<float, T>& pair2)
{
    return pair1.first > pair2.first;
}

static inline float caffe_box_overlap(const util::NormalizedBBox& a, const util::NormalizedBBox& b);

static inline float caffe_norm_box_overlap(const util::NormalizedBBox& a, const util::NormalizedBBox& b);

} // namespace

class DetectionOutputLayerImpl CV_FINAL : public DetectionOutputLayer
{
public:
    unsigned...",1,src\layers\detection_output_layer.cpp,cv,70,cv,1
413177,NAMESPACE_BLOCK,"namespace dnn
{

namespace util
{

class NormalizedBBox
{
public:
    float xmin, ymin, xmax, ymax;

    NormalizedBBox()
        : xmin(0), ymin(0), xmax(0), ymax(0), has_size_(false), size_(0) {}

    float size() const { return size_; }

    bool has_size() const { return has_size_; }

    void set_size(float value) { size_ = value; has_size_ = true; }

    void clear_size() { size_ = 0; has_size_ = false; }

private:
    bool has_size_;
    float size_;
};

template <typename T>
static inline bool SortScorePairDescend(const std::pair<float, T>& pair1,
                          const std::pair<float, T>& pair2)
{
    return pair1.first > pair2.first;
}

static inline float caffe_box_overlap(const util::NormalizedBBox& a, const util::NormalizedBBox& b);

static inline float caffe_norm_box_overlap(const util::NormalizedBBox& a, const util::NormalizedBBox& b);

} // namespace

class DetectionOutputLayerImpl CV_FINAL : public DetectionOutputLayer
{
public:
    unsigned _numClasses;
 ...",1,src\layers\detection_output_layer.cpp,cv.dnn,72,dnn,1
413178,NAMESPACE_BLOCK,"namespace util
{

class NormalizedBBox
{
public:
    float xmin, ymin, xmax, ymax;

    NormalizedBBox()
        : xmin(0), ymin(0), xmax(0), ymax(0), has_size_(false), size_(0) {}

    float size() const { return size_; }

    bool has_size() const { return has_size_; }

    void set_size(float value) { size_ = value; has_size_ = true; }

    void clear_size() { size_ = 0; has_size_ = false; }

private:
    bool has_size_;
    float size_;
};

template <typename T>
static inline bool SortScorePairDescend(const std::pair<float, T>& pair1,
                          const std::pair<float, T>& pair2)
{
    return pair1.first > pair2.first;
}

static inline float caffe_box_overlap(const util::NormalizedBBox& a, const util::NormalizedBBox& b);

static inline float caffe_norm_box_overlap(const util::NormalizedBBox& a, const util::NormalizedBBox& b);

}",1,src\layers\detection_output_layer.cpp,cv.dnn.util,75,util,1
413334,NAMESPACE_BLOCK,<empty>,,src\layers\einsum_layer.cpp,src\layers\einsum_layer.cpp:<global>,,<global>,1
413337,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

static bool IsTransposeReshapeForEinsum(const std::vector<size_t>& perm,
                                        std::vector<int> input_dims,
                                        MatShape& new_shape) {
    // As long as the dims with values > 1 stay in the same order, it's a reshape.
    // Example: Shape=(1,1,1024,4096) -> perm=(2,0,3,1).
    size_t last_permuted_axis = 0;
    for (size_t i = 0; i < perm.size(); ++i) {
        if (input_dims[perm[i]] == 1)
            continue;
        if (perm[i] < last_permuted_axis)
            return false;
        last_permuted_axis = perm[i];
    }
    new_shape.assign(input_dims.begin(), input_dims.end());
    for (size_t i = 0; i < perm.size(); ++i) {
        new_shape[i] = input_dims[perm[i]];
    }
    return true;
}


static Mat Transpose(
    const Mat& input,
    const MatShape& input_shape_override,
    const std::vector<size_t> permutation)
{

    int input_rank = input_shape_override.size();
    CV...",1,src\layers\einsum_layer.cpp,cv,11,cv,1
413338,NAMESPACE_BLOCK,"namespace dnn
{

static bool IsTransposeReshapeForEinsum(const std::vector<size_t>& perm,
                                        std::vector<int> input_dims,
                                        MatShape& new_shape) {
    // As long as the dims with values > 1 stay in the same order, it's a reshape.
    // Example: Shape=(1,1,1024,4096) -> perm=(2,0,3,1).
    size_t last_permuted_axis = 0;
    for (size_t i = 0; i < perm.size(); ++i) {
        if (input_dims[perm[i]] == 1)
            continue;
        if (perm[i] < last_permuted_axis)
            return false;
        last_permuted_axis = perm[i];
    }
    new_shape.assign(input_dims.begin(), input_dims.end());
    for (size_t i = 0; i < perm.size(); ++i) {
        new_shape[i] = input_dims[perm[i]];
    }
    return true;
}


static Mat Transpose(
    const Mat& input,
    const MatShape& input_shape_override,
    const std::vector<size_t> permutation)
{

    int input_rank = input_shape_override.size();
    CV_Assert(input_r...",1,src\layers\einsum_layer.cpp,cv.dnn,13,dnn,1
417182,NAMESPACE_BLOCK,<empty>,,src\layers\elementwise_layers.cpp,src\layers\elementwise_layers.cpp:<global>,,<global>,1
417185,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

using std::abs;
using std::exp;
using std::expm1;
using std::tanh;
using std::pow;
using std::ceil;
using std::floor;
using std::log;
using std::log1p;
using std::sqrt;
using std::round;
using std::acos;
using std::acosh;
using std::asin;
using std::asinh;
using std::atan;
using std::atanh;
using std::cos;
using std::cosh;
using std::erf;
using std::sin;
using std::sinh;
using std::tan;

template<typename Func>
class ElementWiseLayer : public Func::Layer
{
public:
    class PBody : public cv::ParallelLoopBody
    {
    public:
        const Func* func_;
        const Mat* src_;
        Mat* dst_;
        int nstripes_;

        PBody(const Func &func, const Mat &src, Mat& dst, int nstripes)
        {
            func_ = &func;
            src_ = &src;
            dst_ = &dst;
            nstripes_ = nstripes;
        }

        void operator()(const Range &r) const CV_OVERRIDE
        {
            int nstripes = nstripes_, nsamples = 1, outCn = 1;
  ...",1,src\layers\elementwise_layers.cpp,cv,68,cv,1
417186,NAMESPACE_BLOCK,"namespace dnn
{

using std::abs;
using std::exp;
using std::expm1;
using std::tanh;
using std::pow;
using std::ceil;
using std::floor;
using std::log;
using std::log1p;
using std::sqrt;
using std::round;
using std::acos;
using std::acosh;
using std::asin;
using std::asinh;
using std::atan;
using std::atanh;
using std::cos;
using std::cosh;
using std::erf;
using std::sin;
using std::sinh;
using std::tan;

template<typename Func>
class ElementWiseLayer : public Func::Layer
{
public:
    class PBody : public cv::ParallelLoopBody
    {
    public:
        const Func* func_;
        const Mat* src_;
        Mat* dst_;
        int nstripes_;

        PBody(const Func &func, const Mat &src, Mat& dst, int nstripes)
        {
            func_ = &func;
            src_ = &src;
            dst_ = &dst;
            nstripes_ = nstripes;
        }

        void operator()(const Range &r) const CV_OVERRIDE
        {
            int nstripes = nstripes_, nsamples = 1, outCn = 1;
            size_...",1,src\layers\elementwise_layers.cpp,cv.dnn,70,dnn,1
418135,NAMESPACE_BLOCK,"namespace GeluApproximationConstants
{
    static constexpr float sqrt_2_pi = 0.7978845834732056f;
    static constexpr float coef_sqrt_2_pi = 0.044714998453855515f * sqrt_2_pi;
}",1,src\layers\elementwise_layers.cpp,cv.dnn.GeluApproximationConstants,845,GeluApproximationConstants,9
422143,NAMESPACE_BLOCK,<empty>,,src\layers\eltwise_layer.cpp,src\layers\eltwise_layer.cpp:<global>,,<global>,1
422147,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class EltwiseLayerImpl CV_FINAL : public EltwiseLayer
{
public:
    enum EltwiseOp
    {
        PROD = 0,
        SUM = 1,
        MAX = 2,
        DIV = 3,
        MIN = 4,
    } op;
    std::vector<float> coeffs;

    enum OutputChannelsMode
    {
        ELTWISE_CHANNNELS_SAME = 0,              //!< number of channels from inputs must be the same and equal to output's number of channels
        ELTWISE_CHANNNELS_INPUT_0,               //!< number of channels from inputs may be different,
                                                 //!< output's number of channels is equal to number of channels of first input
                                                 //!< number of channels of other inputs should not be greater than number of channels of first input
        ELTWISE_CHANNNELS_INPUT_0_TRUNCATE,      //!< number of channels from inputs may be different,
                                                 //!< output's number of channels is eq...",1,src\layers\eltwise_layer.cpp,cv,63,cv,1
422148,NAMESPACE_BLOCK,"namespace dnn
{

class EltwiseLayerImpl CV_FINAL : public EltwiseLayer
{
public:
    enum EltwiseOp
    {
        PROD = 0,
        SUM = 1,
        MAX = 2,
        DIV = 3,
        MIN = 4,
    } op;
    std::vector<float> coeffs;

    enum OutputChannelsMode
    {
        ELTWISE_CHANNNELS_SAME = 0,              //!< number of channels from inputs must be the same and equal to output's number of channels
        ELTWISE_CHANNNELS_INPUT_0,               //!< number of channels from inputs may be different,
                                                 //!< output's number of channels is equal to number of channels of first input
                                                 //!< number of channels of other inputs should not be greater than number of channels of first input
        ELTWISE_CHANNNELS_INPUT_0_TRUNCATE,      //!< number of channels from inputs may be different,
                                                 //!< output's number of channels is equal to number o...",1,src\layers\eltwise_layer.cpp,cv.dnn,65,dnn,1
422168,NAMESPACE_BLOCK,<empty>,,src\layers\expand_layer.cpp,src\layers\expand_layer.cpp:<global>,,<global>,1
422172,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ExpandLayerImpl CV_FINAL : public ExpandLayer
{
public:
    ExpandLayerImpl(const LayerParams &params) {
        setParamsFrom(params);

        // shape as param
        CV_CheckTrue(params.has(""shape""), ""DNN/Expand: shape is required in Expand layer initialization"");
        DictValue param_shape = params.get(""shape"");
        int ndims_shape = param_shape.size();
        CV_CheckGT(ndims_shape, 0, ""DNN/Expand: ndims of shape must be > 0"");
        target_shape.resize(ndims_shape);
        for (int i = 0; i < ndims_shape; i++) {
            target_shape[i] = param_shape.get<int>(i);
        }

        // FIXME: remove when 0d/1d mat is available
        const_input_1d = params.get(""const_input_1d"", false);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE {
        return backendId == DNN_BACKEND_OPENCV;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requi...",1,src\layers\expand_layer.cpp,cv,8,cv,1
422173,NAMESPACE_BLOCK,"namespace dnn {

class ExpandLayerImpl CV_FINAL : public ExpandLayer
{
public:
    ExpandLayerImpl(const LayerParams &params) {
        setParamsFrom(params);

        // shape as param
        CV_CheckTrue(params.has(""shape""), ""DNN/Expand: shape is required in Expand layer initialization"");
        DictValue param_shape = params.get(""shape"");
        int ndims_shape = param_shape.size();
        CV_CheckGT(ndims_shape, 0, ""DNN/Expand: ndims of shape must be > 0"");
        target_shape.resize(ndims_shape);
        for (int i = 0; i < ndims_shape; i++) {
            target_shape[i] = param_shape.get<int>(i);
        }

        // FIXME: remove when 0d/1d mat is available
        const_input_1d = params.get(""const_input_1d"", false);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE {
        return backendId == DNN_BACKEND_OPENCV;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
   ...",16,src\layers\expand_layer.cpp,cv.dnn,8,dnn,1
422207,NAMESPACE_BLOCK,<empty>,,src\layers\flatten_layer.cpp,src\layers\flatten_layer.cpp:<global>,,<global>,1
422211,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class FlattenLayerImpl CV_FINAL : public FlattenLayer
{
public:
    FlattenLayerImpl(const LayerParams &params)
    {
        _startAxis = params.get<int>(""axis"", 1);
        _endAxis = params.get<int>(""end_axis"", -1);
        setParamsFrom(params);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_CANN;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() > 0);
        for (size_t i = 1; i < inputs.size(); i++)
        {
            CV_Assert(...",1,src\layers\flatten_layer.cpp,cv,59,cv,1
422212,NAMESPACE_BLOCK,"namespace dnn
{

class FlattenLayerImpl CV_FINAL : public FlattenLayer
{
public:
    FlattenLayerImpl(const LayerParams &params)
    {
        _startAxis = params.get<int>(""axis"", 1);
        _endAxis = params.get<int>(""end_axis"", -1);
        setParamsFrom(params);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_CANN;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() > 0);
        for (size_t i = 1; i < inputs.size(); i++)
        {
            CV_Assert(inputs[i] == in...",1,src\layers\flatten_layer.cpp,cv.dnn,61,dnn,1
422232,NAMESPACE_BLOCK,<empty>,,src\layers\flow_warp_layer.cpp,src\layers\flow_warp_layer.cpp:<global>,,<global>,1
422236,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class FlowWarpLayerImpl CV_FINAL : public FlowWarpLayer
{
public:
    FlowWarpLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        String fill_string = toLowerCase(params.get<String>(""FillParameter"", ""ZERO""));
        if (fill_string != ""zero"")
            CV_Error(Error::StsNotImplemented, ""Only zero filling supported."");
        fill_value = 0;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() == 2);
        CV_Assert_N(inputs[0][0] == inputs[1][0], inputs[1][1] == 2,
                    inputs[0][2] == inputs[1][2], inputs[0][3] == inputs[1][3]);

        outputs.assign(1, inputs[0]);
        return false;
    }

    void forward(InputArrayOfArrays i...",1,src\layers\flow_warp_layer.cpp,cv,12,cv,1
422237,NAMESPACE_BLOCK,"namespace dnn {

class FlowWarpLayerImpl CV_FINAL : public FlowWarpLayer
{
public:
    FlowWarpLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        String fill_string = toLowerCase(params.get<String>(""FillParameter"", ""ZERO""));
        if (fill_string != ""zero"")
            CV_Error(Error::StsNotImplemented, ""Only zero filling supported."");
        fill_value = 0;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() == 2);
        CV_Assert_N(inputs[0][0] == inputs[1][0], inputs[1][1] == 2,
                    inputs[0][2] == inputs[1][2], inputs[0][3] == inputs[1][3]);

        outputs.assign(1, inputs[0]);
        return false;
    }

    void forward(InputArrayOfArrays inputs_arr, Outp...",16,src\layers\flow_warp_layer.cpp,cv.dnn,12,dnn,1
422279,NAMESPACE_BLOCK,<empty>,,src\layers\fully_connected_layer.cpp,src\layers\fully_connected_layer.cpp:<global>,,<global>,1
422283,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class FullyConnectedLayerImpl CV_FINAL : public InnerProductLayer
{
public:
    enum { VEC_ALIGN = 8 };

#ifdef HAVE_OPENCL
    Ptr<OCL4DNNInnerProduct<float> > innerProductOp;
    std::vector<UMat> umat_blobs;
    std::vector<UMat> half_blobs;
#endif

    FullyConnectedLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        transA = params.get<bool>(""transA"", false);
        transB = params.get<bool>(""transB"", false);

        bias = params.get<bool>(""bias_term"", true);
        axis = params.get<int>(""axis"", 1);
        isMatMul = params.get<bool>(""is_matmul"", false);
        if (!blobs.empty())
        {
            CV_Assert(1 <= blobs.size() && blobs.size() <= 2);
            int numOutput = params.get<int>(""num_output"");
            int innerSize = (int)blobs[0].total() / numOutput;

            CV_Assert(blobs[0].dims >= 2 && (size_t)(innerSize * numOutput) == blobs[0].total());
            CV_Assert(!bias || (blobs.siz...",1,src\layers\fully_connected_layer.cpp,cv,66,cv,1
422284,NAMESPACE_BLOCK,"namespace dnn
{

class FullyConnectedLayerImpl CV_FINAL : public InnerProductLayer
{
public:
    enum { VEC_ALIGN = 8 };

#ifdef HAVE_OPENCL
    Ptr<OCL4DNNInnerProduct<float> > innerProductOp;
    std::vector<UMat> umat_blobs;
    std::vector<UMat> half_blobs;
#endif

    FullyConnectedLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        transA = params.get<bool>(""transA"", false);
        transB = params.get<bool>(""transB"", false);

        bias = params.get<bool>(""bias_term"", true);
        axis = params.get<int>(""axis"", 1);
        isMatMul = params.get<bool>(""is_matmul"", false);
        if (!blobs.empty())
        {
            CV_Assert(1 <= blobs.size() && blobs.size() <= 2);
            int numOutput = params.get<int>(""num_output"");
            int innerSize = (int)blobs[0].total() / numOutput;

            CV_Assert(blobs[0].dims >= 2 && (size_t)(innerSize * numOutput) == blobs[0].total());
            CV_Assert(!bias || (blobs.size() == 2 && (si...",1,src\layers\fully_connected_layer.cpp,cv.dnn,68,dnn,1
422304,NAMESPACE_BLOCK,<empty>,,src\layers\gather_elements_layer.cpp,src\layers\gather_elements_layer.cpp:<global>,,<global>,1
422308,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

static inline int calculateOffset(int outer_dim, const MatShape &shape_indices, int axis_skip, const MatStep &step_data) {
    int offset = 0;
    for (int axis = static_cast<int>(shape_indices.size()) - 2; axis >= 0; axis--) {
        int dim = shape_indices[axis];
        if (axis != axis_skip) {
            offset += (outer_dim % dim) * step_data[axis];
        }
        outer_dim /= dim;
    }
    return offset;
}

class GatherElementsLayerImpl CV_FINAL : public GatherElementsLayer
{
public:
    GatherElementsLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        axis = params.get<int>(""axis"", 0);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
       ...",1,src\layers\gather_elements_layer.cpp,cv,8,cv,1
422309,NAMESPACE_BLOCK,"namespace dnn {

static inline int calculateOffset(int outer_dim, const MatShape &shape_indices, int axis_skip, const MatStep &step_data) {
    int offset = 0;
    for (int axis = static_cast<int>(shape_indices.size()) - 2; axis >= 0; axis--) {
        int dim = shape_indices[axis];
        if (axis != axis_skip) {
            offset += (outer_dim % dim) * step_data[axis];
        }
        outer_dim /= dim;
    }
    return offset;
}

class GatherElementsLayerImpl CV_FINAL : public GatherElementsLayer
{
public:
    GatherElementsLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        axis = params.get<int>(""axis"", 0);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                      ...",16,src\layers\gather_elements_layer.cpp,cv.dnn,8,dnn,1
422383,NAMESPACE_BLOCK,<empty>,,src\layers\gather_layer.cpp,src\layers\gather_layer.cpp:<global>,,<global>,1
422387,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class GatherLayerImpl CV_FINAL : public GatherLayer
{
public:
    GatherLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        m_axis = params.get<int>(""axis"", 0);
        m_real_ndims = params.get<int>(""real_ndims"", -1);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_CheckEQ(inputs.size(), 2ull, """");
        MatShape inpShape = inputs[0];
        const int axis = normalize_axis(m_axis, inpShape);

        inpShape.erase(inpShape.begin() + axis);
        auto end = m_real_ndims == -1 ? inputs[1].end() : inputs[1].begin() + m_real_ndims;
        inpShape.i...",1,src\layers\gather_layer.cpp,cv,9,cv,1
422388,NAMESPACE_BLOCK,"namespace dnn {

class GatherLayerImpl CV_FINAL : public GatherLayer
{
public:
    GatherLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        m_axis = params.get<int>(""axis"", 0);
        m_real_ndims = params.get<int>(""real_ndims"", -1);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_CheckEQ(inputs.size(), 2ull, """");
        MatShape inpShape = inputs[0];
        const int axis = normalize_axis(m_axis, inpShape);

        inpShape.erase(inpShape.begin() + axis);
        auto end = m_real_ndims == -1 ? inputs[1].end() : inputs[1].begin() + m_real_ndims;
        inpShape.insert(inpShape....",16,src\layers\gather_layer.cpp,cv.dnn,9,dnn,1
422420,NAMESPACE_BLOCK,<empty>,,src\layers\gemm_layer.cpp,src\layers\gemm_layer.cpp:<global>,,<global>,1
422424,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class GemmLayerImpl CV_FINAL : public GemmLayer {
public:
    GemmLayerImpl(const LayerParams& params) {
        setParamsFrom(params);

        trans_a = params.get<bool>(""transA"", false);
        trans_b = params.get<bool>(""transB"", false);
        alpha = params.get<float>(""alpha"", 1.0f);
        beta = params.get<float>(""beta"", 1.0f);

        const_B = params.get<bool>(""constB"", false); // true means blobs[0] is B
        const_C = params.get<bool>(""constC"", false); // true means blobs.back() is C
        have_bias = params.get<bool>(""have_bias"", false); // NOTE: have_bias being true does not mean bias is constant

        real_ndims_C = params.get<int>(""real_ndims_C"", -1);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE {
        return backendId == DNN_BACKEND_OPENCV ||
               (backendId == DNN_BACKEND_CUDA && const_B && !trans_a) ||
               backendId == DNN_BACKEND_CANN ||
               backendId == DNN_BACKEN...",1,src\layers\gemm_layer.cpp,cv,21,cv,1
422425,NAMESPACE_BLOCK,"namespace dnn {

class GemmLayerImpl CV_FINAL : public GemmLayer {
public:
    GemmLayerImpl(const LayerParams& params) {
        setParamsFrom(params);

        trans_a = params.get<bool>(""transA"", false);
        trans_b = params.get<bool>(""transB"", false);
        alpha = params.get<float>(""alpha"", 1.0f);
        beta = params.get<float>(""beta"", 1.0f);

        const_B = params.get<bool>(""constB"", false); // true means blobs[0] is B
        const_C = params.get<bool>(""constC"", false); // true means blobs.back() is C
        have_bias = params.get<bool>(""have_bias"", false); // NOTE: have_bias being true does not mean bias is constant

        real_ndims_C = params.get<int>(""real_ndims_C"", -1);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE {
        return backendId == DNN_BACKEND_OPENCV ||
               (backendId == DNN_BACKEND_CUDA && const_B && !trans_a) ||
               backendId == DNN_BACKEND_CANN ||
               backendId == DNN_BACKEND_INFERENCE_ENG...",16,src\layers\gemm_layer.cpp,cv.dnn,21,dnn,1
422459,NAMESPACE_BLOCK,<empty>,,src\layers\instance_norm_layer.cpp,src\layers\instance_norm_layer.cpp:<global>,,<global>,1
422463,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

// https://github.com/onnx/onnx/blob/main/docs/Operators.md#InstanceNormalization
class InstanceNormLayerImpl CV_FINAL : public InstanceNormLayer {
public:
    InstanceNormLayerImpl(const LayerParams &params) {
        setParamsFrom(params);

        epsilon = params.get<float>(""epsilon"", 1e-5);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA;
            //    backendId == DNN_BACKEND_CANN; // not supported due to 1d mat shape issue
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE {
        const auto &input = inputs[0];...",1,src\layers\instance_norm_layer.cpp,cv,29,cv,1
422464,NAMESPACE_BLOCK,"namespace dnn {

// https://github.com/onnx/onnx/blob/main/docs/Operators.md#InstanceNormalization
class InstanceNormLayerImpl CV_FINAL : public InstanceNormLayer {
public:
    InstanceNormLayerImpl(const LayerParams &params) {
        setParamsFrom(params);

        epsilon = params.get<float>(""epsilon"", 1e-5);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA;
            //    backendId == DNN_BACKEND_CANN; // not supported due to 1d mat shape issue
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE {
        const auto &input = inputs[0];
        const ...",16,src\layers\instance_norm_layer.cpp,cv.dnn,29,dnn,1
422500,NAMESPACE_BLOCK,<empty>,,src\layers\layer_norm.cpp,src\layers\layer_norm.cpp:<global>,,<global>,1
422504,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

// https://github.com/onnx/onnx/blob/main/docs/Operators.md#LayerNormalization
class LayerNormLayerImpl CV_FINAL : public LayerNormLayer
{
public:
    LayerNormLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        // standard attr
        axis = params.get<int>(""axis"", -1);
        epsilon = params.get<float>(""epsilon"", 1e-5);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA   ||
               (backendId == DNN_BACKEND_CANN && axis != -1); // axis=-1 not supported due to 1d mat shape problem
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
 ...",1,src\layers\layer_norm.cpp,cv,29,cv,1
422505,NAMESPACE_BLOCK,"namespace dnn {

// https://github.com/onnx/onnx/blob/main/docs/Operators.md#LayerNormalization
class LayerNormLayerImpl CV_FINAL : public LayerNormLayer
{
public:
    LayerNormLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        // standard attr
        axis = params.get<int>(""axis"", -1);
        epsilon = params.get<float>(""epsilon"", 1e-5);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
            return true;
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA   ||
               (backendId == DNN_BACKEND_CANN && axis != -1); // axis=-1 not supported due to 1d mat shape problem
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                ...",16,src\layers\layer_norm.cpp,cv.dnn,29,dnn,1
422523,NAMESPACE_BLOCK,<empty>,,src\layers\layers_common.cpp,src\layers\layers_common.cpp:<global>,,<global>,1
422527,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

namespace util
{

std::string makeName(const std::string& str1, const std::string& str2)
{
    return str1 + str2;
}

bool getParameter(const LayerParams &params, const std::string& nameBase, const std::string& nameAll,
                  std::vector<size_t>& parameter, bool hasDefault = false, const std::vector<size_t>& defaultValue = std::vector<size_t>(2, 0))
{
    std::string nameH = makeName(nameBase, std::string(""_h""));
    std::string nameW = makeName(nameBase, std::string(""_w""));
    std::string nameAll_ = nameAll;
    if (nameAll_ == """")
        nameAll_ = nameBase;

    if (params.has(nameH) && params.has(nameW))
    {
        CV_Assert(params.get<int>(nameH) >= 0 && params.get<int>(nameW) >= 0);
        parameter.push_back(params.get<int>(nameH));
        parameter.push_back(params.get<int>(nameW));
        return true;
    }
    else
    {
        if (params.has(nameAll_))
        {
            DictValue param = params.get(nameAll_);
      ...",1,src\layers\layers_common.cpp,cv,46,cv,1
422528,NAMESPACE_BLOCK,"namespace dnn
{

namespace util
{

std::string makeName(const std::string& str1, const std::string& str2)
{
    return str1 + str2;
}

bool getParameter(const LayerParams &params, const std::string& nameBase, const std::string& nameAll,
                  std::vector<size_t>& parameter, bool hasDefault = false, const std::vector<size_t>& defaultValue = std::vector<size_t>(2, 0))
{
    std::string nameH = makeName(nameBase, std::string(""_h""));
    std::string nameW = makeName(nameBase, std::string(""_w""));
    std::string nameAll_ = nameAll;
    if (nameAll_ == """")
        nameAll_ = nameBase;

    if (params.has(nameH) && params.has(nameW))
    {
        CV_Assert(params.get<int>(nameH) >= 0 && params.get<int>(nameW) >= 0);
        parameter.push_back(params.get<int>(nameH));
        parameter.push_back(params.get<int>(nameW));
        return true;
    }
    else
    {
        if (params.has(nameAll_))
        {
            DictValue param = params.get(nameAll_);
            for (int ...",1,src\layers\layers_common.cpp,cv.dnn,48,dnn,1
422529,NAMESPACE_BLOCK,"namespace util
{

std::string makeName(const std::string& str1, const std::string& str2)
{
    return str1 + str2;
}

bool getParameter(const LayerParams &params, const std::string& nameBase, const std::string& nameAll,
                  std::vector<size_t>& parameter, bool hasDefault = false, const std::vector<size_t>& defaultValue = std::vector<size_t>(2, 0))
{
    std::string nameH = makeName(nameBase, std::string(""_h""));
    std::string nameW = makeName(nameBase, std::string(""_w""));
    std::string nameAll_ = nameAll;
    if (nameAll_ == """")
        nameAll_ = nameBase;

    if (params.has(nameH) && params.has(nameW))
    {
        CV_Assert(params.get<int>(nameH) >= 0 && params.get<int>(nameW) >= 0);
        parameter.push_back(params.get<int>(nameH));
        parameter.push_back(params.get<int>(nameW));
        return true;
    }
    else
    {
        if (params.has(nameAll_))
        {
            DictValue param = params.get(nameAll_);
            for (int i = 0; i < param....",1,src\layers\layers_common.cpp,cv.dnn.util,51,util,1
423644,NAMESPACE_BLOCK,<empty>,,src\layers\layers_common.hpp,src\layers\layers_common.hpp:<global>,,<global>,1
423648,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
void getConvolutionKernelParams(const LayerParams &params, std::vector<size_t>& kernel, std::vector<size_t>& pads_begin,
                                std::vector<size_t>& pads_end, std::vector<size_t>& strides, std::vector<size_t>& dilations,
                                cv::String &padMode, std::vector<size_t>& adjust_pads, bool& useWinograd);

void getPoolingKernelParams(const LayerParams &params, std::vector<size_t>& kernel, std::vector<bool>& globalPooling,
                            std::vector<size_t>& pads_begin, std::vector<size_t>& pads_end, std::vector<size_t>& strides, cv::String &padMode);

void getConvPoolOutParams(const std::vector<int>& inp, const std::vector<size_t>& kernel,
                          const std::vector<size_t>& stride, const String &padMode,
                          const std::vector<size_t>& dilation, std::vector<int>& out);

void getConvPoolPaddings(const std::vector<int>& inp, const std::vector<size_t>& kernel...",1,src\layers\layers_common.hpp,cv,58,cv,1
423649,NAMESPACE_BLOCK,"namespace dnn
{
void getConvolutionKernelParams(const LayerParams &params, std::vector<size_t>& kernel, std::vector<size_t>& pads_begin,
                                std::vector<size_t>& pads_end, std::vector<size_t>& strides, std::vector<size_t>& dilations,
                                cv::String &padMode, std::vector<size_t>& adjust_pads, bool& useWinograd);

void getPoolingKernelParams(const LayerParams &params, std::vector<size_t>& kernel, std::vector<bool>& globalPooling,
                            std::vector<size_t>& pads_begin, std::vector<size_t>& pads_end, std::vector<size_t>& strides, cv::String &padMode);

void getConvPoolOutParams(const std::vector<int>& inp, const std::vector<size_t>& kernel,
                          const std::vector<size_t>& stride, const String &padMode,
                          const std::vector<size_t>& dilation, std::vector<int>& out);

void getConvPoolPaddings(const std::vector<int>& inp, const std::vector<size_t>& kernel,
             ...",1,src\layers\layers_common.hpp,cv.dnn,60,dnn,1
423702,NAMESPACE_BLOCK,<empty>,,src\layers\layers_common.simd.hpp,src\layers\layers_common.simd.hpp:<global>,,<global>,1
423706,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

void fastGEMM1T( const float* vec, const float* weights,
                 size_t wstep, const float* bias,
                 float* dst, int nvecs, int vecsize );
void fastGEMM( const float* aptr, size_t astep, const float* bptr,
               size_t bstep, float* cptr, size_t cstep,
               int ma, int na, int nb );

#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY) && CV_AVX

#if !CV_FMA3 // AVX workaround
#undef _mm256_fmadd_ps
#define _mm256_fmadd_ps(a, b, c) _mm256_add_ps(c, _mm256_mul_ps(a, b))
#endif

// Used to generate the mask used when calculating tails
static const uint32_t tailMaskArray[15] = {
    0, 0, 0, 0, 0, 0, 0, 0,
    0xffffffffUL, 0xffffffffUL, 0xffffffffUL, 0xffffffffUL, 0xffffffffUL, 0xffffffffUL, 0xffffffffUL
};

// dst = vec * weights^t + bias
// Requires that vecsize is at least 8 or equal to 0 to avoid memory access problems. Does not require alignment.
void fastGEMM1T( const flo...",1,src\layers\layers_common.simd.hpp,cv,45,cv,1
423707,NAMESPACE_BLOCK,"namespace dnn {
CV_CPU_OPTIMIZATION_NAMESPACE_BEGIN

void fastGEMM1T( const float* vec, const float* weights,
                 size_t wstep, const float* bias,
                 float* dst, int nvecs, int vecsize );
void fastGEMM( const float* aptr, size_t astep, const float* bptr,
               size_t bstep, float* cptr, size_t cstep,
               int ma, int na, int nb );

#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY) && CV_AVX

#if !CV_FMA3 // AVX workaround
#undef _mm256_fmadd_ps
#define _mm256_fmadd_ps(a, b, c) _mm256_add_ps(c, _mm256_mul_ps(a, b))
#endif

// Used to generate the mask used when calculating tails
static const uint32_t tailMaskArray[15] = {
    0, 0, 0, 0, 0, 0, 0, 0,
    0xffffffffUL, 0xffffffffUL, 0xffffffffUL, 0xffffffffUL, 0xffffffffUL, 0xffffffffUL, 0xffffffffUL
};

// dst = vec * weights^t + bias
// Requires that vecsize is at least 8 or equal to 0 to avoid memory access problems. Does not require alignment.
void fastGEMM1T( const float* vec, const ...",1,src\layers\layers_common.simd.hpp,cv.dnn,46,dnn,1
423763,NAMESPACE_BLOCK,<empty>,,src\layers\lrn_layer.cpp,src\layers\lrn_layer.cpp:<global>,,<global>,1
423767,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class LRNLayerImpl CV_FINAL : public LRNLayer
{
public:
    LRNLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        type = -1;
        String nrmType = params.get<String>(""norm_region"", ""ACROSS_CHANNELS"");
        if (nrmType == ""ACROSS_CHANNELS"")
            type = CHANNEL_NRM;
        else if (nrmType == ""WITHIN_CHANNEL"")
            type = SPATIAL_NRM;
        else
            CV_Error(Error::StsBadArg, ""Unknown region type \"""" + nrmType + ""\"""");

        size = params.get<int>(""local_size"", 5);
        if (size % 2 != 1 || size <= 0)
            CV_Error(Error::StsBadArg, ""LRN layer supports only positive odd values for local_size"");

        alpha = params.get<double>(""alpha"", 0.0001);
        beta = params.get<double>(""beta"", 0.75);
        bias = params.get<double>(""bias"", 1);
        normBySize = params.get<bool>(""norm_by_size"", true);
    }

#ifdef HAVE_OPENCL
    Ptr<OCL4DNNLRN<float> > lrnOp;
#endif

    virtual...",1,src\layers\lrn_layer.cpp,cv,67,cv,1
423768,NAMESPACE_BLOCK,"namespace dnn
{

class LRNLayerImpl CV_FINAL : public LRNLayer
{
public:
    LRNLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        type = -1;
        String nrmType = params.get<String>(""norm_region"", ""ACROSS_CHANNELS"");
        if (nrmType == ""ACROSS_CHANNELS"")
            type = CHANNEL_NRM;
        else if (nrmType == ""WITHIN_CHANNEL"")
            type = SPATIAL_NRM;
        else
            CV_Error(Error::StsBadArg, ""Unknown region type \"""" + nrmType + ""\"""");

        size = params.get<int>(""local_size"", 5);
        if (size % 2 != 1 || size <= 0)
            CV_Error(Error::StsBadArg, ""LRN layer supports only positive odd values for local_size"");

        alpha = params.get<double>(""alpha"", 0.0001);
        beta = params.get<double>(""beta"", 0.75);
        bias = params.get<double>(""bias"", 1);
        normBySize = params.get<bool>(""norm_by_size"", true);
    }

#ifdef HAVE_OPENCL
    Ptr<OCL4DNNLRN<float> > lrnOp;
#endif

    virtual bool supportBa...",1,src\layers\lrn_layer.cpp,cv.dnn,69,dnn,1
423800,NAMESPACE_BLOCK,<empty>,,src\layers\matmul_layer.cpp,src\layers\matmul_layer.cpp:<global>,,<global>,1
423804,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class MatMulLayerImpl CV_FINAL : public MatMulLayer {
 public:
    MatMulLayerImpl(const LayerParams& params) {
        setParamsFrom(params);

        trans_a = params.get<bool>(""transA"", false);
        trans_b = params.get<bool>(""transB"", false);
        alpha = params.get<float>(""alpha"", 1.f);
        beta = params.get<float>(""beta"", 1.f);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE {
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH ||
               (backendId == DNN_BACKEND_VKCOM && haveVulkan() && !trans_a && !trans_b) ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_CANN;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<Ma...",1,src\layers\matmul_layer.cpp,cv,26,cv,1
423805,NAMESPACE_BLOCK,"namespace dnn {

class MatMulLayerImpl CV_FINAL : public MatMulLayer {
 public:
    MatMulLayerImpl(const LayerParams& params) {
        setParamsFrom(params);

        trans_a = params.get<bool>(""transA"", false);
        trans_b = params.get<bool>(""transB"", false);
        alpha = params.get<float>(""alpha"", 1.f);
        beta = params.get<float>(""beta"", 1.f);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE {
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH ||
               (backendId == DNN_BACKEND_VKCOM && haveVulkan() && !trans_a && !trans_b) ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_CANN;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::vector<MatShape> &intern...",16,src\layers\matmul_layer.cpp,cv.dnn,26,dnn,1
423835,NAMESPACE_BLOCK,<empty>,,src\layers\max_unpooling_layer.cpp,src\layers\max_unpooling_layer.cpp:<global>,,<global>,1
423839,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class MaxUnpoolLayerImpl CV_FINAL : public MaxUnpoolLayer
{
public:
    MaxUnpoolLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        poolKernel = Size(params.get<int>(""pool_k_w""), params.get<int>(""pool_k_h""));
        poolPad = Size(params.get<int>(""pool_pad_w""), params.get<int>(""pool_pad_h""));
        poolStride = Size(params.get<int>(""pool_stride_w""), params.get<int>(""pool_stride_h""));
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH ||
               (backendId == DNN_BACKEND_HALIDE && haveHalide() && !poolPad.width && !poolPad.height);
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         s...",1,src\layers\max_unpooling_layer.cpp,cv,25,cv,1
423840,NAMESPACE_BLOCK,"namespace dnn
{

class MaxUnpoolLayerImpl CV_FINAL : public MaxUnpoolLayer
{
public:
    MaxUnpoolLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        poolKernel = Size(params.get<int>(""pool_k_w""), params.get<int>(""pool_k_h""));
        poolPad = Size(params.get<int>(""pool_pad_w""), params.get<int>(""pool_pad_h""));
        poolStride = Size(params.get<int>(""pool_stride_w""), params.get<int>(""pool_stride_h""));
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA ||
               backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH ||
               (backendId == DNN_BACKEND_HALIDE && haveHalide() && !poolPad.width && !poolPad.height);
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatS...",1,src\layers\max_unpooling_layer.cpp,cv.dnn,27,dnn,1
423876,NAMESPACE_BLOCK,<empty>,,src\layers\mvn_layer.cpp,src\layers\mvn_layer.cpp:<global>,,<global>,1
423880,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class MVNLayerImpl CV_FINAL : public MVNLayer
{
public:
    MVNLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        // Caffe params
        normVariance = params.get<bool>(""normalize_variance"", true);
        acrossChannels = params.get<bool>(""across_channels"", false);
        eps = params.get<double>(""eps"", 1e-9);

        fuse_batch_norm = false;
        fuse_relu = false;
        relu_slope = 0.f;
        zeroDev = false;
    }

    Mat scale, shift;
#ifdef HAVE_OPENCL
    UMat umat_scale, umat_shift;
#endif
    bool fuse_batch_norm;

    Ptr<ReLULayer> activ_relu;
    float relu_slope;
    bool fuse_relu;
    bool zeroDev;  // TODO: Doesn't considered in Intel's Inference Engine backend.
    bool setActivation(const Ptr<ActivationLayer>& layer) CV_OVERRIDE
    {
        if (!layer.empty() && !fuse_relu && !fuse_batch_norm)
        {
            layer->getScaleShift(scale, shift);
            fuse_batch_norm = !scale.e...",1,src\layers\mvn_layer.cpp,cv,63,cv,1
423881,NAMESPACE_BLOCK,"namespace dnn
{

class MVNLayerImpl CV_FINAL : public MVNLayer
{
public:
    MVNLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        // Caffe params
        normVariance = params.get<bool>(""normalize_variance"", true);
        acrossChannels = params.get<bool>(""across_channels"", false);
        eps = params.get<double>(""eps"", 1e-9);

        fuse_batch_norm = false;
        fuse_relu = false;
        relu_slope = 0.f;
        zeroDev = false;
    }

    Mat scale, shift;
#ifdef HAVE_OPENCL
    UMat umat_scale, umat_shift;
#endif
    bool fuse_batch_norm;

    Ptr<ReLULayer> activ_relu;
    float relu_slope;
    bool fuse_relu;
    bool zeroDev;  // TODO: Doesn't considered in Intel's Inference Engine backend.
    bool setActivation(const Ptr<ActivationLayer>& layer) CV_OVERRIDE
    {
        if (!layer.empty() && !fuse_relu && !fuse_batch_norm)
        {
            layer->getScaleShift(scale, shift);
            fuse_batch_norm = !scale.empty() || !shif...",1,src\layers\mvn_layer.cpp,cv.dnn,65,dnn,1
423917,NAMESPACE_BLOCK,<empty>,,src\layers\nary_eltwise_layers.cpp,src\layers\nary_eltwise_layers.cpp:<global>,,<global>,1
423921,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class NaryEltwiseLayerImpl CV_FINAL : public NaryEltwiseLayer
{
public:
    enum class OPERATION
    {
        AND = 0,
        EQUAL,
        GREATER,
        GREATER_EQUAL,
        LESS,
        LESS_EQUAL,
        OR,
        POW,
        XOR,
        BITSHIFT,
        MAX,
        MEAN,
        MIN,
        MOD,
        PROD,
        SUB,
        SUM,
        ADD,
        DIV,
        WHERE,
    } op;

    NaryEltwiseLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        String operation = toLowerCase(params.get<String>(""operation"", ""sum""));

        if (operation == ""equal"")
            op = OPERATION::EQUAL;
        else if (operation == ""greater"")
            op = OPERATION::GREATER;
        else if (operation == ""greaterorequal"")
            op = OPERATION::GREATER_EQUAL;
        else if (operation == ""less"")
            op = OPERATION::LESS;
        else if (operation == ""lessorequal"")
            op = OPERATION::L...",1,src\layers\nary_eltwise_layers.cpp,cv,22,cv,1
423922,NAMESPACE_BLOCK,"namespace dnn
{

class NaryEltwiseLayerImpl CV_FINAL : public NaryEltwiseLayer
{
public:
    enum class OPERATION
    {
        AND = 0,
        EQUAL,
        GREATER,
        GREATER_EQUAL,
        LESS,
        LESS_EQUAL,
        OR,
        POW,
        XOR,
        BITSHIFT,
        MAX,
        MEAN,
        MIN,
        MOD,
        PROD,
        SUB,
        SUM,
        ADD,
        DIV,
        WHERE,
    } op;

    NaryEltwiseLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        String operation = toLowerCase(params.get<String>(""operation"", ""sum""));

        if (operation == ""equal"")
            op = OPERATION::EQUAL;
        else if (operation == ""greater"")
            op = OPERATION::GREATER;
        else if (operation == ""greaterorequal"")
            op = OPERATION::GREATER_EQUAL;
        else if (operation == ""less"")
            op = OPERATION::LESS;
        else if (operation == ""lessorequal"")
            op = OPERATION::LESS_EQUAL;
    ...",1,src\layers\nary_eltwise_layers.cpp,cv.dnn,24,dnn,1
423950,NAMESPACE_BLOCK,<empty>,,src\layers\normalize_bbox_layer.cpp,src\layers\normalize_bbox_layer.cpp:<global>,,<global>,1
423954,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class NormalizeBBoxLayerImpl CV_FINAL : public NormalizeBBoxLayer
{
public:
    NormalizeBBoxLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        pnorm = params.get<float>(""p"", 2);
        epsilon = params.get<float>(""eps"", 1e-10f);
        acrossSpatial = params.get<bool>(""across_spatial"", true);
        startAxis = params.get<int>(""start_axis"", 1);
        CV_Assert(!params.has(""across_spatial"") || !params.has(""end_axis""));
        endAxis = params.get<int>(""end_axis"", acrossSpatial ? -1 : startAxis);
        CV_Assert(pnorm > 0);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
        {
            if (pnorm != 2)
                return false;

            return startAxis == 1;
        }
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               (backendId == DNN_BACKEND_CUDA && (pnorm == 1 || pnorm...",1,src\layers\normalize_bbox_layer.cpp,cv,54,cv,1
423955,NAMESPACE_BLOCK,"namespace dnn {

class NormalizeBBoxLayerImpl CV_FINAL : public NormalizeBBoxLayer
{
public:
    NormalizeBBoxLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        pnorm = params.get<float>(""p"", 2);
        epsilon = params.get<float>(""eps"", 1e-10f);
        acrossSpatial = params.get<bool>(""across_spatial"", true);
        startAxis = params.get<int>(""start_axis"", 1);
        CV_Assert(!params.has(""across_spatial"") || !params.has(""end_axis""));
        endAxis = params.get<int>(""end_axis"", acrossSpatial ? -1 : startAxis);
        CV_Assert(pnorm > 0);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
        {
            if (pnorm != 2)
                return false;

            return startAxis == 1;
        }
#endif
        return backendId == DNN_BACKEND_OPENCV ||
               (backendId == DNN_BACKEND_CUDA && (pnorm == 1 || pnorm == 2));
    }
...",16,src\layers\normalize_bbox_layer.cpp,cv.dnn,54,dnn,1
423975,NAMESPACE_BLOCK,<empty>,,src\layers\not_implemented_layer.cpp,src\layers\not_implemented_layer.cpp:<global>,,<global>,1
423979,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN

inline namespace detail {

class NotImplementedImpl CV_FINAL : public NotImplemented
{
public:
    NotImplementedImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert(params.has(""type""));
        std::stringstream ss;
        ss << ""Node for layer '"" << params.name << ""' of type '"" << params.get(""type"") << ""' wasn't initialized."";
        msg = ss.str();
    }

    CV_DEPRECATED_EXTERNAL
    virtual void finalize(const std::vector<Mat*> &input, std::vector<Mat> &output) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
    }

    virtual void finalize(InputArrayOfArrays inputs, OutputArrayOfArrays outputs) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
    }

    CV_DEPRECATED_EXTERNAL
    virtual void forward(std::vector<Mat*> &input, std::vector<Mat> &output, std::vector<Mat> &internals) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
   ...",1,src\layers\not_implemented_layer.cpp,cv,8,cv,1
423980,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

inline namespace detail {

class NotImplementedImpl CV_FINAL : public NotImplemented
{
public:
    NotImplementedImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert(params.has(""type""));
        std::stringstream ss;
        ss << ""Node for layer '"" << params.name << ""' of type '"" << params.get(""type"") << ""' wasn't initialized."";
        msg = ss.str();
    }

    CV_DEPRECATED_EXTERNAL
    virtual void finalize(const std::vector<Mat*> &input, std::vector<Mat> &output) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
    }

    virtual void finalize(InputArrayOfArrays inputs, OutputArrayOfArrays outputs) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
    }

    CV_DEPRECATED_EXTERNAL
    virtual void forward(std::vector<Mat*> &input, std::vector<Mat> &output, std::vector<Mat> &internals) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
    }

    virtual...",16,src\layers\not_implemented_layer.cpp,cv.dnn,8,dnn,1
423982,NAMESPACE_BLOCK,"inline namespace detail {

class NotImplementedImpl CV_FINAL : public NotImplemented
{
public:
    NotImplementedImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert(params.has(""type""));
        std::stringstream ss;
        ss << ""Node for layer '"" << params.name << ""' of type '"" << params.get(""type"") << ""' wasn't initialized."";
        msg = ss.str();
    }

    CV_DEPRECATED_EXTERNAL
    virtual void finalize(const std::vector<Mat*> &input, std::vector<Mat> &output) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
    }

    virtual void finalize(InputArrayOfArrays inputs, OutputArrayOfArrays outputs) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
    }

    CV_DEPRECATED_EXTERNAL
    virtual void forward(std::vector<Mat*> &input, std::vector<Mat> &output, std::vector<Mat> &internals) CV_OVERRIDE
    {
        CV_Error(Error::StsNotImplemented, msg);
    }

    virtual void forward(InputArrayOfArrays inputs_a...",1,src\layers\not_implemented_layer.cpp,cv.dnn.detail,11,detail,2
424049,NAMESPACE_BLOCK,<empty>,,src\layers\padding_layer.cpp,src\layers\padding_layer.cpp:<global>,,<global>,1
424053,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class PaddingLayerImpl CV_FINAL : public PaddingLayer
{
public:
    PaddingLayerImpl(const LayerParams &params)
    {
        setParamsFrom(params);
        paddingValue = params.get<float>(""value"", 0);
        inputDims = params.get<int>(""input_dims"", -1);
        paddingType = params.get<String>(""type"", ""constant"");

        CV_Assert(params.has(""paddings""));
        const DictValue& paddingsParam = params.get(""paddings"");
        CV_Assert((paddingsParam.size() & 1) == 0);

        paddings.resize(paddingsParam.size() / 2);
        for (int i = 0; i < paddings.size(); ++i)
        {
            paddings[i].first = paddingsParam.get<int>(i * 2);  // Pad before.
            paddings[i].second = paddingsParam.get<int>(i * 2 + 1);  // Pad after.
            CV_Assert_N(paddings[i].first >= 0, paddings[i].second >= 0);
        }
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
     ...",1,src\layers\padding_layer.cpp,cv,27,cv,1
424054,NAMESPACE_BLOCK,"namespace dnn
{

class PaddingLayerImpl CV_FINAL : public PaddingLayer
{
public:
    PaddingLayerImpl(const LayerParams &params)
    {
        setParamsFrom(params);
        paddingValue = params.get<float>(""value"", 0);
        inputDims = params.get<int>(""input_dims"", -1);
        paddingType = params.get<String>(""type"", ""constant"");

        CV_Assert(params.has(""paddings""));
        const DictValue& paddingsParam = params.get(""paddings"");
        CV_Assert((paddingsParam.size() & 1) == 0);

        paddings.resize(paddingsParam.size() / 2);
        for (int i = 0; i < paddings.size(); ++i)
        {
            paddings[i].first = paddingsParam.get<int>(i * 2);  // Pad before.
            paddings[i].second = paddingsParam.get<int>(i * 2 + 1);  // Pad after.
            CV_Assert_N(paddings[i].first >= 0, paddings[i].second >= 0);
        }
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                    ...",1,src\layers\padding_layer.cpp,cv.dnn,29,dnn,1
424096,NAMESPACE_BLOCK,<empty>,,src\layers\permute_layer.cpp,src\layers\permute_layer.cpp:<global>,,<global>,1
424100,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
class PermuteLayerImpl CV_FINAL : public PermuteLayer
{
public:
    void checkNeedForPermutation()
    {
        _needsPermute = false;
        for (size_t i = 0; i < _numAxes; ++i)
        {
            if (_order[i] != i)
            {
                _needsPermute = true;
                break;
            }
        }
    }

    PermuteLayerImpl(const LayerParams &params)
        : _count(0), _needsPermute(false), _numAxes(0)
    {
        if (!params.has(""order""))
        {
            return;
        }

        DictValue paramOrder = params.get(""order"");
        _numAxes = paramOrder.size();

        for (size_t i = 0; i < _numAxes; i++)
        {
            int currentOrder = paramOrder.get<int>(i);
            if (currentOrder < 0 || currentOrder > _numAxes)
            {
                CV_Error(Error::StsBadArg,
                         format(""Orders of dimensions in Permute layer parameter""
                                ""must be in [0...%...",1,src\layers\permute_layer.cpp,cv,65,cv,1
424101,NAMESPACE_BLOCK,"namespace dnn
{
class PermuteLayerImpl CV_FINAL : public PermuteLayer
{
public:
    void checkNeedForPermutation()
    {
        _needsPermute = false;
        for (size_t i = 0; i < _numAxes; ++i)
        {
            if (_order[i] != i)
            {
                _needsPermute = true;
                break;
            }
        }
    }

    PermuteLayerImpl(const LayerParams &params)
        : _count(0), _needsPermute(false), _numAxes(0)
    {
        if (!params.has(""order""))
        {
            return;
        }

        DictValue paramOrder = params.get(""order"");
        _numAxes = paramOrder.size();

        for (size_t i = 0; i < _numAxes; i++)
        {
            int currentOrder = paramOrder.get<int>(i);
            if (currentOrder < 0 || currentOrder > _numAxes)
            {
                CV_Error(Error::StsBadArg,
                         format(""Orders of dimensions in Permute layer parameter""
                                ""must be in [0...%zu]"", _numAxes ...",1,src\layers\permute_layer.cpp,cv.dnn,67,dnn,1
424161,NAMESPACE_BLOCK,<empty>,,src\layers\pooling_layer.cpp,src\layers\pooling_layer.cpp:<global>,,<global>,1
424165,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
static inline int roundRoiSize(float v)
{
    return (int)(v + (v >= 0.f ? 0.5f : -0.5f));
}

class PoolingLayerImpl CV_FINAL : public PoolingLayer
{
public:
    PoolingLayerImpl(const LayerParams& params)
    {
        computeMaxIdx = true;
        globalPooling = false;
        isGlobalPooling = std::vector<bool>(3, false);

        hasDynamicShapes = params.get<bool>(""has_dynamic_shapes"", false);
        shapesInitialized = !hasDynamicShapes;

        if (params.has(""pool"") || params.has(""kernel_size"") ||
            params.has(""kernel_w"") || params.has(""kernel_h""))
        {
            String pool = toLowerCase(params.get<String>(""pool"", ""max""));
            if (pool == ""max"")
                type = MAX;
            else if (pool == ""ave"")
                type = AVE;
            else if (pool == ""stochastic"")
                type = STOCHASTIC;
            else if (pool == ""sum"")
                type = SUM;
            else
                CV_Error...",1,src\layers\pooling_layer.cpp,cv,93,cv,1
424166,NAMESPACE_BLOCK,"namespace dnn
{
static inline int roundRoiSize(float v)
{
    return (int)(v + (v >= 0.f ? 0.5f : -0.5f));
}

class PoolingLayerImpl CV_FINAL : public PoolingLayer
{
public:
    PoolingLayerImpl(const LayerParams& params)
    {
        computeMaxIdx = true;
        globalPooling = false;
        isGlobalPooling = std::vector<bool>(3, false);

        hasDynamicShapes = params.get<bool>(""has_dynamic_shapes"", false);
        shapesInitialized = !hasDynamicShapes;

        if (params.has(""pool"") || params.has(""kernel_size"") ||
            params.has(""kernel_w"") || params.has(""kernel_h""))
        {
            String pool = toLowerCase(params.get<String>(""pool"", ""max""));
            if (pool == ""max"")
                type = MAX;
            else if (pool == ""ave"")
                type = AVE;
            else if (pool == ""stochastic"")
                type = STOCHASTIC;
            else if (pool == ""sum"")
                type = SUM;
            else
                CV_Error(Error::StsBadA...",1,src\layers\pooling_layer.cpp,cv.dnn,95,dnn,1
424229,NAMESPACE_BLOCK,<empty>,,src\layers\prior_box_layer.cpp,src\layers\prior_box_layer.cpp:<global>,,<global>,1
424233,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class PriorBoxLayerImpl CV_FINAL : public PriorBoxLayer
{
public:
    static bool getParameterDict(const LayerParams &params,
                                 const std::string &parameterName,
                                 DictValue& result)
    {
        if (!params.has(parameterName))
        {
            return false;
        }

        result = params.get(parameterName);
        return true;
    }

    template<typename T>
    T getParameter(const LayerParams &params,
                   const std::string &parameterName,
                   const size_t &idx=0,
                   const bool required=true,
                   const T& defaultValue=T())
    {
        DictValue dictValue;
        bool success = getParameterDict(params, parameterName, dictValue);
        if(!success)
        {
            if(required)
            {
                std::string message = _layerName;
                message += "" layer parameter does not contain "";
     ...",1,src\layers\prior_box_layer.cpp,cv,74,cv,1
424234,NAMESPACE_BLOCK,"namespace dnn
{

class PriorBoxLayerImpl CV_FINAL : public PriorBoxLayer
{
public:
    static bool getParameterDict(const LayerParams &params,
                                 const std::string &parameterName,
                                 DictValue& result)
    {
        if (!params.has(parameterName))
        {
            return false;
        }

        result = params.get(parameterName);
        return true;
    }

    template<typename T>
    T getParameter(const LayerParams &params,
                   const std::string &parameterName,
                   const size_t &idx=0,
                   const bool required=true,
                   const T& defaultValue=T())
    {
        DictValue dictValue;
        bool success = getParameterDict(params, parameterName, dictValue);
        if(!success)
        {
            if(required)
            {
                std::string message = _layerName;
                message += "" layer parameter does not contain "";
                mess...",1,src\layers\prior_box_layer.cpp,cv.dnn,76,dnn,1
424270,NAMESPACE_BLOCK,<empty>,,src\layers\proposal_layer.cpp,src\layers\proposal_layer.cpp:<global>,,<global>,1
424274,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ProposalLayerImpl CV_FINAL : public ProposalLayer
{
public:
    ProposalLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        featStride = params.get<uint32_t>(""feat_stride"", 16);
        baseSize = params.get<uint32_t>(""base_size"", 16);
        // uint32_t minSize = params.get<uint32_t>(""min_size"", 16);
        keepTopBeforeNMS = params.get<uint32_t>(""pre_nms_topn"", 6000);
        keepTopAfterNMS = params.get<uint32_t>(""post_nms_topn"", 300);
        nmsThreshold = params.get<float>(""nms_thresh"", 0.7);
        ratios = params.get(""ratio"");
        scales = params.get(""scale"");

        {
            LayerParams lp;
            lp.set(""step"", featStride);
            lp.set(""flip"", false);
            lp.set(""clip"", false);
            lp.set(""normalized_bbox"", false);
            lp.set(""offset"", 0.5 * baseSize / featStride);

            // Unused values.
            float variance[] = {0.1f, 0.1f, 0.2f, 0.2f};
    ...",1,src\layers\proposal_layer.cpp,cv,20,cv,1
424275,NAMESPACE_BLOCK,"namespace dnn {

class ProposalLayerImpl CV_FINAL : public ProposalLayer
{
public:
    ProposalLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        featStride = params.get<uint32_t>(""feat_stride"", 16);
        baseSize = params.get<uint32_t>(""base_size"", 16);
        // uint32_t minSize = params.get<uint32_t>(""min_size"", 16);
        keepTopBeforeNMS = params.get<uint32_t>(""pre_nms_topn"", 6000);
        keepTopAfterNMS = params.get<uint32_t>(""post_nms_topn"", 300);
        nmsThreshold = params.get<float>(""nms_thresh"", 0.7);
        ratios = params.get(""ratio"");
        scales = params.get(""scale"");

        {
            LayerParams lp;
            lp.set(""step"", featStride);
            lp.set(""flip"", false);
            lp.set(""clip"", false);
            lp.set(""normalized_bbox"", false);
            lp.set(""offset"", 0.5 * baseSize / featStride);

            // Unused values.
            float variance[] = {0.1f, 0.1f, 0.2f, 0.2f};
            lp.set(...",16,src\layers\proposal_layer.cpp,cv.dnn,20,dnn,1
424303,NAMESPACE_BLOCK,<empty>,,src\layers\recurrent_layers.cpp,src\layers\recurrent_layers.cpp:<global>,,<global>,1
424307,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

template<typename Dtype>
static void tanh(const Mat &src, Mat &dst)
{
    MatConstIterator_<Dtype> itSrc = src.begin<Dtype>();
    MatIterator_<Dtype> itDst = dst.begin<Dtype>();

    for (; itSrc != src.end<Dtype>(); itSrc++, itDst++)
        *itDst = std::tanh(*itSrc);
}

//TODO: make utils method
static void tanh(const Mat &src, Mat &dst)
{
    dst.create(src.dims, (const int*)src.size, src.type());

    if (src.type() == CV_32F)
        tanh<float>(src, dst);
    else if (src.type() == CV_64F)
        tanh<double>(src, dst);
    else
        CV_Error(Error::StsUnsupportedFormat, ""Function supports only floating point types"");
}

static void sigmoid(const Mat &src, Mat &dst)
{
    cv::exp(-src, dst);
    cv::pow(1 + dst, -1, dst);
}

typedef void (*ActivationFunction)(const Mat &src, Mat &dst);
static ActivationFunction get_activation_function(const String& activation) {
    // most used activations for PyTorch and TF : Tanh, Sigmoid
    // if you ...",1,src\layers\recurrent_layers.cpp,cv,55,cv,1
424308,NAMESPACE_BLOCK,"namespace dnn
{

template<typename Dtype>
static void tanh(const Mat &src, Mat &dst)
{
    MatConstIterator_<Dtype> itSrc = src.begin<Dtype>();
    MatIterator_<Dtype> itDst = dst.begin<Dtype>();

    for (; itSrc != src.end<Dtype>(); itSrc++, itDst++)
        *itDst = std::tanh(*itSrc);
}

//TODO: make utils method
static void tanh(const Mat &src, Mat &dst)
{
    dst.create(src.dims, (const int*)src.size, src.type());

    if (src.type() == CV_32F)
        tanh<float>(src, dst);
    else if (src.type() == CV_64F)
        tanh<double>(src, dst);
    else
        CV_Error(Error::StsUnsupportedFormat, ""Function supports only floating point types"");
}

static void sigmoid(const Mat &src, Mat &dst)
{
    cv::exp(-src, dst);
    cv::pow(1 + dst, -1, dst);
}

typedef void (*ActivationFunction)(const Mat &src, Mat &dst);
static ActivationFunction get_activation_function(const String& activation) {
    // most used activations for PyTorch and TF : Tanh, Sigmoid
    // if you need to support...",1,src\layers\recurrent_layers.cpp,cv.dnn,57,dnn,1
424678,NAMESPACE_BLOCK,<empty>,,src\layers\reduce_layer.cpp,src\layers\reduce_layer.cpp:<global>,,<global>,1
424682,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ReduceLayerImpl CV_FINAL : public ReduceLayer
{
public:
    ReduceLayerImpl(const LayerParams& params) {
        setParamsFrom(params);

        // set reduce type
        CV_Assert(params.has(""reduce""));
        String op_type = toLowerCase(params.get<String>(""reduce""));
        if (op_type == ""max"")
            reduce_type = ReduceType::MAX;
        else if (op_type == ""min"")
            reduce_type = ReduceType::MIN;
        else if (op_type == ""mean"")
            reduce_type = ReduceType::MEAN;
        else if (op_type == ""sum"")
            reduce_type = ReduceType::SUM;
        else if (op_type == ""sum_square"")
            reduce_type = ReduceType::SUM_SQUARE;
        else if (op_type == ""l1"")
            reduce_type = ReduceType::L1;
        else if (op_type == ""l2"")
            reduce_type = ReduceType::L2;
        else if (op_type == ""log_sum"")
            reduce_type = ReduceType::LOG_SUM;
        else if (op_type == ""log_sum_exp"")
    ...",1,src\layers\reduce_layer.cpp,cv,9,cv,1
424683,NAMESPACE_BLOCK,"namespace dnn {

class ReduceLayerImpl CV_FINAL : public ReduceLayer
{
public:
    ReduceLayerImpl(const LayerParams& params) {
        setParamsFrom(params);

        // set reduce type
        CV_Assert(params.has(""reduce""));
        String op_type = toLowerCase(params.get<String>(""reduce""));
        if (op_type == ""max"")
            reduce_type = ReduceType::MAX;
        else if (op_type == ""min"")
            reduce_type = ReduceType::MIN;
        else if (op_type == ""mean"")
            reduce_type = ReduceType::MEAN;
        else if (op_type == ""sum"")
            reduce_type = ReduceType::SUM;
        else if (op_type == ""sum_square"")
            reduce_type = ReduceType::SUM_SQUARE;
        else if (op_type == ""l1"")
            reduce_type = ReduceType::L1;
        else if (op_type == ""l2"")
            reduce_type = ReduceType::L2;
        else if (op_type == ""log_sum"")
            reduce_type = ReduceType::LOG_SUM;
        else if (op_type == ""log_sum_exp"")
            reduce_...",16,src\layers\reduce_layer.cpp,cv.dnn,9,dnn,1
424717,NAMESPACE_BLOCK,<empty>,,src\layers\region_layer.cpp,src\layers\region_layer.cpp:<global>,,<global>,1
424721,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class RegionLayerImpl CV_FINAL : public RegionLayer
{
public:
    int coords, classes, anchors, classfix;
    float thresh, scale_x_y;
    int new_coords;
    bool useSoftmax, useLogistic;
#ifdef HAVE_OPENCL
    UMat blob_umat;
#endif

    RegionLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert(blobs.size() == 1);

        thresh = params.get<float>(""thresh"", 0.2);
        coords = params.get<int>(""coords"", 4);
        classes = params.get<int>(""classes"", 0);
        anchors = params.get<int>(""anchors"", 5);
        classfix = params.get<int>(""classfix"", 0);
        useSoftmax = params.get<bool>(""softmax"", false);
        useLogistic = params.get<bool>(""logistic"", false);
        nmsThreshold = params.get<float>(""nms_threshold"", 0.4);
        scale_x_y = params.get<float>(""scale_x_y"", 1.0); // Yolov4
        new_coords = params.get<int>(""new_coords"", 0); // Yolov4x-mish

        CV_Assert(nmsThreshold >= 0.);
 ...",1,src\layers\region_layer.cpp,cv,64,cv,1
424722,NAMESPACE_BLOCK,"namespace dnn
{

class RegionLayerImpl CV_FINAL : public RegionLayer
{
public:
    int coords, classes, anchors, classfix;
    float thresh, scale_x_y;
    int new_coords;
    bool useSoftmax, useLogistic;
#ifdef HAVE_OPENCL
    UMat blob_umat;
#endif

    RegionLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        CV_Assert(blobs.size() == 1);

        thresh = params.get<float>(""thresh"", 0.2);
        coords = params.get<int>(""coords"", 4);
        classes = params.get<int>(""classes"", 0);
        anchors = params.get<int>(""anchors"", 5);
        classfix = params.get<int>(""classfix"", 0);
        useSoftmax = params.get<bool>(""softmax"", false);
        useLogistic = params.get<bool>(""logistic"", false);
        nmsThreshold = params.get<float>(""nms_threshold"", 0.4);
        scale_x_y = params.get<float>(""scale_x_y"", 1.0); // Yolov4
        new_coords = params.get<int>(""new_coords"", 0); // Yolov4x-mish

        CV_Assert(nmsThreshold >= 0.);
        CV_Asser...",1,src\layers\region_layer.cpp,cv.dnn,66,dnn,1
424758,NAMESPACE_BLOCK,<empty>,,src\layers\reorg_layer.cpp,src\layers\reorg_layer.cpp:<global>,,<global>,1
424762,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class ReorgLayerImpl CV_FINAL : public ReorgLayer
{
    int reorgStride;
public:

    ReorgLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        reorgStride = params.get<int>(""reorg_stride"", 2);
        CV_Assert(reorgStride > 0);
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() > 0);
        outputs = std::vector<MatShape>(inputs.size(), shape(
            inputs[0][0],
            inputs[0][1] * reorgStride * reorgStride,
            inputs[0][2] / reorgStride,
            inputs[0][3] / reorgStride));

        CV_Assert(outputs[0][0] > 0 && outputs[0][1] > 0 && outputs[0][2] > 0 && outputs[0][3] > 0);
        CV_Assert(total(outputs[0]) == total(inputs[0]));

        return false;...",1,src\layers\reorg_layer.cpp,cv,68,cv,1
424763,NAMESPACE_BLOCK,"namespace dnn
{

class ReorgLayerImpl CV_FINAL : public ReorgLayer
{
    int reorgStride;
public:

    ReorgLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        reorgStride = params.get<int>(""reorg_stride"", 2);
        CV_Assert(reorgStride > 0);
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() > 0);
        outputs = std::vector<MatShape>(inputs.size(), shape(
            inputs[0][0],
            inputs[0][1] * reorgStride * reorgStride,
            inputs[0][2] / reorgStride,
            inputs[0][3] / reorgStride));

        CV_Assert(outputs[0][0] > 0 && outputs[0][1] > 0 && outputs[0][2] > 0 && outputs[0][3] > 0);
        CV_Assert(total(outputs[0]) == total(inputs[0]));

        return false;
    }

    vir...",1,src\layers\reorg_layer.cpp,cv.dnn,70,dnn,1
424799,NAMESPACE_BLOCK,<empty>,,src\layers\reshape_layer.cpp,src\layers\reshape_layer.cpp:<global>,,<global>,1
424803,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

static void computeShapeByReshapeMask(const MatShape &srcShape,
                                      const MatShape &maskShape,
                                      Range srcRange /*= Range::all()*/,
                                      MatShape& dstShape)
{
    int srcShapeSize = (int)srcShape.size();
    int maskShapeSize = (int)maskShape.size();

    srcRange = normalize_axis_range(srcRange, srcShapeSize);

    bool explicitMask = !maskShape.empty();  // All mask values are positive.
    for (int i = 0, n = maskShape.size(); i < n && explicitMask; ++i)
    {
        explicitMask = maskShape[i] > 0;
    }
    // Working range of source shape is a range where area(src) == area(mask).
    if (explicitMask)
    {
        int maskTotal = total(maskShape);
        // Go from the end of mask until we collect required total.
        bool matched = false;
        for (int i = srcRange.end - 1; i >= srcRange.start; --i)
        {
            if (matched)
...",1,src\layers\reshape_layer.cpp,cv,59,cv,1
424804,NAMESPACE_BLOCK,"namespace dnn
{

static void computeShapeByReshapeMask(const MatShape &srcShape,
                                      const MatShape &maskShape,
                                      Range srcRange /*= Range::all()*/,
                                      MatShape& dstShape)
{
    int srcShapeSize = (int)srcShape.size();
    int maskShapeSize = (int)maskShape.size();

    srcRange = normalize_axis_range(srcRange, srcShapeSize);

    bool explicitMask = !maskShape.empty();  // All mask values are positive.
    for (int i = 0, n = maskShape.size(); i < n && explicitMask; ++i)
    {
        explicitMask = maskShape[i] > 0;
    }
    // Working range of source shape is a range where area(src) == area(mask).
    if (explicitMask)
    {
        int maskTotal = total(maskShape);
        // Go from the end of mask until we collect required total.
        bool matched = false;
        for (int i = srcRange.end - 1; i >= srcRange.start; --i)
        {
            if (matched)
            {
 ...",1,src\layers\reshape_layer.cpp,cv.dnn,61,dnn,1
425267,NAMESPACE_BLOCK,<empty>,,src\layers\resize_layer.cpp,src\layers\resize_layer.cpp:<global>,,<global>,1
425271,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ResizeLayerImpl : public ResizeLayer
{
public:
    ResizeLayerImpl(const LayerParams& params) : zoomFactorWidth(params.get<float>(""zoom_factor_x"", params.get<float>(""zoom_factor"", 0))),
                                                 zoomFactorHeight(params.get<float>(""zoom_factor_y"", params.get<float>(""zoom_factor"", 0))),
                                                 scaleWidth(0), scaleHeight(0)
    {
        setParamsFrom(params);
        outWidth = params.get<float>(""width"", 0);
        outHeight = params.get<float>(""height"", 0);
        if (params.has(""zoom_factor""))
        {
            CV_Assert(!params.has(""zoom_factor_x"") && !params.has(""zoom_factor_y""));
        }
        else if (params.has(""zoom_factor_x"") || params.has(""zoom_factor_y""))
        {
            CV_Assert(params.has(""zoom_factor_x"") && params.has(""zoom_factor_y""));
        }
        interpolation = params.get<String>(""interpolation"");
        CV_Check(interpolation...",1,src\layers\resize_layer.cpp,cv,28,cv,1
425272,NAMESPACE_BLOCK,"namespace dnn {

class ResizeLayerImpl : public ResizeLayer
{
public:
    ResizeLayerImpl(const LayerParams& params) : zoomFactorWidth(params.get<float>(""zoom_factor_x"", params.get<float>(""zoom_factor"", 0))),
                                                 zoomFactorHeight(params.get<float>(""zoom_factor_y"", params.get<float>(""zoom_factor"", 0))),
                                                 scaleWidth(0), scaleHeight(0)
    {
        setParamsFrom(params);
        outWidth = params.get<float>(""width"", 0);
        outHeight = params.get<float>(""height"", 0);
        if (params.has(""zoom_factor""))
        {
            CV_Assert(!params.has(""zoom_factor_x"") && !params.has(""zoom_factor_y""));
        }
        else if (params.has(""zoom_factor_x"") || params.has(""zoom_factor_y""))
        {
            CV_Assert(params.has(""zoom_factor_x"") && params.has(""zoom_factor_y""));
        }
        interpolation = params.get<String>(""interpolation"");
        CV_Check(interpolation, interpolation...",16,src\layers\resize_layer.cpp,cv.dnn,28,dnn,1
425507,NAMESPACE_BLOCK,<empty>,,src\layers\scale_layer.cpp,src\layers\scale_layer.cpp:<global>,,<global>,1
425511,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class ScaleLayerImpl CV_FINAL : public ScaleLayer
{
public:
#ifdef HAVE_WEBNN
    mutable int dims;
    mutable int numChannels;
#endif
    ScaleLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        hasBias = params.get<bool>(""bias_term"", false);
        axis = params.get<int>(""axis"", 1);
        hasWeights = false;
        mode = params.get<String>(""mode"", ""scale"");
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        outputs.assign(1, inputs[0]);
#ifdef HAVE_WEBNN
        dims = inputs[0].size();
        numChannels = 1;
        if (inputs.size() > 1)
        {
            for (const size_t& dim : inputs[1])
                numChannels *= dim;
        }
#endif
        return true;
    }

    virtual void final...",1,src\layers\scale_layer.cpp,cv,28,cv,1
425512,NAMESPACE_BLOCK,"namespace dnn
{

class ScaleLayerImpl CV_FINAL : public ScaleLayer
{
public:
#ifdef HAVE_WEBNN
    mutable int dims;
    mutable int numChannels;
#endif
    ScaleLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        hasBias = params.get<bool>(""bias_term"", false);
        axis = params.get<int>(""axis"", 1);
        hasWeights = false;
        mode = params.get<String>(""mode"", ""scale"");
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        outputs.assign(1, inputs[0]);
#ifdef HAVE_WEBNN
        dims = inputs[0].size();
        numChannels = 1;
        if (inputs.size() > 1)
        {
            for (const size_t& dim : inputs[1])
                numChannels *= dim;
        }
#endif
        return true;
    }

    virtual void finalize(InputArrayO...",1,src\layers\scale_layer.cpp,cv.dnn,30,dnn,1
425650,NAMESPACE_BLOCK,<empty>,,src\layers\scatterND_layer.cpp,src\layers\scatterND_layer.cpp:<global>,,<global>,1
425654,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ScatterNDLayerImpl CV_FINAL : public ScatterNDLayer
{
public:
    enum class REDUCTION
    {
        NONE = 1,
        ADD,
        MUL,
        MAX,
        MIN
    } reduction;

    ScatterNDLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        String reduction_name = toLowerCase(params.get<String>(""reduction"", ""none""));
        if (reduction_name == ""none"")
            reduction = REDUCTION::NONE;
        else if (reduction_name == ""add"")
            reduction = REDUCTION::ADD;
        else if (reduction_name == ""mul"")
            reduction = REDUCTION::MUL;
        else if (reduction_name == ""max"")
            reduction = REDUCTION::MAX;
        else if (reduction_name == ""min"")
            reduction = REDUCTION::MIN;
        else
            CV_Error(cv::Error::StsBadArg, ""Unkown reduction \"""" + reduction_name + ""\"""");
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backend...",1,src\layers\scatterND_layer.cpp,cv,10,cv,1
425655,NAMESPACE_BLOCK,"namespace dnn {

class ScatterNDLayerImpl CV_FINAL : public ScatterNDLayer
{
public:
    enum class REDUCTION
    {
        NONE = 1,
        ADD,
        MUL,
        MAX,
        MIN
    } reduction;

    ScatterNDLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        String reduction_name = toLowerCase(params.get<String>(""reduction"", ""none""));
        if (reduction_name == ""none"")
            reduction = REDUCTION::NONE;
        else if (reduction_name == ""add"")
            reduction = REDUCTION::ADD;
        else if (reduction_name == ""mul"")
            reduction = REDUCTION::MUL;
        else if (reduction_name == ""max"")
            reduction = REDUCTION::MAX;
        else if (reduction_name == ""min"")
            reduction = REDUCTION::MIN;
        else
            CV_Error(cv::Error::StsBadArg, ""Unkown reduction \"""" + reduction_name + ""\"""");
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKE...",16,src\layers\scatterND_layer.cpp,cv.dnn,10,dnn,1
425675,NAMESPACE_BLOCK,<empty>,,src\layers\scatter_layer.cpp,src\layers\scatter_layer.cpp:<global>,,<global>,1
425679,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ScatterLayerImpl CV_FINAL : public ScatterLayer
{
public:
    enum class REDUCTION
    {
        NONE = 1,
        ADD,
        MUL,
        MAX,
        MIN
    } reduction;

    ScatterLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        axis = params.get<int>(""axis"", 0);
        String reduction_name = toLowerCase(params.get<String>(""reduction"", ""none""));
        if (reduction_name == ""none"")
            reduction = REDUCTION::NONE;
        else if (reduction_name == ""add"")
            reduction = REDUCTION::ADD;
        else if (reduction_name == ""mul"")
            reduction = REDUCTION::MUL;
        else if (reduction_name == ""max"")
            reduction = REDUCTION::MAX;
        else if (reduction_name == ""min"")
            reduction = REDUCTION::MIN;
        else
            CV_Error(cv::Error::StsBadArg, ""Unkown reduction \"""" + reduction_name + ""\"""");
    }

    virtual bool supportBackend(int backendId) CV_...",1,src\layers\scatter_layer.cpp,cv,10,cv,1
425680,NAMESPACE_BLOCK,"namespace dnn {

class ScatterLayerImpl CV_FINAL : public ScatterLayer
{
public:
    enum class REDUCTION
    {
        NONE = 1,
        ADD,
        MUL,
        MAX,
        MIN
    } reduction;

    ScatterLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);

        axis = params.get<int>(""axis"", 0);
        String reduction_name = toLowerCase(params.get<String>(""reduction"", ""none""));
        if (reduction_name == ""none"")
            reduction = REDUCTION::NONE;
        else if (reduction_name == ""add"")
            reduction = REDUCTION::ADD;
        else if (reduction_name == ""mul"")
            reduction = REDUCTION::MUL;
        else if (reduction_name == ""max"")
            reduction = REDUCTION::MAX;
        else if (reduction_name == ""min"")
            reduction = REDUCTION::MIN;
        else
            CV_Error(cv::Error::StsBadArg, ""Unkown reduction \"""" + reduction_name + ""\"""");
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
...",16,src\layers\scatter_layer.cpp,cv.dnn,10,dnn,1
425700,NAMESPACE_BLOCK,<empty>,,src\layers\shuffle_channel_layer.cpp,src\layers\shuffle_channel_layer.cpp:<global>,,<global>,1
425704,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class ShuffleChannelLayerImpl CV_FINAL : public ShuffleChannelLayer
{
public:
    ShuffleChannelLayerImpl(const LayerParams& params)
    {
        group = params.get<int>(""group"", 1);
        setParamsFrom(params);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() == 1 && inputs[0].size() == 4);
        CV_Assert(inputs[0][1] % group == 0);
        Layer::getMemoryShapes(inputs, requiredOutputs, outputs, internals);
        return group == 1;
    }

    virtual void finalize(InputArrayOfArrays inputs_arr, OutputArrayOfArrays outputs_arr) CV_OVERRID...",1,src\layers\shuffle_channel_layer.cpp,cv,15,cv,1
425705,NAMESPACE_BLOCK,"namespace dnn {

class ShuffleChannelLayerImpl CV_FINAL : public ShuffleChannelLayer
{
public:
    ShuffleChannelLayerImpl(const LayerParams& params)
    {
        group = params.get<int>(""group"", 1);
        setParamsFrom(params);
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() == 1 && inputs[0].size() == 4);
        CV_Assert(inputs[0][1] % group == 0);
        Layer::getMemoryShapes(inputs, requiredOutputs, outputs, internals);
        return group == 1;
    }

    virtual void finalize(InputArrayOfArrays inputs_arr, OutputArrayOfArrays outputs_arr) CV_OVERRIDE
    {
       ...",16,src\layers\shuffle_channel_layer.cpp,cv.dnn,15,dnn,1
425741,NAMESPACE_BLOCK,<empty>,,src\layers\slice_layer.cpp,src\layers\slice_layer.cpp:<global>,,<global>,1
425745,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

Range normalizeRange(const Range& input_range, int n)
{
    Range range = input_range;

    range.start = std::min(std::max(range.start, -n), n - 1);
    if (range.start < 0)
    {
        range.start += n;
    }

    range.end = std::min(std::max(range.end, -n), n);
    if (range.end < 0)
    {
        range.end += n;
    }

    return range;
}

// TODO: support cv::Range with steps and negative steps to get rid of this transformation
void tranformForNegSteps(const MatShape& inpShape, std::vector<std::vector<Range> >& sliceRanges, std::vector<std::vector<int> >& sliceSteps)
{
    // in case of negative steps,
    // x of shape [5, 10], x[5:0:-1, 10:1:-3] <=> np.flip(x[1:5:1, 2:10:3], aixs=(0, 1))
    // new_end_i = start_i + 1 > dim_i ? dim_i : start_i + 1
    // new_start_i = end + 1
    // new_start_i = new_end_i - 1 - ((new_end_i - 1 - new_start_i) / abs(step_i)) * abs(step_i)
    int start, end, new_start, new_end, step;
    for (int i = 0; i < s...",1,src\layers\slice_layer.cpp,cv,63,cv,1
425746,NAMESPACE_BLOCK,"namespace dnn
{

Range normalizeRange(const Range& input_range, int n)
{
    Range range = input_range;

    range.start = std::min(std::max(range.start, -n), n - 1);
    if (range.start < 0)
    {
        range.start += n;
    }

    range.end = std::min(std::max(range.end, -n), n);
    if (range.end < 0)
    {
        range.end += n;
    }

    return range;
}

// TODO: support cv::Range with steps and negative steps to get rid of this transformation
void tranformForNegSteps(const MatShape& inpShape, std::vector<std::vector<Range> >& sliceRanges, std::vector<std::vector<int> >& sliceSteps)
{
    // in case of negative steps,
    // x of shape [5, 10], x[5:0:-1, 10:1:-3] <=> np.flip(x[1:5:1, 2:10:3], aixs=(0, 1))
    // new_end_i = start_i + 1 > dim_i ? dim_i : start_i + 1
    // new_start_i = end + 1
    // new_start_i = new_end_i - 1 - ((new_end_i - 1 - new_start_i) / abs(step_i)) * abs(step_i)
    int start, end, new_start, new_end, step;
    for (int i = 0; i < sliceSteps[0].si...",1,src\layers\slice_layer.cpp,cv.dnn,65,dnn,1
426788,NAMESPACE_BLOCK,<empty>,,src\layers\softmax_layer.cpp,src\layers\softmax_layer.cpp:<global>,,<global>,1
426792,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class SoftMaxLayerImpl CV_FINAL : public SoftmaxLayer
{
public:

    SoftMaxLayerImpl(const LayerParams& params)
    {
        axisRaw = params.get<int>(""axis"", -1);
        logSoftMax = params.get<bool>(""log_softmax"", false);
        setParamsFrom(params);
    }

#ifdef HAVE_OPENCL
    Ptr<OCL4DNNSoftmax<float> > softmaxOp;
#endif

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        bool inplace = Layer::getMemoryShapes(inputs, requiredOutputs, outputs, internals);
        MatShape shape = inputs[0];
        int cAxis = normalize_axis(axisRaw, shape.size());
        shape[cAxis] = 1;
        internals.assign(1, shape);
        return inplace;
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        ...",1,src\layers\softmax_layer.cpp,cv,68,cv,1
426793,NAMESPACE_BLOCK,"namespace dnn
{

class SoftMaxLayerImpl CV_FINAL : public SoftmaxLayer
{
public:

    SoftMaxLayerImpl(const LayerParams& params)
    {
        axisRaw = params.get<int>(""axis"", -1);
        logSoftMax = params.get<bool>(""log_softmax"", false);
        setParamsFrom(params);
    }

#ifdef HAVE_OPENCL
    Ptr<OCL4DNNSoftmax<float> > softmaxOp;
#endif

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        bool inplace = Layer::getMemoryShapes(inputs, requiredOutputs, outputs, internals);
        MatShape shape = inputs[0];
        int cAxis = normalize_axis(axisRaw, shape.size());
        shape[cAxis] = 1;
        internals.assign(1, shape);
        return inplace;
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
#ifdef HAVE_INF_ENGINE
        if (backendId =...",1,src\layers\softmax_layer.cpp,cv.dnn,70,dnn,1
426817,NAMESPACE_BLOCK,<empty>,,src\layers\split_layer.cpp,src\layers\split_layer.cpp:<global>,,<global>,1
426821,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{

class SplitLayerImpl CV_FINAL : public SplitLayer
{
public:
    SplitLayerImpl(const LayerParams &params)
    {
        setParamsFrom(params);
        //TODO: maybe ""top_count"" param is useless because it can be determined by output connections number
        if (params.has(""top_count""))
        {
            outputsCount = params.get<int>(""top_count"");
            CV_Assert(outputsCount >= 0);
        }
        else
        {
            outputsCount = -1;
        }
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() == 1);

        Layer::getMemory...",1,src\layers\split_layer.cpp,cv,52,cv,1
426822,NAMESPACE_BLOCK,"namespace dnn
{

class SplitLayerImpl CV_FINAL : public SplitLayer
{
public:
    SplitLayerImpl(const LayerParams &params)
    {
        setParamsFrom(params);
        //TODO: maybe ""top_count"" param is useless because it can be determined by output connections number
        if (params.has(""top_count""))
        {
            outputsCount = params.get<int>(""top_count"");
            CV_Assert(outputsCount >= 0);
        }
        else
        {
            outputsCount = -1;
        }
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV ||
               backendId == DNN_BACKEND_CUDA;
    }

    bool getMemoryShapes(const std::vector<MatShape> &inputs,
                         const int requiredOutputs,
                         std::vector<MatShape> &outputs,
                         std::vector<MatShape> &internals) const CV_OVERRIDE
    {
        CV_Assert(inputs.size() == 1);

        Layer::getMemoryShapes(inputs, ...",1,src\layers\split_layer.cpp,cv.dnn,54,dnn,1
426844,NAMESPACE_BLOCK,<empty>,,src\layers\tile_layer.cpp,src\layers\tile_layer.cpp:<global>,,<global>,1
426848,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

class TileLayerImpl CV_FINAL : public TileLayer
{
public:
    TileLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        if (params.has(""repeats""))
        {
            DictValue param_repeats = params.get(""repeats"");
            int n_repeats = param_repeats.size();

            CV_Assert(n_repeats > 0);
            repeats.resize(n_repeats);
            for (int i = 0; i < n_repeats; i++)
                repeats[i] = param_repeats.get<int>(i);
        }
        else
            CV_Error(Error::StsNotImplemented, ""Tile: repeats needs to be treated as parameter but it is missing."");
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                         ...",1,src\layers\tile_layer.cpp,cv,10,cv,1
426849,NAMESPACE_BLOCK,"namespace dnn {

class TileLayerImpl CV_FINAL : public TileLayer
{
public:
    TileLayerImpl(const LayerParams& params)
    {
        setParamsFrom(params);
        if (params.has(""repeats""))
        {
            DictValue param_repeats = params.get(""repeats"");
            int n_repeats = param_repeats.size();

            CV_Assert(n_repeats > 0);
            repeats.resize(n_repeats);
            for (int i = 0; i < n_repeats; i++)
                repeats[i] = param_repeats.get<int>(i);
        }
        else
            CV_Error(Error::StsNotImplemented, ""Tile: repeats needs to be treated as parameter but it is missing."");
    }

    virtual bool supportBackend(int backendId) CV_OVERRIDE
    {
        return backendId == DNN_BACKEND_OPENCV;
    }

    virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                 const int requiredOutputs,
                                 std::vector<MatShape> &outputs,
                                 std::ve...",16,src\layers\tile_layer.cpp,cv.dnn,10,dnn,1
426883,NAMESPACE_BLOCK,<empty>,,src\legacy_backend.cpp,src\legacy_backend.cpp:<global>,,<global>,1
426887,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


BackendNode::BackendNode(int backendId)
    : backendId(backendId)
{}

BackendNode::~BackendNode() {}

BackendWrapper::BackendWrapper(int backendId, int targetId)
    : backendId(backendId)
    , targetId(targetId)
{}

BackendWrapper::BackendWrapper(int targetId, const cv::Mat& m)
{
    CV_Error(Error::StsNotImplemented,
            ""Constructor of backend wrapper must be implemented"");
}

BackendWrapper::BackendWrapper(const Ptr<BackendWrapper>& base, const MatShape& shape)
{
    CV_Error(Error::StsNotImplemented,
            ""Constructor of backend wrapper must be implemented"");
}

BackendWrapper::~BackendWrapper() {}



inline namespace detail {


Ptr<BackendWrapper> wrapMat(int backendId, int targetId, cv::Mat& m)
{
    if (backendId == DNN_BACKEND_OPENCV)
    {
        if (targetId == DNN_TARGET_CPU)
            return Ptr<BackendWrapper>();
#ifdef HAVE_OPENCL
        else if (IS_DNN_OPENCL_TARGET(targetId))
            r...",1,src\legacy_backend.cpp,cv,18,cv,1
426888,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


BackendNode::BackendNode(int backendId)
    : backendId(backendId)
{}

BackendNode::~BackendNode() {}

BackendWrapper::BackendWrapper(int backendId, int targetId)
    : backendId(backendId)
    , targetId(targetId)
{}

BackendWrapper::BackendWrapper(int targetId, const cv::Mat& m)
{
    CV_Error(Error::StsNotImplemented,
            ""Constructor of backend wrapper must be implemented"");
}

BackendWrapper::BackendWrapper(const Ptr<BackendWrapper>& base, const MatShape& shape)
{
    CV_Error(Error::StsNotImplemented,
            ""Constructor of backend wrapper must be implemented"");
}

BackendWrapper::~BackendWrapper() {}



inline namespace detail {


Ptr<BackendWrapper> wrapMat(int backendId, int targetId, cv::Mat& m)
{
    if (backendId == DNN_BACKEND_OPENCV)
    {
        if (targetId == DNN_TARGET_CPU)
            return Ptr<BackendWrapper>();
#ifdef HAVE_OPENCL
        else if (IS_DNN_OPENCL_TARGET(targetId))
            return OpenCLBac...",1,src\legacy_backend.cpp,cv.dnn,19,dnn,1
426893,NAMESPACE_BLOCK,<empty>,,src\legacy_backend.hpp,src\legacy_backend.hpp:<global>,,<global>,1
426897,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN
inline namespace detail {


#ifdef HAVE_OPENCL
class OpenCLBackendWrapper : public BackendWrapper
{
public:
    OpenCLBackendWrapper(Mat& m)
        : BackendWrapper(DNN_BACKEND_OPENCV, DNN_TARGET_OPENCL)
    {
        m.copyTo(umat);
        host = &m;
        hostDirty = false;
    }

    OpenCLBackendWrapper(const Ptr<BackendWrapper>& baseBuffer, Mat& m)
        : BackendWrapper(DNN_BACKEND_OPENCV, DNN_TARGET_OPENCL)
    {
        Ptr<OpenCLBackendWrapper> base = baseBuffer.dynamicCast<OpenCLBackendWrapper>();
        CV_Assert(!base.empty());

        host = &m;

        int shape[] = { 1, (int)base->umat.total() };
        umat = base->umat.reshape(1, 2, &shape[0])
                       .colRange(0, host->total())
                       .reshape(1, host->dims, &host->size[0]);
        hostDirty = false;
    }

    static Ptr<BackendWrapper> create(Mat& m)
    {
        return Ptr<BackendWrapper>(new OpenCLBackendWrapper(m)...",1,src\legacy_backend.hpp,cv,10,cv,1
426898,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN
inline namespace detail {


#ifdef HAVE_OPENCL
class OpenCLBackendWrapper : public BackendWrapper
{
public:
    OpenCLBackendWrapper(Mat& m)
        : BackendWrapper(DNN_BACKEND_OPENCV, DNN_TARGET_OPENCL)
    {
        m.copyTo(umat);
        host = &m;
        hostDirty = false;
    }

    OpenCLBackendWrapper(const Ptr<BackendWrapper>& baseBuffer, Mat& m)
        : BackendWrapper(DNN_BACKEND_OPENCV, DNN_TARGET_OPENCL)
    {
        Ptr<OpenCLBackendWrapper> base = baseBuffer.dynamicCast<OpenCLBackendWrapper>();
        CV_Assert(!base.empty());

        host = &m;

        int shape[] = { 1, (int)base->umat.total() };
        umat = base->umat.reshape(1, 2, &shape[0])
                       .colRange(0, host->total())
                       .reshape(1, host->dims, &host->size[0]);
        hostDirty = false;
    }

    static Ptr<BackendWrapper> create(Mat& m)
    {
        return Ptr<BackendWrapper>(new OpenCLBackendWrapper(m));
    }

    s...",16,src\legacy_backend.hpp,cv.dnn,10,dnn,1
426900,NAMESPACE_BLOCK,"inline namespace detail {


#ifdef HAVE_OPENCL
class OpenCLBackendWrapper : public BackendWrapper
{
public:
    OpenCLBackendWrapper(Mat& m)
        : BackendWrapper(DNN_BACKEND_OPENCV, DNN_TARGET_OPENCL)
    {
        m.copyTo(umat);
        host = &m;
        hostDirty = false;
    }

    OpenCLBackendWrapper(const Ptr<BackendWrapper>& baseBuffer, Mat& m)
        : BackendWrapper(DNN_BACKEND_OPENCV, DNN_TARGET_OPENCL)
    {
        Ptr<OpenCLBackendWrapper> base = baseBuffer.dynamicCast<OpenCLBackendWrapper>();
        CV_Assert(!base.empty());

        host = &m;

        int shape[] = { 1, (int)base->umat.total() };
        umat = base->umat.reshape(1, 2, &shape[0])
                       .colRange(0, host->total())
                       .reshape(1, host->dims, &host->size[0]);
        hostDirty = false;
    }

    static Ptr<BackendWrapper> create(Mat& m)
    {
        return Ptr<BackendWrapper>(new OpenCLBackendWrapper(m));
    }

    static Ptr<BackendWrapper> create(const P...",1,src\legacy_backend.hpp,cv.dnn.detail,12,detail,2
427708,NAMESPACE_BLOCK,<empty>,,src\math_utils.hpp,src\math_utils.hpp:<global>,,<global>,1
427712,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

const float kNegativeInfinity = -std::numeric_limits<float>::infinity();

const float kMinLogDiffFloat = std::log(FLT_EPSILON);

#if !defined(_MSC_VER) || (_MSC_VER >= 1700)
inline float Log1p(float x) {  return log1pf(x); }
#else
inline float Log1p(float x) {
  const float cutoff = 1.0e-07;
  if (x < cutoff)
    return x - 2 * x * x;
  else
    return Log(1.0 + x);
}
#endif

inline float Exp(float x) { return expf(x); }

inline float LogAdd(float x, float y) {
  float diff;
  if (x < y) {
    diff = x - y;
    x = y;
  } else {
    diff = y - x;
  }
  // diff is negative.  x is now the larger one.

  if (diff >= kMinLogDiffFloat) {
    float res;
    res = x + Log1p(Exp(diff));
    return res;
  } else {
    return x;  // return the larger one.
  }
}

}}",1,src\math_utils.hpp,cv,42,cv,1
427713,NAMESPACE_BLOCK,"namespace dnn {

const float kNegativeInfinity = -std::numeric_limits<float>::infinity();

const float kMinLogDiffFloat = std::log(FLT_EPSILON);

#if !defined(_MSC_VER) || (_MSC_VER >= 1700)
inline float Log1p(float x) {  return log1pf(x); }
#else
inline float Log1p(float x) {
  const float cutoff = 1.0e-07;
  if (x < cutoff)
    return x - 2 * x * x;
  else
    return Log(1.0 + x);
}
#endif

inline float Exp(float x) { return expf(x); }

inline float LogAdd(float x, float y) {
  float diff;
  if (x < y) {
    diff = x - y;
    x = y;
  } else {
    diff = y - x;
  }
  // diff is negative.  x is now the larger one.

  if (diff >= kMinLogDiffFloat) {
    float res;
    res = x + Log1p(Exp(diff));
    return res;
  } else {
    return x;  // return the larger one.
  }
}

}",16,src\math_utils.hpp,cv.dnn,42,dnn,1
427811,NAMESPACE_BLOCK,<empty>,,src\model.cpp,src\model.cpp:<global>,,<global>,1
427815,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {

struct Model::Impl
{
//protected:
    Net    net;

    Size   size;
    Scalar mean;
    Scalar scale = Scalar::all(1.0);
    bool   swapRB = false;
    bool   crop = false;
    Mat    blob;
    std::vector<String> outNames;

public:
    virtual ~Impl() {}
    Impl() {}
    Impl(const Impl&) = delete;
    Impl(Impl&&) = delete;

    virtual Net& getNetwork() const { return const_cast<Net&>(net); }

    virtual void setPreferableBackend(Backend backendId) { net.setPreferableBackend(backendId); }
    virtual void setPreferableTarget(Target targetId) { net.setPreferableTarget(targetId); }
    virtual void enableWinograd(bool useWinograd) { net.enableWinograd(useWinograd); }

    virtual
    void initNet(const Net& network)
    {
        CV_TRACE_FUNCTION();
        net = network;

        outNames = net.getUnconnectedOutLayersNames();
        std::vector<MatShape> inLayerShapes;
        std::vector<MatShape> outLayerShapes;
        net.getLayerShapes(Mat...",1,src\model.cpp,cv,14,cv,1
427816,NAMESPACE_BLOCK,"namespace dnn {

struct Model::Impl
{
//protected:
    Net    net;

    Size   size;
    Scalar mean;
    Scalar scale = Scalar::all(1.0);
    bool   swapRB = false;
    bool   crop = false;
    Mat    blob;
    std::vector<String> outNames;

public:
    virtual ~Impl() {}
    Impl() {}
    Impl(const Impl&) = delete;
    Impl(Impl&&) = delete;

    virtual Net& getNetwork() const { return const_cast<Net&>(net); }

    virtual void setPreferableBackend(Backend backendId) { net.setPreferableBackend(backendId); }
    virtual void setPreferableTarget(Target targetId) { net.setPreferableTarget(targetId); }
    virtual void enableWinograd(bool useWinograd) { net.enableWinograd(useWinograd); }

    virtual
    void initNet(const Net& network)
    {
        CV_TRACE_FUNCTION();
        net = network;

        outNames = net.getUnconnectedOutLayersNames();
        std::vector<MatShape> inLayerShapes;
        std::vector<MatShape> outLayerShapes;
        net.getLayerShapes(MatShape(), 0, inL...",1,src\model.cpp,cv.dnn,15,dnn,1
431277,NAMESPACE_BLOCK,<empty>,,src\net.cpp,src\net.cpp:<global>,,<global>,1
431281,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

Net::Net()
    : impl(makePtr<Net::Impl>())
{
}

Net::~Net()
{
}

int Net::addLayer(const String& name, const String& type, const int& dtype, LayerParams& params)
{
    CV_TRACE_FUNCTION();
    CV_Assert(impl);
    return impl->addLayer(name, type, dtype, params);
}

int Net::addLayer(const String& name, const String& type, LayerParams& params)
{
    CV_TRACE_FUNCTION();
    return addLayer(name, type, CV_32F, params);
}

int Net::addLayerToPrev(const String& name, const String& type, const int& dtype, LayerParams& params)
{
    CV_TRACE_FUNCTION();
    CV_Assert(impl);
    return impl->addLayerToPrev(name, type, dtype, params);
}

int Net::addLayerToPrev(const String& name, const String& type, LayerParams& params)
{
    CV_TRACE_FUNCTION();
    return addLayerToPrev(name, type, CV_32F, params);
}

void Net::connect(int outLayerId, int outNum, int inpLayerId, int inpNum)
{
    CV_TRACE_FUNCTION();
    CV_Assert(impl);
    impl-...",1,src\net.cpp,cv,9,cv,1
431282,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

Net::Net()
    : impl(makePtr<Net::Impl>())
{
}

Net::~Net()
{
}

int Net::addLayer(const String& name, const String& type, const int& dtype, LayerParams& params)
{
    CV_TRACE_FUNCTION();
    CV_Assert(impl);
    return impl->addLayer(name, type, dtype, params);
}

int Net::addLayer(const String& name, const String& type, LayerParams& params)
{
    CV_TRACE_FUNCTION();
    return addLayer(name, type, CV_32F, params);
}

int Net::addLayerToPrev(const String& name, const String& type, const int& dtype, LayerParams& params)
{
    CV_TRACE_FUNCTION();
    CV_Assert(impl);
    return impl->addLayerToPrev(name, type, dtype, params);
}

int Net::addLayerToPrev(const String& name, const String& type, LayerParams& params)
{
    CV_TRACE_FUNCTION();
    return addLayerToPrev(name, type, CV_32F, params);
}

void Net::connect(int outLayerId, int outNum, int inpLayerId, int inpNum)
{
    CV_TRACE_FUNCTION();
    CV_Assert(impl);
    impl->connect(outLay...",1,src\net.cpp,cv.dnn,10,dnn,1
431291,NAMESPACE_BLOCK,<empty>,,src\net_cann.cpp,src\net_cann.cpp:<global>,,<global>,1
431295,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_CANN

static std::shared_ptr<ge::ModelBufferData> compileCannGraph(std::shared_ptr<ge::Graph> graph);

class NetImplCann CV_FINAL : public Net::Impl
{
public:
    typedef Net::Impl Base;

    bool newWasSupported, netWasConverted;

    explicit NetImplCann(const Ptr<Net::Impl>& basePtr)
        : Net::Impl()
    {
        CV_LOG_INFO(NULL, ""Initializing NetImplCann"");
        basePtr_ = basePtr;
        newWasSupported = true;
        netWasConverted = false;

        init();

        CV_LOG_INFO(NULL, ""Finished initializing NetImplCann"");
    }

    void init()
    {
        CV_TRACE_FUNCTION();
        CV_Assert(basePtr_);
        Net::Impl& base = *basePtr_;
        CV_Assert(!base.netWasAllocated);
        CV_Assert(!base.netWasQuantized); // does not support quantized net for now
        netInputLayer = base.netInputLayer;
        blobsToKeep = base.blobsToKeep;
        layers = base.layers;
        for (MapIdT...",1,src\net_cann.cpp,cv,11,cv,1
431296,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_CANN

static std::shared_ptr<ge::ModelBufferData> compileCannGraph(std::shared_ptr<ge::Graph> graph);

class NetImplCann CV_FINAL : public Net::Impl
{
public:
    typedef Net::Impl Base;

    bool newWasSupported, netWasConverted;

    explicit NetImplCann(const Ptr<Net::Impl>& basePtr)
        : Net::Impl()
    {
        CV_LOG_INFO(NULL, ""Initializing NetImplCann"");
        basePtr_ = basePtr;
        newWasSupported = true;
        netWasConverted = false;

        init();

        CV_LOG_INFO(NULL, ""Finished initializing NetImplCann"");
    }

    void init()
    {
        CV_TRACE_FUNCTION();
        CV_Assert(basePtr_);
        Net::Impl& base = *basePtr_;
        CV_Assert(!base.netWasAllocated);
        CV_Assert(!base.netWasQuantized); // does not support quantized net for now
        netInputLayer = base.netInputLayer;
        blobsToKeep = base.blobsToKeep;
        layers = base.layers;
        for (MapIdToLayerData::ite...",16,src\net_cann.cpp,cv.dnn,11,dnn,1
431303,NAMESPACE_BLOCK,<empty>,,src\net_impl.cpp,src\net_impl.cpp:<global>,,<global>,1
431307,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


static int g_networkId = 0;


detail::NetImplBase::NetImplBase()
    : networkId(CV_XADD(&g_networkId, 1))
    , networkDumpCounter(0)
    , dumpLevel(getParam_DNN_NETWORK_DUMP())
{
    // nothing
}


std::string detail::NetImplBase::getDumpFileNameBase() const
{
    std::string dumpFileNameBase = cv::format(""ocv_dnn_net_%05d_%02d"", networkId, networkDumpCounter++);
    return dumpFileNameBase;
}


Net::Impl::~Impl()
{
#ifdef HAVE_VULKAN
    if (context)
        context->reset();
#endif
}


Net::Impl::Impl()
{
    // allocate fake net input layer
    netInputLayer = Ptr<DataLayer>(new DataLayer());
    LayerData& inpl = layers.insert(make_pair(0, LayerData())).first->second;
    inpl.id = 0;
    netInputLayer->name = inpl.name = ""_input"";
    inpl.type = ""__NetInputLayer__"";
    inpl.layerInstance = netInputLayer;
    layerNameToId.insert(std::make_pair(inpl.name, inpl.id));

    lastLayerId = 0;
    netWasAllocated = false;
 ...",1,src\net_impl.cpp,cv,9,cv,1
431308,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


static int g_networkId = 0;


detail::NetImplBase::NetImplBase()
    : networkId(CV_XADD(&g_networkId, 1))
    , networkDumpCounter(0)
    , dumpLevel(getParam_DNN_NETWORK_DUMP())
{
    // nothing
}


std::string detail::NetImplBase::getDumpFileNameBase() const
{
    std::string dumpFileNameBase = cv::format(""ocv_dnn_net_%05d_%02d"", networkId, networkDumpCounter++);
    return dumpFileNameBase;
}


Net::Impl::~Impl()
{
#ifdef HAVE_VULKAN
    if (context)
        context->reset();
#endif
}


Net::Impl::Impl()
{
    // allocate fake net input layer
    netInputLayer = Ptr<DataLayer>(new DataLayer());
    LayerData& inpl = layers.insert(make_pair(0, LayerData())).first->second;
    inpl.id = 0;
    netInputLayer->name = inpl.name = ""_input"";
    inpl.type = ""__NetInputLayer__"";
    inpl.layerInstance = netInputLayer;
    layerNameToId.insert(std::make_pair(inpl.name, inpl.id));

    lastLayerId = 0;
    netWasAllocated = false;
    netWasQuanti...",1,src\net_impl.cpp,cv.dnn,10,dnn,1
431341,NAMESPACE_BLOCK,<empty>,,src\net_impl.hpp,src\net_impl.hpp:<global>,,<global>,1
431345,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

using std::make_pair;
using std::string;

// NB: Implementation is divided between of multiple .cpp files
struct Net::Impl : public detail::NetImplBase
{
    typedef std::map<int, LayerShapes> LayersShapesMap;
    typedef std::map<int, LayerData> MapIdToLayerData;

    virtual ~Impl();
    Impl();
    Impl(const Impl&) = delete;

    // Inheritance support
    Ptr<Net::Impl> basePtr_;

    Ptr<DataLayer> netInputLayer;
    std::vector<LayerPin> blobsToKeep;
    MapIdToLayerData layers;
    std::map<String, int> layerNameToId;
    std::map<std::string, int> outputNameToId;  // use registerOutput() to populate outputs
    BlobManager blobManager;
    int preferableBackend;
    int preferableTarget;
    String halideConfigFile;
    bool hasDynamicShapes;
    // Map host data to backend specific wrapper.
    std::map<void*, Ptr<BackendWrapper>> backendWrappers;

    int lastLayerId;

    bool netWasAllocated;
    bool netWasQuantiz...",1,src\net_impl.hpp,cv,29,cv,1
431346,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

using std::make_pair;
using std::string;

// NB: Implementation is divided between of multiple .cpp files
struct Net::Impl : public detail::NetImplBase
{
    typedef std::map<int, LayerShapes> LayersShapesMap;
    typedef std::map<int, LayerData> MapIdToLayerData;

    virtual ~Impl();
    Impl();
    Impl(const Impl&) = delete;

    // Inheritance support
    Ptr<Net::Impl> basePtr_;

    Ptr<DataLayer> netInputLayer;
    std::vector<LayerPin> blobsToKeep;
    MapIdToLayerData layers;
    std::map<String, int> layerNameToId;
    std::map<std::string, int> outputNameToId;  // use registerOutput() to populate outputs
    BlobManager blobManager;
    int preferableBackend;
    int preferableTarget;
    String halideConfigFile;
    bool hasDynamicShapes;
    // Map host data to backend specific wrapper.
    std::map<void*, Ptr<BackendWrapper>> backendWrappers;

    int lastLayerId;

    bool netWasAllocated;
    bool netWasQuantized;
    bool fu...",1,src\net_impl.hpp,cv.dnn,30,dnn,1
431359,NAMESPACE_BLOCK,<empty>,,src\net_impl_backend.cpp,src\net_impl_backend.cpp:<global>,,<global>,1
431363,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


Ptr<BackendWrapper> Net::Impl::wrap(Mat& host)
{
    if (preferableBackend == DNN_BACKEND_OPENCV &&
            (preferableTarget == DNN_TARGET_CPU || preferableTarget == DNN_TARGET_CPU_FP16))
        return Ptr<BackendWrapper>();

    MatShape shape(host.dims);
    for (int i = 0; i < host.dims; ++i)
        shape[i] = host.size[i];

    void* data = host.data;
    if (backendWrappers.find(data) != backendWrappers.end())
    {
        Ptr<BackendWrapper> baseBuffer = backendWrappers[data];
        if (preferableBackend == DNN_BACKEND_OPENCV)
        {
#ifdef HAVE_OPENCL
            CV_Assert(IS_DNN_OPENCL_TARGET(preferableTarget));
            return OpenCLBackendWrapper::create(baseBuffer, host);
#else
            CV_Error(Error::StsInternal, """");
#endif
        }
        else if (preferableBackend == DNN_BACKEND_HALIDE)
        {
            CV_Assert(haveHalide());
#ifdef HAVE_HALIDE
            return Ptr<BackendWrapper>(...",1,src\net_impl_backend.cpp,cv,13,cv,1
431364,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


Ptr<BackendWrapper> Net::Impl::wrap(Mat& host)
{
    if (preferableBackend == DNN_BACKEND_OPENCV &&
            (preferableTarget == DNN_TARGET_CPU || preferableTarget == DNN_TARGET_CPU_FP16))
        return Ptr<BackendWrapper>();

    MatShape shape(host.dims);
    for (int i = 0; i < host.dims; ++i)
        shape[i] = host.size[i];

    void* data = host.data;
    if (backendWrappers.find(data) != backendWrappers.end())
    {
        Ptr<BackendWrapper> baseBuffer = backendWrappers[data];
        if (preferableBackend == DNN_BACKEND_OPENCV)
        {
#ifdef HAVE_OPENCL
            CV_Assert(IS_DNN_OPENCL_TARGET(preferableTarget));
            return OpenCLBackendWrapper::create(baseBuffer, host);
#else
            CV_Error(Error::StsInternal, """");
#endif
        }
        else if (preferableBackend == DNN_BACKEND_HALIDE)
        {
            CV_Assert(haveHalide());
#ifdef HAVE_HALIDE
            return Ptr<BackendWrapper>(new HalideBacke...",1,src\net_impl_backend.cpp,cv.dnn,14,dnn,1
431373,NAMESPACE_BLOCK,<empty>,,src\net_impl_fuse.cpp,src\net_impl_fuse.cpp:<global>,,<global>,1
431377,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


void Net::Impl::enableFusion(bool fusion_)
{
    if (fusion != fusion_)
    {
        fusion = fusion_;
        clear();
    }
}


#if 0
#define printf_(args) printf args
#else
#define printf_(args)
#endif


void Net::Impl::fuseLayers(const std::vector<LayerPin>& blobsToKeep_)
{
    CV_TRACE_FUNCTION();

    if(!fusion || (preferableBackend != DNN_BACKEND_OPENCV &&
                    preferableBackend != DNN_BACKEND_CUDA &&
                    preferableBackend != DNN_BACKEND_INFERENCE_ENGINE_NGRAPH &&
                    preferableBackend != DNN_BACKEND_TIMVX &&
                    preferableBackend != DNN_BACKEND_VKCOM))
       return;

#if 0  // FIXIT mode without fusion is broken due to unsupported layers and handling of ""custom"" nodes
    if (preferableBackend == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
        return;
#endif

    // scan through all the layers. If there is convolution layer followed by the activation layer,...",1,src\net_impl_fuse.cpp,cv,13,cv,1
431378,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


void Net::Impl::enableFusion(bool fusion_)
{
    if (fusion != fusion_)
    {
        fusion = fusion_;
        clear();
    }
}


#if 0
#define printf_(args) printf args
#else
#define printf_(args)
#endif


void Net::Impl::fuseLayers(const std::vector<LayerPin>& blobsToKeep_)
{
    CV_TRACE_FUNCTION();

    if(!fusion || (preferableBackend != DNN_BACKEND_OPENCV &&
                    preferableBackend != DNN_BACKEND_CUDA &&
                    preferableBackend != DNN_BACKEND_INFERENCE_ENGINE_NGRAPH &&
                    preferableBackend != DNN_BACKEND_TIMVX &&
                    preferableBackend != DNN_BACKEND_VKCOM))
       return;

#if 0  // FIXIT mode without fusion is broken due to unsupported layers and handling of ""custom"" nodes
    if (preferableBackend == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
        return;
#endif

    // scan through all the layers. If there is convolution layer followed by the activation layer,
    // we try ...",1,src\net_impl_fuse.cpp,cv.dnn,14,dnn,1
431397,NAMESPACE_BLOCK,<empty>,,src\net_openvino.cpp,src\net_openvino.cpp:<global>,,<global>,1
431401,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_INF_ENGINE

// TODO: use ""string"" target specifier
class NetImplOpenVINO CV_FINAL : public Net::Impl
{
public:
    typedef Net::Impl Base;

    // this default constructor is used with OpenVINO native loader
    // TODO: dedicated Impl?
    NetImplOpenVINO()
        : Net::Impl()
    {
        preferableBackend = DNN_BACKEND_INFERENCE_ENGINE_NGRAPH;
    }

    // constructor to derive execution implementation from the loaded network
    explicit NetImplOpenVINO(const Ptr<Net::Impl>& basePtr)
        : Net::Impl()
    {
        basePtr_ = basePtr;
        init();
    }

    void init()
    {
        CV_TRACE_FUNCTION();
        CV_Assert(basePtr_);
        Net::Impl& base = *basePtr_;
        CV_Assert(!base.netWasAllocated);
        netInputLayer = base.netInputLayer;
        blobsToKeep = base.blobsToKeep;
        layers = base.layers;
        for (MapIdToLayerData::iterator it = layers.begin(); it != layers.end();...",1,src\net_openvino.cpp,cv,17,cv,1
431402,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_INF_ENGINE

// TODO: use ""string"" target specifier
class NetImplOpenVINO CV_FINAL : public Net::Impl
{
public:
    typedef Net::Impl Base;

    // this default constructor is used with OpenVINO native loader
    // TODO: dedicated Impl?
    NetImplOpenVINO()
        : Net::Impl()
    {
        preferableBackend = DNN_BACKEND_INFERENCE_ENGINE_NGRAPH;
    }

    // constructor to derive execution implementation from the loaded network
    explicit NetImplOpenVINO(const Ptr<Net::Impl>& basePtr)
        : Net::Impl()
    {
        basePtr_ = basePtr;
        init();
    }

    void init()
    {
        CV_TRACE_FUNCTION();
        CV_Assert(basePtr_);
        Net::Impl& base = *basePtr_;
        CV_Assert(!base.netWasAllocated);
        netInputLayer = base.netInputLayer;
        blobsToKeep = base.blobsToKeep;
        layers = base.layers;
        for (MapIdToLayerData::iterator it = layers.begin(); it != layers.end(); it++)
        ...",1,src\net_openvino.cpp,cv.dnn,18,dnn,1
431409,NAMESPACE_BLOCK,<empty>,,src\net_quantization.cpp,src\net_quantization.cpp:<global>,,<global>,1
431413,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


// FIXIT drop from inference API
static
void getQuantizationParams(const Mat& src, std::vector<float>& scales, std::vector<int>& zeropoints)
{
    const int qmin = -128; // INT8_MIN
    const int qmax = 127;  // INT8_MAX

    double rmin, rmax, sc, zp;
    cv::minMaxIdx(src, &rmin, &rmax);

    // 0 must be present in the range [rmin, rmax]
    rmin = std::min(rmin, 0.0);
    rmax = std::max(rmax, 0.0);

    sc = (rmax == rmin) ? 1.0 : (rmax - rmin)/(qmax - qmin);
    zp = qmin - (rmin/sc);

    scales.push_back((float)sc);
    zeropoints.push_back((int)std::round(zp));
}

// FIXIT drop from inference API
Net Net::Impl::quantize(Net& net, InputArrayOfArrays calibData, int inputsDtype, int outputsDtype, bool perChannel)
{
    // Net can be quantized only once.
    if (netWasQuantized)
        CV_Error(Error::StsBadArg, ""Cannot quantize a quantized net"");

    CV_CheckType(inputsDtype, inputsDtype == CV_32F || inputsDtype == CV_...",1,src\net_quantization.cpp,cv,9,cv,1
431414,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


// FIXIT drop from inference API
static
void getQuantizationParams(const Mat& src, std::vector<float>& scales, std::vector<int>& zeropoints)
{
    const int qmin = -128; // INT8_MIN
    const int qmax = 127;  // INT8_MAX

    double rmin, rmax, sc, zp;
    cv::minMaxIdx(src, &rmin, &rmax);

    // 0 must be present in the range [rmin, rmax]
    rmin = std::min(rmin, 0.0);
    rmax = std::max(rmax, 0.0);

    sc = (rmax == rmin) ? 1.0 : (rmax - rmin)/(qmax - qmin);
    zp = qmin - (rmin/sc);

    scales.push_back((float)sc);
    zeropoints.push_back((int)std::round(zp));
}

// FIXIT drop from inference API
Net Net::Impl::quantize(Net& net, InputArrayOfArrays calibData, int inputsDtype, int outputsDtype, bool perChannel)
{
    // Net can be quantized only once.
    if (netWasQuantized)
        CV_Error(Error::StsBadArg, ""Cannot quantize a quantized net"");

    CV_CheckType(inputsDtype, inputsDtype == CV_32F || inputsDtype == CV_8S, ""Input dept...",1,src\net_quantization.cpp,cv.dnn,10,dnn,1
431454,NAMESPACE_BLOCK,<empty>,,src\nms.cpp,src\nms.cpp:<global>,,<global>,1
431457,NAMESPACE_BLOCK,"namespace cv { namespace dnn {
CV__DNN_INLINE_NS_BEGIN

template <typename T>
static inline float rectOverlap(const T& a, const T& b)
{
    return 1.f - static_cast<float>(jaccardDistance(a, b));
}

void NMSBoxes(const std::vector<Rect>& bboxes, const std::vector<float>& scores,
                          const float score_threshold, const float nms_threshold,
                          std::vector<int>& indices, const float eta, const int top_k)
{
    CV_Assert_N(bboxes.size() == scores.size(), score_threshold >= 0,
        nms_threshold >= 0, eta > 0);
    NMSFast_(bboxes, scores, score_threshold, nms_threshold, eta, top_k, indices, rectOverlap);
}

void NMSBoxes(const std::vector<Rect2d>& bboxes, const std::vector<float>& scores,
                          const float score_threshold, const float nms_threshold,
                          std::vector<int>& indices, const float eta, const int top_k)
{
    CV_Assert_N(bboxes.size() == scores.size(), score_threshold >= 0,
        nms_thr...",1,src\nms.cpp,cv,13,cv,1
431458,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

template <typename T>
static inline float rectOverlap(const T& a, const T& b)
{
    return 1.f - static_cast<float>(jaccardDistance(a, b));
}

void NMSBoxes(const std::vector<Rect>& bboxes, const std::vector<float>& scores,
                          const float score_threshold, const float nms_threshold,
                          std::vector<int>& indices, const float eta, const int top_k)
{
    CV_Assert_N(bboxes.size() == scores.size(), score_threshold >= 0,
        nms_threshold >= 0, eta > 0);
    NMSFast_(bboxes, scores, score_threshold, nms_threshold, eta, top_k, indices, rectOverlap);
}

void NMSBoxes(const std::vector<Rect2d>& bboxes, const std::vector<float>& scores,
                          const float score_threshold, const float nms_threshold,
                          std::vector<int>& indices, const float eta, const int top_k)
{
    CV_Assert_N(bboxes.size() == scores.size(), score_threshold >= 0,
        nms_threshold >= 0, et...",16,src\nms.cpp,cv.dnn,13,dnn,1
432177,NAMESPACE_BLOCK,<empty>,,src\nms.inl.hpp,src\nms.inl.hpp:<global>,,<global>,1
432181,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {

namespace
{

template <typename T>
static inline bool SortScorePairDescend(const std::pair<float, T>& pair1,
                          const std::pair<float, T>& pair2)
{
    return pair1.first > pair2.first;
}

} // namespace

// Get max scores with corresponding indices.
//    scores: a set of scores.
//    threshold: only consider scores higher than the threshold.
//    top_k: if -1, keep all; otherwise, keep at most top_k.
//    score_index_vec: store the sorted (score, index) pair.
inline void GetMaxScoreIndex(const std::vector<float>& scores, const float threshold, const int top_k,
                      std::vector<std::pair<float, int> >& score_index_vec)
{
    CV_DbgAssert(score_index_vec.empty());
    // Generate index score pairs.
    for (size_t i = 0; i < scores.size(); ++i)
    {
        if (scores[i] > threshold)
        {
            score_index_vec.push_back(std::make_pair(scores[i], i));
        }
    }

    // Sort the score pair acc...",1,src\nms.inl.hpp,cv,13,cv,1
432182,NAMESPACE_BLOCK,"namespace dnn {

namespace
{

template <typename T>
static inline bool SortScorePairDescend(const std::pair<float, T>& pair1,
                          const std::pair<float, T>& pair2)
{
    return pair1.first > pair2.first;
}

} // namespace

// Get max scores with corresponding indices.
//    scores: a set of scores.
//    threshold: only consider scores higher than the threshold.
//    top_k: if -1, keep all; otherwise, keep at most top_k.
//    score_index_vec: store the sorted (score, index) pair.
inline void GetMaxScoreIndex(const std::vector<float>& scores, const float threshold, const int top_k,
                      std::vector<std::pair<float, int> >& score_index_vec)
{
    CV_DbgAssert(score_index_vec.empty());
    // Generate index score pairs.
    for (size_t i = 0; i < scores.size(); ++i)
    {
        if (scores[i] > threshold)
        {
            score_index_vec.push_back(std::make_pair(scores[i], i));
        }
    }

    // Sort the score pair according to the s...",1,src\nms.inl.hpp,cv.dnn,14,dnn,1
432183,NAMESPACE_BLOCK,"namespace
{

template <typename T>
static inline bool SortScorePairDescend(const std::pair<float, T>& pair1,
                          const std::pair<float, T>& pair2)
{
    return pair1.first > pair2.first;
}

}",1,src\nms.inl.hpp,cv.dnn.anonymous_namespace_0,16,,1
432425,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\include\common.hpp,src\ocl4dnn\include\common.hpp:<global>,,<global>,1
432435,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\include\default_kernel_config.hpp,src\ocl4dnn\include\default_kernel_config.hpp:<global>,,<global>,1
433300,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\include\math_functions.hpp,src\ocl4dnn\include\math_functions.hpp:<global>,,<global>,1
433304,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
namespace ocl4dnn
{

enum CBLAS_TRANSPOSE {CblasNoTrans=111, CblasTrans=112, CblasConjTrans=113};

template<typename Dtype>
bool ocl4dnnGEMMCommon(const CBLAS_TRANSPOSE TransB,
                       const int32_t M, const int32_t N, const int32_t K,
                       const UMat A, const UMat B,
                       const UMat B_image, UMat C,
                       const size_t max_image_size);

template<typename Dtype>
ocl::Image2D ocl4dnnGEMMCopyBufferToImage(UMat buffer, int offset,
                                          bool is_matrix_a, bool transpose,
                                          bool padding, int padded_height,
                                          int padded_width, int height,
                                          int width,  int ld);

template<typename Dtype>
bool ocl4dnnGEMV(const CBLAS_TRANSPOSE TransA,
                 const int32_t M, const int32_t N, const Dtype alpha,
                 const UMat A, const i...",1,src\ocl4dnn\include\math_functions.hpp,cv,47,cv,1
433305,NAMESPACE_BLOCK,"namespace dnn
{
namespace ocl4dnn
{

enum CBLAS_TRANSPOSE {CblasNoTrans=111, CblasTrans=112, CblasConjTrans=113};

template<typename Dtype>
bool ocl4dnnGEMMCommon(const CBLAS_TRANSPOSE TransB,
                       const int32_t M, const int32_t N, const int32_t K,
                       const UMat A, const UMat B,
                       const UMat B_image, UMat C,
                       const size_t max_image_size);

template<typename Dtype>
ocl::Image2D ocl4dnnGEMMCopyBufferToImage(UMat buffer, int offset,
                                          bool is_matrix_a, bool transpose,
                                          bool padding, int padded_height,
                                          int padded_width, int height,
                                          int width,  int ld);

template<typename Dtype>
bool ocl4dnnGEMV(const CBLAS_TRANSPOSE TransA,
                 const int32_t M, const int32_t N, const Dtype alpha,
                 const UMat A, const int32_t offA, co...",1,src\ocl4dnn\include\math_functions.hpp,cv.dnn,49,dnn,1
433306,NAMESPACE_BLOCK,"namespace ocl4dnn
{

enum CBLAS_TRANSPOSE {CblasNoTrans=111, CblasTrans=112, CblasConjTrans=113};

template<typename Dtype>
bool ocl4dnnGEMMCommon(const CBLAS_TRANSPOSE TransB,
                       const int32_t M, const int32_t N, const int32_t K,
                       const UMat A, const UMat B,
                       const UMat B_image, UMat C,
                       const size_t max_image_size);

template<typename Dtype>
ocl::Image2D ocl4dnnGEMMCopyBufferToImage(UMat buffer, int offset,
                                          bool is_matrix_a, bool transpose,
                                          bool padding, int padded_height,
                                          int padded_width, int height,
                                          int width,  int ld);

template<typename Dtype>
bool ocl4dnnGEMV(const CBLAS_TRANSPOSE TransA,
                 const int32_t M, const int32_t N, const Dtype alpha,
                 const UMat A, const int32_t offA, const UMat x,
    ...",1,src\ocl4dnn\include\math_functions.hpp,cv.dnn.ocl4dnn,51,ocl4dnn,1
433389,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\include\ocl4dnn.hpp,src\ocl4dnn\include\ocl4dnn.hpp:<global>,,<global>,1
433393,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace ocl4dnn {

struct OCL4DNNConvConfig
{
    OCL4DNNConvConfig() :
        kernel(1, 1),
        stride(1, 1),
        dilation(1, 1),
        group(1),
        bias_term(false),
        use_half(false)
    {
        pads = {0, 0, 0, 0};
    }
    MatShape in_shape;
    MatShape out_shape;
    Size kernel;
    std::vector<int> pads; // [pad_top, pad_bottom, pad_left, pad_right]
    Size stride;
    Size dilation;
    int group; // = 1;
    bool bias_term; // = false;
    bool use_half; // = false;
};

typedef enum {
    OCL4DNN_CONV_FUSED_ACTIV_NONE                 = 0,
    OCL4DNN_CONV_FUSED_ACTIV_RELU                 = 1,
    OCL4DNN_CONV_FUSED_ACTIV_PRELU                = 2,
    OCL4DNN_CONV_FUSED_ACTIV_POWER                = 3,
    OCL4DNN_CONV_FUSED_ACTIV_TANH                 = 4,
    OCL4DNN_CONV_FUSED_ACTIV_RELU6                = 5
} ocl4dnnFusedActiv_t;

template<typename Dtype>
class OCL4DNNConvSpatial
{
    public:
        explicit OCL...",1,src\ocl4dnn\include\ocl4dnn.hpp,cv,52,cv,1
433394,NAMESPACE_BLOCK,"namespace dnn { namespace ocl4dnn {

struct OCL4DNNConvConfig
{
    OCL4DNNConvConfig() :
        kernel(1, 1),
        stride(1, 1),
        dilation(1, 1),
        group(1),
        bias_term(false),
        use_half(false)
    {
        pads = {0, 0, 0, 0};
    }
    MatShape in_shape;
    MatShape out_shape;
    Size kernel;
    std::vector<int> pads; // [pad_top, pad_bottom, pad_left, pad_right]
    Size stride;
    Size dilation;
    int group; // = 1;
    bool bias_term; // = false;
    bool use_half; // = false;
};

typedef enum {
    OCL4DNN_CONV_FUSED_ACTIV_NONE                 = 0,
    OCL4DNN_CONV_FUSED_ACTIV_RELU                 = 1,
    OCL4DNN_CONV_FUSED_ACTIV_PRELU                = 2,
    OCL4DNN_CONV_FUSED_ACTIV_POWER                = 3,
    OCL4DNN_CONV_FUSED_ACTIV_TANH                 = 4,
    OCL4DNN_CONV_FUSED_ACTIV_RELU6                = 5
} ocl4dnnFusedActiv_t;

template<typename Dtype>
class OCL4DNNConvSpatial
{
    public:
        explicit OCL4DNNConvSpatial...",16,src\ocl4dnn\include\ocl4dnn.hpp,cv.dnn,52,dnn,1
433395,NAMESPACE_BLOCK,"namespace ocl4dnn {

struct OCL4DNNConvConfig
{
    OCL4DNNConvConfig() :
        kernel(1, 1),
        stride(1, 1),
        dilation(1, 1),
        group(1),
        bias_term(false),
        use_half(false)
    {
        pads = {0, 0, 0, 0};
    }
    MatShape in_shape;
    MatShape out_shape;
    Size kernel;
    std::vector<int> pads; // [pad_top, pad_bottom, pad_left, pad_right]
    Size stride;
    Size dilation;
    int group; // = 1;
    bool bias_term; // = false;
    bool use_half; // = false;
};

typedef enum {
    OCL4DNN_CONV_FUSED_ACTIV_NONE                 = 0,
    OCL4DNN_CONV_FUSED_ACTIV_RELU                 = 1,
    OCL4DNN_CONV_FUSED_ACTIV_PRELU                = 2,
    OCL4DNN_CONV_FUSED_ACTIV_POWER                = 3,
    OCL4DNN_CONV_FUSED_ACTIV_TANH                 = 4,
    OCL4DNN_CONV_FUSED_ACTIV_RELU6                = 5
} ocl4dnnFusedActiv_t;

template<typename Dtype>
class OCL4DNNConvSpatial
{
    public:
        explicit OCL4DNNConvSpatial(OCL4DNNConvConf...",32,src\ocl4dnn\include\ocl4dnn.hpp,cv.dnn.ocl4dnn,52,ocl4dnn,1
434181,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\src\common.cpp,src\ocl4dnn\src\common.cpp:<global>,,<global>,1
434231,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\src\math_functions.cpp,src\ocl4dnn\src\math_functions.cpp:<global>,,<global>,1
434235,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace ocl4dnn {

enum gemm_data_type_t
{
    TYPE_FLOAT = 1,
    TYPE_HALF = 2
};

// Create and copy buffer to image for GEMM's matrix A and B.
// Will return image to caller if the input image is NULL. Otherwise,
// will use the image directly. It's caller's responsibility to
// release the created image.
template<typename Dtype>
ocl::Image2D ocl4dnnGEMMCopyBufferToImage(UMat buffer, int offset,
                                          bool is_matrix_a, bool transpose,
                                          bool padding, int padded_height,
                                          int padded_width, int height,
                                          int width, int ld)
{
    ocl::Image2D image;
    String opts = format(""-DTYPE=%d"", TYPE_FLOAT);

    if (!is_matrix_a && transpose)
    {
        if (ld == width)
        {
            image = ocl::Image2D(buffer);
        } else {
            // For matrix B with transpose, we need to handle th...",1,src\ocl4dnn\src\math_functions.cpp,cv,51,cv,1
434236,NAMESPACE_BLOCK,"namespace dnn { namespace ocl4dnn {

enum gemm_data_type_t
{
    TYPE_FLOAT = 1,
    TYPE_HALF = 2
};

// Create and copy buffer to image for GEMM's matrix A and B.
// Will return image to caller if the input image is NULL. Otherwise,
// will use the image directly. It's caller's responsibility to
// release the created image.
template<typename Dtype>
ocl::Image2D ocl4dnnGEMMCopyBufferToImage(UMat buffer, int offset,
                                          bool is_matrix_a, bool transpose,
                                          bool padding, int padded_height,
                                          int padded_width, int height,
                                          int width, int ld)
{
    ocl::Image2D image;
    String opts = format(""-DTYPE=%d"", TYPE_FLOAT);

    if (!is_matrix_a && transpose)
    {
        if (ld == width)
        {
            image = ocl::Image2D(buffer);
        } else {
            // For matrix B with transpose, we need to handle them differently....",16,src\ocl4dnn\src\math_functions.cpp,cv.dnn,51,dnn,1
434237,NAMESPACE_BLOCK,"namespace ocl4dnn {

enum gemm_data_type_t
{
    TYPE_FLOAT = 1,
    TYPE_HALF = 2
};

// Create and copy buffer to image for GEMM's matrix A and B.
// Will return image to caller if the input image is NULL. Otherwise,
// will use the image directly. It's caller's responsibility to
// release the created image.
template<typename Dtype>
ocl::Image2D ocl4dnnGEMMCopyBufferToImage(UMat buffer, int offset,
                                          bool is_matrix_a, bool transpose,
                                          bool padding, int padded_height,
                                          int padded_width, int height,
                                          int width, int ld)
{
    ocl::Image2D image;
    String opts = format(""-DTYPE=%d"", TYPE_FLOAT);

    if (!is_matrix_a && transpose)
    {
        if (ld == width)
        {
            image = ocl::Image2D(buffer);
        } else {
            // For matrix B with transpose, we need to handle them differently.
            // ...",32,src\ocl4dnn\src\math_functions.cpp,cv.dnn.ocl4dnn,51,ocl4dnn,1
436807,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\src\ocl4dnn_conv_spatial.cpp,src\ocl4dnn\src\ocl4dnn_conv_spatial.cpp:<global>,,<global>,1
436811,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace ocl4dnn {
static cv::Mutex kernelConfigMutex;
typedef std::map<std::string, std::string> kernel_hash_t;
static kernel_hash_t kernelConfigMap;
static bool defaultConfigLoaded = false;

static bool enableWorkaroundIDLF()
{
    static bool param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_WORKAROUND_IDLF"", true);
    return param;
}

static bool dumpFailedResult()
{
    static bool param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_DUMP_FAILED_RESULT"", false);
    return param;
}

static size_t testAllKernels()
{
    static size_t param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_TEST_ALL_KERNELS"", 0);
    return param;
}

static bool raiseOnCheckError()
{
    static bool param = utils::getConfigurationParameterBool(""OPENCV_OCL4DNN_TUNING_RAISE_CHECK_ERROR"", false);
    return param;
}

static std::string sanitize(const std::string& s)
{
    std::string s_ = s;
    for (size_t i = 0; i < s_.size(); i++)
    {
 ...",1,src\ocl4dnn\src\ocl4dnn_conv_spatial.cpp,cv,66,cv,1
436812,NAMESPACE_BLOCK,"namespace dnn { namespace ocl4dnn {
static cv::Mutex kernelConfigMutex;
typedef std::map<std::string, std::string> kernel_hash_t;
static kernel_hash_t kernelConfigMap;
static bool defaultConfigLoaded = false;

static bool enableWorkaroundIDLF()
{
    static bool param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_WORKAROUND_IDLF"", true);
    return param;
}

static bool dumpFailedResult()
{
    static bool param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_DUMP_FAILED_RESULT"", false);
    return param;
}

static size_t testAllKernels()
{
    static size_t param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_TEST_ALL_KERNELS"", 0);
    return param;
}

static bool raiseOnCheckError()
{
    static bool param = utils::getConfigurationParameterBool(""OPENCV_OCL4DNN_TUNING_RAISE_CHECK_ERROR"", false);
    return param;
}

static std::string sanitize(const std::string& s)
{
    std::string s_ = s;
    for (size_t i = 0; i < s_.size(); i++)
    {
        char c =...",16,src\ocl4dnn\src\ocl4dnn_conv_spatial.cpp,cv.dnn,66,dnn,1
436813,NAMESPACE_BLOCK,"namespace ocl4dnn {
static cv::Mutex kernelConfigMutex;
typedef std::map<std::string, std::string> kernel_hash_t;
static kernel_hash_t kernelConfigMap;
static bool defaultConfigLoaded = false;

static bool enableWorkaroundIDLF()
{
    static bool param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_WORKAROUND_IDLF"", true);
    return param;
}

static bool dumpFailedResult()
{
    static bool param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_DUMP_FAILED_RESULT"", false);
    return param;
}

static size_t testAllKernels()
{
    static size_t param = utils::getConfigurationParameterSizeT(""OPENCV_OCL4DNN_TEST_ALL_KERNELS"", 0);
    return param;
}

static bool raiseOnCheckError()
{
    static bool param = utils::getConfigurationParameterBool(""OPENCV_OCL4DNN_TUNING_RAISE_CHECK_ERROR"", false);
    return param;
}

static std::string sanitize(const std::string& s)
{
    std::string s_ = s;
    for (size_t i = 0; i < s_.size(); i++)
    {
        char c = s_[i];
        ...",32,src\ocl4dnn\src\ocl4dnn_conv_spatial.cpp,cv.dnn.ocl4dnn,66,ocl4dnn,1
444087,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\src\ocl4dnn_inner_product.cpp,src\ocl4dnn\src\ocl4dnn_inner_product.cpp:<global>,,<global>,1
444091,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace ocl4dnn {
template<typename Dtype>
OCL4DNNInnerProduct<Dtype>::OCL4DNNInnerProduct(OCL4DNNInnerProductConfig config)
{
    bias_term_  = config.bias_term;
    transpose_  = config.transpose;
    N_ = num_output_ = config.num_output;
    M_ = config.M;
    K_ = config.K;
    phase_test_ = config.phase_test;
    image_copied_ = false;
    use_half_ = config.use_half;
}

template<typename Dtype>
OCL4DNNInnerProduct<Dtype>::~OCL4DNNInnerProduct()
{
}

template<typename Dtype>
bool OCL4DNNInnerProduct<Dtype>::Forward(const UMat& bottom,
                                         const UMat& weight,
                                         const UMat& bias,
                                         UMat& top)
{
    bool ret;

    if (M_ == 1)
    {
        ret = ocl4dnnGEMV<Dtype>(CblasNoTrans, N_, K_, (Dtype) 1.,
                                 weight, 0, bottom, 0, (Dtype) 0., top, 0);

        if (bias_term_ && ret)
            ret = ocl4dnnAXPY<D...",1,src\ocl4dnn\src\ocl4dnn_inner_product.cpp,cv,48,cv,1
444092,NAMESPACE_BLOCK,"namespace dnn { namespace ocl4dnn {
template<typename Dtype>
OCL4DNNInnerProduct<Dtype>::OCL4DNNInnerProduct(OCL4DNNInnerProductConfig config)
{
    bias_term_  = config.bias_term;
    transpose_  = config.transpose;
    N_ = num_output_ = config.num_output;
    M_ = config.M;
    K_ = config.K;
    phase_test_ = config.phase_test;
    image_copied_ = false;
    use_half_ = config.use_half;
}

template<typename Dtype>
OCL4DNNInnerProduct<Dtype>::~OCL4DNNInnerProduct()
{
}

template<typename Dtype>
bool OCL4DNNInnerProduct<Dtype>::Forward(const UMat& bottom,
                                         const UMat& weight,
                                         const UMat& bias,
                                         UMat& top)
{
    bool ret;

    if (M_ == 1)
    {
        ret = ocl4dnnGEMV<Dtype>(CblasNoTrans, N_, K_, (Dtype) 1.,
                                 weight, 0, bottom, 0, (Dtype) 0., top, 0);

        if (bias_term_ && ret)
            ret = ocl4dnnAXPY<Dtype>(N_, 1, bi...",16,src\ocl4dnn\src\ocl4dnn_inner_product.cpp,cv.dnn,48,dnn,1
444093,NAMESPACE_BLOCK,"namespace ocl4dnn {
template<typename Dtype>
OCL4DNNInnerProduct<Dtype>::OCL4DNNInnerProduct(OCL4DNNInnerProductConfig config)
{
    bias_term_  = config.bias_term;
    transpose_  = config.transpose;
    N_ = num_output_ = config.num_output;
    M_ = config.M;
    K_ = config.K;
    phase_test_ = config.phase_test;
    image_copied_ = false;
    use_half_ = config.use_half;
}

template<typename Dtype>
OCL4DNNInnerProduct<Dtype>::~OCL4DNNInnerProduct()
{
}

template<typename Dtype>
bool OCL4DNNInnerProduct<Dtype>::Forward(const UMat& bottom,
                                         const UMat& weight,
                                         const UMat& bias,
                                         UMat& top)
{
    bool ret;

    if (M_ == 1)
    {
        ret = ocl4dnnGEMV<Dtype>(CblasNoTrans, N_, K_, (Dtype) 1.,
                                 weight, 0, bottom, 0, (Dtype) 0., top, 0);

        if (bias_term_ && ret)
            ret = ocl4dnnAXPY<Dtype>(N_, 1, bias, 0, top, 0);
...",32,src\ocl4dnn\src\ocl4dnn_inner_product.cpp,cv.dnn.ocl4dnn,48,ocl4dnn,1
444307,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\src\ocl4dnn_lrn.cpp,src\ocl4dnn\src\ocl4dnn_lrn.cpp:<global>,,<global>,1
444311,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace ocl4dnn {
template<typename Dtype>
OCL4DNNLRN<Dtype>::OCL4DNNLRN(OCL4DNNLRNConfig config)
{
    lrn_type_ = config.lrn_type;
    phase_test_ = config.phase_test;
    size_ = config.local_size;
    CHECK_EQ(size_ % 2, 1)<< ""LRN only supports odd values for local_size"";
    alpha_ = config.alpha;
    beta_ = config.beta;
    k_ = config.k;
    norm_by_size_ = config.norm_by_size;
    num_ = config.batch_size;
    channels_ = config.channels;
    height_ = config.height;
    width_ = config.width;
    use_half_ = config.use_half;
}

template<typename Dtype>
bool OCL4DNNLRN<Dtype>::Forward(const UMat& bottom, UMat& top)
{
    bool ret = true;

    switch (lrn_type_)
    {
    case LRNParameter_NormRegion_ACROSS_CHANNELS:
        ret = crossChannelForward(bottom, top);
        break;
    case LRNParameter_NormRegion_WITHIN_CHANNEL:
        //TODO
        //WithinChannelForward(bottom_data, top_data);
        ret = false;
        break;
    default...",1,src\ocl4dnn\src\ocl4dnn_lrn.cpp,cv,48,cv,1
444312,NAMESPACE_BLOCK,"namespace dnn { namespace ocl4dnn {
template<typename Dtype>
OCL4DNNLRN<Dtype>::OCL4DNNLRN(OCL4DNNLRNConfig config)
{
    lrn_type_ = config.lrn_type;
    phase_test_ = config.phase_test;
    size_ = config.local_size;
    CHECK_EQ(size_ % 2, 1)<< ""LRN only supports odd values for local_size"";
    alpha_ = config.alpha;
    beta_ = config.beta;
    k_ = config.k;
    norm_by_size_ = config.norm_by_size;
    num_ = config.batch_size;
    channels_ = config.channels;
    height_ = config.height;
    width_ = config.width;
    use_half_ = config.use_half;
}

template<typename Dtype>
bool OCL4DNNLRN<Dtype>::Forward(const UMat& bottom, UMat& top)
{
    bool ret = true;

    switch (lrn_type_)
    {
    case LRNParameter_NormRegion_ACROSS_CHANNELS:
        ret = crossChannelForward(bottom, top);
        break;
    case LRNParameter_NormRegion_WITHIN_CHANNEL:
        //TODO
        //WithinChannelForward(bottom_data, top_data);
        ret = false;
        break;
    default:
        ret =...",16,src\ocl4dnn\src\ocl4dnn_lrn.cpp,cv.dnn,48,dnn,1
444313,NAMESPACE_BLOCK,"namespace ocl4dnn {
template<typename Dtype>
OCL4DNNLRN<Dtype>::OCL4DNNLRN(OCL4DNNLRNConfig config)
{
    lrn_type_ = config.lrn_type;
    phase_test_ = config.phase_test;
    size_ = config.local_size;
    CHECK_EQ(size_ % 2, 1)<< ""LRN only supports odd values for local_size"";
    alpha_ = config.alpha;
    beta_ = config.beta;
    k_ = config.k;
    norm_by_size_ = config.norm_by_size;
    num_ = config.batch_size;
    channels_ = config.channels;
    height_ = config.height;
    width_ = config.width;
    use_half_ = config.use_half;
}

template<typename Dtype>
bool OCL4DNNLRN<Dtype>::Forward(const UMat& bottom, UMat& top)
{
    bool ret = true;

    switch (lrn_type_)
    {
    case LRNParameter_NormRegion_ACROSS_CHANNELS:
        ret = crossChannelForward(bottom, top);
        break;
    case LRNParameter_NormRegion_WITHIN_CHANNEL:
        //TODO
        //WithinChannelForward(bottom_data, top_data);
        ret = false;
        break;
    default:
        ret = false;
        ...",32,src\ocl4dnn\src\ocl4dnn_lrn.cpp,cv.dnn.ocl4dnn,48,ocl4dnn,1
444685,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\src\ocl4dnn_pool.cpp,src\ocl4dnn\src\ocl4dnn_pool.cpp:<global>,,<global>,1
444689,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace ocl4dnn {
template<typename Dtype>
OCL4DNNPool<Dtype>::OCL4DNNPool(OCL4DNNPoolConfig config)
{
    int dims = config.in_shape.size();
    int spatial_dims = config.in_shape.size()-2;

    channels_ = config.channels;
    pool_method_ = config.pool_method;
    avePoolPaddedArea = config.avePoolPaddedArea;
    computeMaxIdx = config.computeMaxIdx;
    use_half = config.use_half;
    kernel_shape_.push_back(config.kernel.height);
    kernel_shape_.push_back(config.kernel.width);
    stride_.push_back(config.stride.height);
    stride_.push_back(config.stride.width);

    for (int i = 0; i < spatial_dims; ++i)
    {
        im_in_shape_.push_back(config.in_shape[dims - spatial_dims + i]);
        im_out_shape_.push_back(config.out_shape[dims - spatial_dims + i]);
    }

    kernel_h_ = kernel_shape_[0];
    kernel_w_ = kernel_shape_[1];
    stride_h_ = stride_[0];
    stride_w_ = stride_[1];
    pad_t_ = config.pad_t;
    pad_l_ = config.pad_l;
 ...",1,src\ocl4dnn\src\ocl4dnn_pool.cpp,cv,49,cv,1
444690,NAMESPACE_BLOCK,"namespace dnn { namespace ocl4dnn {
template<typename Dtype>
OCL4DNNPool<Dtype>::OCL4DNNPool(OCL4DNNPoolConfig config)
{
    int dims = config.in_shape.size();
    int spatial_dims = config.in_shape.size()-2;

    channels_ = config.channels;
    pool_method_ = config.pool_method;
    avePoolPaddedArea = config.avePoolPaddedArea;
    computeMaxIdx = config.computeMaxIdx;
    use_half = config.use_half;
    kernel_shape_.push_back(config.kernel.height);
    kernel_shape_.push_back(config.kernel.width);
    stride_.push_back(config.stride.height);
    stride_.push_back(config.stride.width);

    for (int i = 0; i < spatial_dims; ++i)
    {
        im_in_shape_.push_back(config.in_shape[dims - spatial_dims + i]);
        im_out_shape_.push_back(config.out_shape[dims - spatial_dims + i]);
    }

    kernel_h_ = kernel_shape_[0];
    kernel_w_ = kernel_shape_[1];
    stride_h_ = stride_[0];
    stride_w_ = stride_[1];
    pad_t_ = config.pad_t;
    pad_l_ = config.pad_l;
    pad_r_ = con...",16,src\ocl4dnn\src\ocl4dnn_pool.cpp,cv.dnn,49,dnn,1
444691,NAMESPACE_BLOCK,"namespace ocl4dnn {
template<typename Dtype>
OCL4DNNPool<Dtype>::OCL4DNNPool(OCL4DNNPoolConfig config)
{
    int dims = config.in_shape.size();
    int spatial_dims = config.in_shape.size()-2;

    channels_ = config.channels;
    pool_method_ = config.pool_method;
    avePoolPaddedArea = config.avePoolPaddedArea;
    computeMaxIdx = config.computeMaxIdx;
    use_half = config.use_half;
    kernel_shape_.push_back(config.kernel.height);
    kernel_shape_.push_back(config.kernel.width);
    stride_.push_back(config.stride.height);
    stride_.push_back(config.stride.width);

    for (int i = 0; i < spatial_dims; ++i)
    {
        im_in_shape_.push_back(config.in_shape[dims - spatial_dims + i]);
        im_out_shape_.push_back(config.out_shape[dims - spatial_dims + i]);
    }

    kernel_h_ = kernel_shape_[0];
    kernel_w_ = kernel_shape_[1];
    stride_h_ = stride_[0];
    stride_w_ = stride_[1];
    pad_t_ = config.pad_t;
    pad_l_ = config.pad_l;
    pad_r_ = config.pad_r;
    p...",32,src\ocl4dnn\src\ocl4dnn_pool.cpp,cv.dnn.ocl4dnn,49,ocl4dnn,1
445260,NAMESPACE_BLOCK,<empty>,,src\ocl4dnn\src\ocl4dnn_softmax.cpp,src\ocl4dnn\src\ocl4dnn_softmax.cpp:<global>,,<global>,1
445264,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace ocl4dnn {
template<typename Dtype>
OCL4DNNSoftmax<Dtype>::OCL4DNNSoftmax(OCL4DNNSoftmaxConfig config)
{
    softmax_axis_ = config.axis;
    channels_ = config.channels;
    log_softmax_ = config.logsoftmax;
    use_half_ = config.use_half;

    inner_num_ = 1;
    outer_num_ = 1;
    count_ = 1;
    int32_t scale_sz = 1;
    for (int32_t i = softmax_axis_ + 1; i < config.in_shape.size(); i++)
        inner_num_ *= config.in_shape[i];
    use_slm_ = (config.in_shape[softmax_axis_] * inner_num_ + inner_num_ * 17) <= 8192;
    for (int32_t i = 0; i < softmax_axis_; i++)
        outer_num_ *= config.in_shape[i];
    count_ = inner_num_ + outer_num_;

    std::vector<int32_t> scale_dims = config.in_shape;
    scale_dims[softmax_axis_] = use_slm_ ? 1 : 17;
    for (int32_t i = 0; i < scale_dims.size(); i++)
        scale_sz *= scale_dims[i];

    scale_data_.create(1, scale_sz, CV_32FC1);
}

template<typename Dtype>
OCL4DNNSoftmax<Dtype>::~OCL4DNN...",1,src\ocl4dnn\src\ocl4dnn_softmax.cpp,cv,48,cv,1
445265,NAMESPACE_BLOCK,"namespace dnn { namespace ocl4dnn {
template<typename Dtype>
OCL4DNNSoftmax<Dtype>::OCL4DNNSoftmax(OCL4DNNSoftmaxConfig config)
{
    softmax_axis_ = config.axis;
    channels_ = config.channels;
    log_softmax_ = config.logsoftmax;
    use_half_ = config.use_half;

    inner_num_ = 1;
    outer_num_ = 1;
    count_ = 1;
    int32_t scale_sz = 1;
    for (int32_t i = softmax_axis_ + 1; i < config.in_shape.size(); i++)
        inner_num_ *= config.in_shape[i];
    use_slm_ = (config.in_shape[softmax_axis_] * inner_num_ + inner_num_ * 17) <= 8192;
    for (int32_t i = 0; i < softmax_axis_; i++)
        outer_num_ *= config.in_shape[i];
    count_ = inner_num_ + outer_num_;

    std::vector<int32_t> scale_dims = config.in_shape;
    scale_dims[softmax_axis_] = use_slm_ ? 1 : 17;
    for (int32_t i = 0; i < scale_dims.size(); i++)
        scale_sz *= scale_dims[i];

    scale_data_.create(1, scale_sz, CV_32FC1);
}

template<typename Dtype>
OCL4DNNSoftmax<Dtype>::~OCL4DNNSoftmax()
{
   ...",16,src\ocl4dnn\src\ocl4dnn_softmax.cpp,cv.dnn,48,dnn,1
445266,NAMESPACE_BLOCK,"namespace ocl4dnn {
template<typename Dtype>
OCL4DNNSoftmax<Dtype>::OCL4DNNSoftmax(OCL4DNNSoftmaxConfig config)
{
    softmax_axis_ = config.axis;
    channels_ = config.channels;
    log_softmax_ = config.logsoftmax;
    use_half_ = config.use_half;

    inner_num_ = 1;
    outer_num_ = 1;
    count_ = 1;
    int32_t scale_sz = 1;
    for (int32_t i = softmax_axis_ + 1; i < config.in_shape.size(); i++)
        inner_num_ *= config.in_shape[i];
    use_slm_ = (config.in_shape[softmax_axis_] * inner_num_ + inner_num_ * 17) <= 8192;
    for (int32_t i = 0; i < softmax_axis_; i++)
        outer_num_ *= config.in_shape[i];
    count_ = inner_num_ + outer_num_;

    std::vector<int32_t> scale_dims = config.in_shape;
    scale_dims[softmax_axis_] = use_slm_ ? 1 : 17;
    for (int32_t i = 0; i < scale_dims.size(); i++)
        scale_sz *= scale_dims[i];

    scale_data_.create(1, scale_sz, CV_32FC1);
}

template<typename Dtype>
OCL4DNNSoftmax<Dtype>::~OCL4DNNSoftmax()
{
    scale_data_.rel...",32,src\ocl4dnn\src\ocl4dnn_softmax.cpp,cv.dnn.ocl4dnn,48,ocl4dnn,1
445723,NAMESPACE_BLOCK,<empty>,,src\onnx\onnx_graph_simplifier.cpp,src\onnx\onnx_graph_simplifier.cpp:<global>,,<global>,1
445730,NAMESPACE_BLOCK,<empty>,,src\onnx\onnx_graph_simplifier.hpp,src\onnx\onnx_graph_simplifier.hpp:<global>,,<global>,1
445763,NAMESPACE_BLOCK,<empty>,,src\onnx\onnx_importer.cpp,src\onnx\onnx_importer.cpp:<global>,,<global>,1
445767,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

extern bool DNN_DIAGNOSTICS_RUN;

#ifdef HAVE_PROTOBUF
class ONNXLayerHandler;

template <typename T>
static T getScalarFromMat(Mat m)
{
    CV_Assert(m.total() == 1);
    return m.at<T>(0);
}

class ONNXImporter
{
    FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;

    opencv_onnx::ModelProto model_proto;
    struct LayerInfo {
        int layerId;
        int outputId;
        int depth;
        LayerInfo(int _layerId = 0, int _outputId = 0, int _depth = CV_32F)
            :layerId(_layerId), outputId(_outputId), depth(_depth) {}
    };

    struct TensorInfo {
        int real_ndims;
        TensorInfo(int _real_ndims = 0) : real_ndims(_real_ndims) {}
    };

    std::map<std::string, Mat> getGraphTensors(
                                    const opencv_onnx::GraphProto& graph_proto);
    Mat getBlob(const opencv_onnx::NodeProto& node_proto, int index);
    Mat getBlob(const std::string& input_name);
    TensorInfo ...",1,src\onnx\onnx_importer.cpp,cv,48,cv,1
445768,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

extern bool DNN_DIAGNOSTICS_RUN;

#ifdef HAVE_PROTOBUF
class ONNXLayerHandler;

template <typename T>
static T getScalarFromMat(Mat m)
{
    CV_Assert(m.total() == 1);
    return m.at<T>(0);
}

class ONNXImporter
{
    FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;

    opencv_onnx::ModelProto model_proto;
    struct LayerInfo {
        int layerId;
        int outputId;
        int depth;
        LayerInfo(int _layerId = 0, int _outputId = 0, int _depth = CV_32F)
            :layerId(_layerId), outputId(_outputId), depth(_depth) {}
    };

    struct TensorInfo {
        int real_ndims;
        TensorInfo(int _real_ndims = 0) : real_ndims(_real_ndims) {}
    };

    std::map<std::string, Mat> getGraphTensors(
                                    const opencv_onnx::GraphProto& graph_proto);
    Mat getBlob(const opencv_onnx::NodeProto& node_proto, int index);
    Mat getBlob(const std::string& input_name);
    TensorInfo getBlobExtraInf...",1,src\onnx\onnx_importer.cpp,cv.dnn,49,dnn,1
445836,NAMESPACE_BLOCK,<empty>,,src\op_cann.cpp,src\op_cann.cpp:<global>,,<global>,1
445840,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

#ifdef HAVE_CANN

std::shared_ptr<AclEnvGuard> AclEnvGuard::global_acl_env_ = nullptr;
std::mutex AclEnvGuard::global_acl_env_mutex_;

AclEnvGuard::AclEnvGuard()
{
    CV_LOG_INFO(NULL, ""Start to initialize CANN"");
    ACL_CHECK_RET(aclInit(NULL));
    CV_LOG_INFO(NULL, ""[Success] initialized CANN"");
}

AclEnvGuard::~AclEnvGuard()
{
    CV_LOG_INFO(NULL, ""Start to finalize CANN"");
    ACL_CHECK_RET(aclFinalize());
    CV_LOG_INFO(NULL, ""[Success] finalized CANN"");
}

std::shared_ptr<AclEnvGuard> AclEnvGuard::GetAclEnv()
{
    std::shared_ptr<AclEnvGuard> acl_env;

    std::lock_guard<std::mutex> lock(global_acl_env_mutex_);
    acl_env = global_acl_env_;
    if (acl_env != nullptr)
    {
        CV_LOG_INFO(NULL, ""CANN has been initialized. Skipping..."");
    }
    else
    {
        acl_env = std::make_shared<AclEnvGuard>();
        global_acl_env_ = acl_env;
    }
    return acl_env;
}

CannConstOp::CannConstOp(const uint8_t* data, const int dtype, ...",1,src\op_cann.cpp,cv,15,cv,1
445841,NAMESPACE_BLOCK,"namespace dnn {

#ifdef HAVE_CANN

std::shared_ptr<AclEnvGuard> AclEnvGuard::global_acl_env_ = nullptr;
std::mutex AclEnvGuard::global_acl_env_mutex_;

AclEnvGuard::AclEnvGuard()
{
    CV_LOG_INFO(NULL, ""Start to initialize CANN"");
    ACL_CHECK_RET(aclInit(NULL));
    CV_LOG_INFO(NULL, ""[Success] initialized CANN"");
}

AclEnvGuard::~AclEnvGuard()
{
    CV_LOG_INFO(NULL, ""Start to finalize CANN"");
    ACL_CHECK_RET(aclFinalize());
    CV_LOG_INFO(NULL, ""[Success] finalized CANN"");
}

std::shared_ptr<AclEnvGuard> AclEnvGuard::GetAclEnv()
{
    std::shared_ptr<AclEnvGuard> acl_env;

    std::lock_guard<std::mutex> lock(global_acl_env_mutex_);
    acl_env = global_acl_env_;
    if (acl_env != nullptr)
    {
        CV_LOG_INFO(NULL, ""CANN has been initialized. Skipping..."");
    }
    else
    {
        acl_env = std::make_shared<AclEnvGuard>();
        global_acl_env_ = acl_env;
    }
    return acl_env;
}

CannConstOp::CannConstOp(const uint8_t* data, const int dtype, const std::vect...",16,src\op_cann.cpp,cv.dnn,15,dnn,1
445873,NAMESPACE_BLOCK,<empty>,,src\op_cann.hpp,src\op_cann.hpp:<global>,,<global>,1
445877,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

#ifdef HAVE_CANN

CV__DNN_INLINE_NS_BEGIN

void switchToCannBackend(Net& net);

CV__DNN_INLINE_NS_END

    class CannNet;

    class AclEnvGuard {
    public:
        explicit AclEnvGuard();
        ~AclEnvGuard();
        static std::shared_ptr<AclEnvGuard> GetAclEnv();

    private:
        static std::shared_ptr<AclEnvGuard> global_acl_env_;
        static std::mutex global_acl_env_mutex_;
    };

    class CannConstOp
    {
    public:
        CannConstOp(const uint8_t* data, const int dtype, const std::vector<int>& shape, const std::string& name);
        std::shared_ptr<ge::op::Const> getOp() { return op_; }
        std::shared_ptr<ge::TensorDesc> getTensorDesc() { return desc_; }
    private:
        std::shared_ptr<ge::op::Const> op_;
        std::shared_ptr<ge::TensorDesc> desc_;
    };

    class CannBackendNode : public BackendNode
    {
    public:
        CannBackendNode(const std::shared_ptr<ge::Operator>& op);
        std::shared_ptr<ge...",1,src\op_cann.hpp,cv,55,cv,1
445878,NAMESPACE_BLOCK,"namespace dnn {

#ifdef HAVE_CANN

CV__DNN_INLINE_NS_BEGIN

void switchToCannBackend(Net& net);

CV__DNN_INLINE_NS_END

    class CannNet;

    class AclEnvGuard {
    public:
        explicit AclEnvGuard();
        ~AclEnvGuard();
        static std::shared_ptr<AclEnvGuard> GetAclEnv();

    private:
        static std::shared_ptr<AclEnvGuard> global_acl_env_;
        static std::mutex global_acl_env_mutex_;
    };

    class CannConstOp
    {
    public:
        CannConstOp(const uint8_t* data, const int dtype, const std::vector<int>& shape, const std::string& name);
        std::shared_ptr<ge::op::Const> getOp() { return op_; }
        std::shared_ptr<ge::TensorDesc> getTensorDesc() { return desc_; }
    private:
        std::shared_ptr<ge::op::Const> op_;
        std::shared_ptr<ge::TensorDesc> desc_;
    };

    class CannBackendNode : public BackendNode
    {
    public:
        CannBackendNode(const std::shared_ptr<ge::Operator>& op);
        std::shared_ptr<ge::Operator> get...",16,src\op_cann.hpp,cv.dnn,55,dnn,1
445888,NAMESPACE_BLOCK,<empty>,,src\op_cuda.cpp,src\op_cuda.cpp:<global>,,<global>,1
445919,NAMESPACE_BLOCK,<empty>,,src\op_cuda.hpp,src\op_cuda.hpp:<global>,,<global>,1
445923,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

    constexpr bool IS_DNN_CUDA_TARGET(int id) {
        return id == DNN_TARGET_CUDA_FP16 || id == DNN_TARGET_CUDA;
    }

    constexpr bool haveCUDA() {
#ifdef HAVE_CUDA
        return true;
#else
        return false;
#endif
    }

#ifdef HAVE_CUDA
    namespace cuda4dnn { namespace csl {
        struct CSLContext {
            Stream stream;
            cublas::Handle cublas_handle;
            cudnn::Handle cudnn_handle;
        };

        /** @brief creates Tensor object from cv::Mat (only the header is created, i.e. no data is copied)
         *
         * \tparam      T   element type for the tensor
         * \param[in]   mat cv::Mat from which the shape must be inferred
         *
         * \return a Tensor object with the shape of \p mat
         */
        template <class T>
        Tensor<T> makeTensorHeader(const Mat& mat) {
            auto sizes = shape(mat);
            return Tensor<T>(std::begin(sizes), std::end(sizes));
        }...",1,src\op_cuda.hpp,cv,26,cv,1
445924,NAMESPACE_BLOCK,"namespace dnn {

    constexpr bool IS_DNN_CUDA_TARGET(int id) {
        return id == DNN_TARGET_CUDA_FP16 || id == DNN_TARGET_CUDA;
    }

    constexpr bool haveCUDA() {
#ifdef HAVE_CUDA
        return true;
#else
        return false;
#endif
    }

#ifdef HAVE_CUDA
    namespace cuda4dnn { namespace csl {
        struct CSLContext {
            Stream stream;
            cublas::Handle cublas_handle;
            cudnn::Handle cudnn_handle;
        };

        /** @brief creates Tensor object from cv::Mat (only the header is created, i.e. no data is copied)
         *
         * \tparam      T   element type for the tensor
         * \param[in]   mat cv::Mat from which the shape must be inferred
         *
         * \return a Tensor object with the shape of \p mat
         */
        template <class T>
        Tensor<T> makeTensorHeader(const Mat& mat) {
            auto sizes = shape(mat);
            return Tensor<T>(std::begin(sizes), std::end(sizes));
        }

        /** @...",16,src\op_cuda.hpp,cv.dnn,26,dnn,1
445959,NAMESPACE_BLOCK,<empty>,,src\op_halide.cpp,src\op_halide.cpp:<global>,,<global>,1
445963,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


void Net::Impl::setHalideScheduler(const String& scheduler)
{
    halideConfigFile = scheduler;
}


#ifdef HAVE_HALIDE


void Net::Impl::compileHalide()
{
    CV_TRACE_FUNCTION();

    CV_Assert(preferableBackend == DNN_BACKEND_HALIDE);

    HalideScheduler scheduler(halideConfigFile);
    std::vector< std::reference_wrapper<LayerData> > compileList; compileList.reserve(64);
    for (MapIdToLayerData::iterator it = layers.begin(); it != layers.end(); ++it)
    {
        LayerData& ld = it->second;
        Ptr<Layer> layer = ld.layerInstance;
        if (layer->supportBackend(DNN_BACKEND_HALIDE) && !ld.skip)
        {
            CV_Assert(!ld.backendNodes[DNN_BACKEND_HALIDE].empty());
            bool scheduled = scheduler.process(ld.backendNodes[DNN_BACKEND_HALIDE]);
            if (!scheduled)
            {
                // Use automatic scheduling provided by layer.
                layer->applyHalideScheduler(ld.backendNo...",1,src\op_halide.cpp,cv,20,cv,1
445964,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


void Net::Impl::setHalideScheduler(const String& scheduler)
{
    halideConfigFile = scheduler;
}


#ifdef HAVE_HALIDE


void Net::Impl::compileHalide()
{
    CV_TRACE_FUNCTION();

    CV_Assert(preferableBackend == DNN_BACKEND_HALIDE);

    HalideScheduler scheduler(halideConfigFile);
    std::vector< std::reference_wrapper<LayerData> > compileList; compileList.reserve(64);
    for (MapIdToLayerData::iterator it = layers.begin(); it != layers.end(); ++it)
    {
        LayerData& ld = it->second;
        Ptr<Layer> layer = ld.layerInstance;
        if (layer->supportBackend(DNN_BACKEND_HALIDE) && !ld.skip)
        {
            CV_Assert(!ld.backendNodes[DNN_BACKEND_HALIDE].empty());
            bool scheduled = scheduler.process(ld.backendNodes[DNN_BACKEND_HALIDE]);
            if (!scheduled)
            {
                // Use automatic scheduling provided by layer.
                layer->applyHalideScheduler(ld.backendNodes[DNN_BACKEND...",1,src\op_halide.cpp,cv.dnn,21,dnn,1
446068,NAMESPACE_BLOCK,<empty>,,src\op_halide.hpp,src\op_halide.hpp:<global>,,<global>,1
446072,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
#ifdef HAVE_HALIDE
    // Returns four-dimensional buffer with float32 type that wrap cv::Mat data.
    // No data copy here.
    Halide::Buffer<float> wrapToHalideBuffer(const Mat& mat);

    Halide::Buffer<float> wrapToHalideBuffer(const Mat& mat,
                                             const std::vector<int>& shape);

    // Extract batch size, number of channels, width and height from buffer.
    void getCanonicalSize(const Halide::Buffer<>& buffer, int* width, int* height,
                          int* channels, int* batch);

    // Cast pointer and create copy of Halide buffer. No data copy.
    Halide::Buffer<> halideBuffer(const Ptr<BackendWrapper>& ptr);

    std::vector<Halide::Buffer<> > halideBuffers(const std::vector<Ptr<BackendWrapper> >& ptrs);

    class HalideBackendNode : public BackendNode
    {
    public:
        HalideBackendNode(const Halide::Func& func);

        HalideBackendNode(const std::vector<Halide::Func>& funcs);

...",1,src\op_halide.hpp,cv,22,cv,1
446073,NAMESPACE_BLOCK,"namespace dnn
{
#ifdef HAVE_HALIDE
    // Returns four-dimensional buffer with float32 type that wrap cv::Mat data.
    // No data copy here.
    Halide::Buffer<float> wrapToHalideBuffer(const Mat& mat);

    Halide::Buffer<float> wrapToHalideBuffer(const Mat& mat,
                                             const std::vector<int>& shape);

    // Extract batch size, number of channels, width and height from buffer.
    void getCanonicalSize(const Halide::Buffer<>& buffer, int* width, int* height,
                          int* channels, int* batch);

    // Cast pointer and create copy of Halide buffer. No data copy.
    Halide::Buffer<> halideBuffer(const Ptr<BackendWrapper>& ptr);

    std::vector<Halide::Buffer<> > halideBuffers(const std::vector<Ptr<BackendWrapper> >& ptrs);

    class HalideBackendNode : public BackendNode
    {
    public:
        HalideBackendNode(const Halide::Func& func);

        HalideBackendNode(const std::vector<Halide::Func>& funcs);

        // Init...",1,src\op_halide.hpp,cv.dnn,24,dnn,1
446126,NAMESPACE_BLOCK,<empty>,,src\op_inf_engine.cpp,src\op_inf_engine.cpp:<global>,,<global>,1
446130,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

#ifdef HAVE_INF_ENGINE

CV__DNN_INLINE_NS_BEGIN

cv::String getInferenceEngineBackendType()
{
    return ""NGRAPH"";
}
cv::String setInferenceEngineBackendType(const cv::String& newBackendType)
{
    if (newBackendType != ""NGRAPH"")
        CV_Error(Error::StsNotImplemented, cv::format(""DNN/IE: only NGRAPH backend is supported: %s"", newBackendType.c_str()));
    return newBackendType;
}

CV__DNN_INLINE_NS_END

#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
namespace InferenceEngine {

CNNNetwork::CNNNetwork() {}

CNNNetwork::CNNNetwork(std::shared_ptr<ov::Model> model) : model(model) {}

std::shared_ptr<ov::Model> CNNNetwork::getFunction() const {
    return model;
}

void CNNNetwork::serialize(const std::string& xmlPath, const std::string& binPath) {
    ov::pass::Serialize(xmlPath, binPath).run_on_model(model);
}

void CNNNetwork::reshape(const std::map<std::string, std::vector<size_t> >& shapes) {
    std::map<std::string, ov::PartialShape> pa...",1,src\op_inf_engine.cpp,cv,23,cv,1
446131,NAMESPACE_BLOCK,"namespace dnn {

#ifdef HAVE_INF_ENGINE

CV__DNN_INLINE_NS_BEGIN

cv::String getInferenceEngineBackendType()
{
    return ""NGRAPH"";
}
cv::String setInferenceEngineBackendType(const cv::String& newBackendType)
{
    if (newBackendType != ""NGRAPH"")
        CV_Error(Error::StsNotImplemented, cv::format(""DNN/IE: only NGRAPH backend is supported: %s"", newBackendType.c_str()));
    return newBackendType;
}

CV__DNN_INLINE_NS_END

#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
namespace InferenceEngine {

CNNNetwork::CNNNetwork() {}

CNNNetwork::CNNNetwork(std::shared_ptr<ov::Model> model) : model(model) {}

std::shared_ptr<ov::Model> CNNNetwork::getFunction() const {
    return model;
}

void CNNNetwork::serialize(const std::string& xmlPath, const std::string& binPath) {
    ov::pass::Serialize(xmlPath, binPath).run_on_model(model);
}

void CNNNetwork::reshape(const std::map<std::string, std::vector<size_t> >& shapes) {
    std::map<std::string, ov::PartialShape> partialShapes;
  ...",16,src\op_inf_engine.cpp,cv.dnn,23,dnn,1
446154,NAMESPACE_BLOCK,<empty>,,src\op_inf_engine.hpp,src\op_inf_engine.hpp:<global>,,<global>,1
446158,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

CV__DNN_INLINE_NS_BEGIN
namespace openvino {

// TODO: use std::string as parameter
bool checkTarget(Target target);

}  // namespace openvino
CV__DNN_INLINE_NS_END

#ifdef HAVE_INF_ENGINE

Backend& getInferenceEngineBackendTypeParam();

#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
Mat infEngineBlobToMat(const ov::Tensor& blob);

void infEngineBlobsToMats(const ov::TensorVector& blobs,
                          std::vector<Mat>& mats);
#else
Mat infEngineBlobToMat(const InferenceEngine::Blob::Ptr& blob);

void infEngineBlobsToMats(const std::vector<InferenceEngine::Blob::Ptr>& blobs,
                          std::vector<Mat>& mats);
#endif  // OpenVINO >= 2022.1


CV__DNN_INLINE_NS_BEGIN

void switchToOpenVINOBackend(Net& net);

bool isMyriadX();

bool isArmComputePlugin();

CV__DNN_INLINE_NS_END

// A series of wrappers for classes from OpenVINO API 2.0.
// Need just for less conditional compilation inserts.
#if INF_ENGINE_VER_MAJOR_GE(INF...",1,src\op_inf_engine.hpp,cv,64,cv,1
446159,NAMESPACE_BLOCK,"namespace dnn {

CV__DNN_INLINE_NS_BEGIN
namespace openvino {

// TODO: use std::string as parameter
bool checkTarget(Target target);

}  // namespace openvino
CV__DNN_INLINE_NS_END

#ifdef HAVE_INF_ENGINE

Backend& getInferenceEngineBackendTypeParam();

#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
Mat infEngineBlobToMat(const ov::Tensor& blob);

void infEngineBlobsToMats(const ov::TensorVector& blobs,
                          std::vector<Mat>& mats);
#else
Mat infEngineBlobToMat(const InferenceEngine::Blob::Ptr& blob);

void infEngineBlobsToMats(const std::vector<InferenceEngine::Blob::Ptr>& blobs,
                          std::vector<Mat>& mats);
#endif  // OpenVINO >= 2022.1


CV__DNN_INLINE_NS_BEGIN

void switchToOpenVINOBackend(Net& net);

bool isMyriadX();

bool isArmComputePlugin();

CV__DNN_INLINE_NS_END

// A series of wrappers for classes from OpenVINO API 2.0.
// Need just for less conditional compilation inserts.
#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE...",16,src\op_inf_engine.hpp,cv.dnn,64,dnn,1
446170,NAMESPACE_BLOCK,<empty>,,src\op_timvx.cpp,src\op_timvx.cpp:<global>,,<global>,1
446174,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
#ifdef HAVE_TIMVX

CV__DNN_INLINE_NS_BEGIN

// update all comsumer
void Net::Impl::tvUpdateConfictMap(int graphIndex, LayerData& ld, std::vector<std::vector<int> >& graphConflictMap)
{
    if (ld.consumers.empty())
        return;
    for (int i = 0; i < ld.consumers.size(); i++)
    {
        LayerData &consumerld = layers[ld.consumers[i].lid];
        std::vector<int>::iterator it = std::find(graphConflictMap[ld.consumers[i].lid].begin(),
                                                    graphConflictMap[ld.consumers[i].lid].end(), graphIndex);

        if (it == graphConflictMap[ld.consumers[i].lid].end())
        {
            graphConflictMap[ld.consumers[i].lid].push_back(graphIndex);
            tvUpdateConfictMap(graphIndex, consumerld, graphConflictMap);
        }
        else
            continue;
    }
}

// Convert TRANSIENT to OUTPUT
void Net::Impl::tvConvertToOutputNode(const LayerData& ld, Ptr<TimVXBackendWrapper>& targetWrap)
{
    //...",1,src\op_timvx.cpp,cv,14,cv,1
446175,NAMESPACE_BLOCK,"namespace dnn
{
#ifdef HAVE_TIMVX

CV__DNN_INLINE_NS_BEGIN

// update all comsumer
void Net::Impl::tvUpdateConfictMap(int graphIndex, LayerData& ld, std::vector<std::vector<int> >& graphConflictMap)
{
    if (ld.consumers.empty())
        return;
    for (int i = 0; i < ld.consumers.size(); i++)
    {
        LayerData &consumerld = layers[ld.consumers[i].lid];
        std::vector<int>::iterator it = std::find(graphConflictMap[ld.consumers[i].lid].begin(),
                                                    graphConflictMap[ld.consumers[i].lid].end(), graphIndex);

        if (it == graphConflictMap[ld.consumers[i].lid].end())
        {
            graphConflictMap[ld.consumers[i].lid].push_back(graphIndex);
            tvUpdateConfictMap(graphIndex, consumerld, graphConflictMap);
        }
        else
            continue;
    }
}

// Convert TRANSIENT to OUTPUT
void Net::Impl::tvConvertToOutputNode(const LayerData& ld, Ptr<TimVXBackendWrapper>& targetWrap)
{
    // find right lay...",1,src\op_timvx.cpp,cv.dnn,16,dnn,1
446201,NAMESPACE_BLOCK,<empty>,,src\op_timvx.hpp,src\op_timvx.hpp:<global>,,<global>,1
446205,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
#ifdef HAVE_TIMVX

enum tvActivationType{
    tvActNotSupported = -1,
    tvActReLU,
    tvActReLU6,
    tvActTanH,
    tvActSwish,
    tvActMish,
    tvActSigmoid,
    tvActELU
};

// Data copied from/to Mat to/from Tensor. Change the shape of dst if
// needed to make it the same shape as src.
bool copyToTensor(Ptr<tim::vx::Tensor> &dst, const Mat &src);
bool copyToMat(const Mat &dst, Ptr<tim::vx::Tensor> &src);
tvActivationType getTimVXActType(String & actString);

// Convert Mat shape to TimVX TensorShape
tim::vx::ShapeType getShapeTypeFromMat(const Mat& mat, bool ifConst = false);

// if all value in weight
bool getQuantType(const std::vector<float>& scales, int numOutput = -1);

class TimVXInfo;
class TimVXGraph;
class TimVXBackendNode;
class TimVXBackendWrapper;

// Maintain the tvGraph and tvTensor List. For now, every tvGraph only have one output node, and each node
// in tvGraph has only one output too. It could be optimized in future.
// TODO...",1,src\op_timvx.hpp,cv,23,cv,1
446206,NAMESPACE_BLOCK,"namespace dnn
{
#ifdef HAVE_TIMVX

enum tvActivationType{
    tvActNotSupported = -1,
    tvActReLU,
    tvActReLU6,
    tvActTanH,
    tvActSwish,
    tvActMish,
    tvActSigmoid,
    tvActELU
};

// Data copied from/to Mat to/from Tensor. Change the shape of dst if
// needed to make it the same shape as src.
bool copyToTensor(Ptr<tim::vx::Tensor> &dst, const Mat &src);
bool copyToMat(const Mat &dst, Ptr<tim::vx::Tensor> &src);
tvActivationType getTimVXActType(String & actString);

// Convert Mat shape to TimVX TensorShape
tim::vx::ShapeType getShapeTypeFromMat(const Mat& mat, bool ifConst = false);

// if all value in weight
bool getQuantType(const std::vector<float>& scales, int numOutput = -1);

class TimVXInfo;
class TimVXGraph;
class TimVXBackendNode;
class TimVXBackendWrapper;

// Maintain the tvGraph and tvTensor List. For now, every tvGraph only have one output node, and each node
// in tvGraph has only one output too. It could be optimized in future.
// TODO: tvGraph suppo...",1,src\op_timvx.hpp,cv.dnn,25,dnn,1
446226,NAMESPACE_BLOCK,<empty>,,src\op_vkcom.cpp,src\op_vkcom.cpp:<global>,,<global>,1
446230,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
#ifdef HAVE_VULKAN

CV__DNN_INLINE_NS_BEGIN

void Net::Impl::initVkComBackend()
{
    CV_TRACE_FUNCTION();
    CV_Assert(preferableBackend == DNN_BACKEND_VKCOM);

    context = vkcom::Context::create();

    for (MapIdToLayerData::iterator it = layers.begin(); it != layers.end(); it++)
    {
        LayerData &ld = it->second;
        Ptr<Layer> layer = ld.layerInstance;
        if (!layer->supportBackend(preferableBackend))
        {
            continue;
        }

        try
        {
            ld.backendNodes[DNN_BACKEND_VKCOM] = layer->initVkCom(ld.inputBlobsWrappers, ld.outputBlobsWrappers);
        }
        catch (const cv::Exception& e)
        {
            CV_LOG_ERROR(NULL, ""initVkCom failed, fallback to CPU implementation. "" << e.what());
            ld.backendNodes[DNN_BACKEND_VKCOM] = Ptr<BackendNode>();
        }
    }
}

CV__DNN_INLINE_NS_END


///////////////////////////////////////////////////////////////////////////////
int trans...",1,src\op_vkcom.cpp,cv,13,cv,1
446231,NAMESPACE_BLOCK,"namespace dnn
{
#ifdef HAVE_VULKAN

CV__DNN_INLINE_NS_BEGIN

void Net::Impl::initVkComBackend()
{
    CV_TRACE_FUNCTION();
    CV_Assert(preferableBackend == DNN_BACKEND_VKCOM);

    context = vkcom::Context::create();

    for (MapIdToLayerData::iterator it = layers.begin(); it != layers.end(); it++)
    {
        LayerData &ld = it->second;
        Ptr<Layer> layer = ld.layerInstance;
        if (!layer->supportBackend(preferableBackend))
        {
            continue;
        }

        try
        {
            ld.backendNodes[DNN_BACKEND_VKCOM] = layer->initVkCom(ld.inputBlobsWrappers, ld.outputBlobsWrappers);
        }
        catch (const cv::Exception& e)
        {
            CV_LOG_ERROR(NULL, ""initVkCom failed, fallback to CPU implementation. "" << e.what());
            ld.backendNodes[DNN_BACKEND_VKCOM] = Ptr<BackendNode>();
        }
    }
}

CV__DNN_INLINE_NS_END


///////////////////////////////////////////////////////////////////////////////
int transFusedActivType(...",1,src\op_vkcom.cpp,cv.dnn,15,dnn,1
446249,NAMESPACE_BLOCK,<empty>,,src\op_vkcom.hpp,src\op_vkcom.hpp:<global>,,<global>,1
446253,NAMESPACE_BLOCK,"namespace cv
{
namespace dnn
{
#ifdef HAVE_VULKAN
std::vector<vkcom::Tensor> VkComTensors(const std::vector<Ptr<BackendWrapper> >& ptrs);

vkcom::Tensor VkComTensor(const Ptr<BackendWrapper>& ptr);

// the input is the OpenCV activation layer, and the output is the activation in Vulkan backend.
int transFusedActivType(Ptr<ActivationLayer> &actLayer);

// Data copied from/to Mat to/from Tensor. Change the shape of dst if
// needed to make it the same shape as src
void copyToMat(Mat &dst, const vkcom::Tensor &src);
void copyToTensor(vkcom::Tensor &dst, const Mat &src);

void printTensor(vkcom::Tensor &dst);

// VkComBackendNode contains the input and output of a layer/op.
// And the specific weight and the parameter information of the layer will be saved in the Op instance.
class VkComBackendNode : public BackendNode
{
public:
    VkComBackendNode(const std::vector<Ptr<BackendWrapper> >& inputsWrapper,
                     const Ptr<vkcom::OpBase>& op,
                     const std::...",1,src\op_vkcom.hpp,cv,16,cv,1
446254,NAMESPACE_BLOCK,"namespace dnn
{
#ifdef HAVE_VULKAN
std::vector<vkcom::Tensor> VkComTensors(const std::vector<Ptr<BackendWrapper> >& ptrs);

vkcom::Tensor VkComTensor(const Ptr<BackendWrapper>& ptr);

// the input is the OpenCV activation layer, and the output is the activation in Vulkan backend.
int transFusedActivType(Ptr<ActivationLayer> &actLayer);

// Data copied from/to Mat to/from Tensor. Change the shape of dst if
// needed to make it the same shape as src
void copyToMat(Mat &dst, const vkcom::Tensor &src);
void copyToTensor(vkcom::Tensor &dst, const Mat &src);

void printTensor(vkcom::Tensor &dst);

// VkComBackendNode contains the input and output of a layer/op.
// And the specific weight and the parameter information of the layer will be saved in the Op instance.
class VkComBackendNode : public BackendNode
{
public:
    VkComBackendNode(const std::vector<Ptr<BackendWrapper> >& inputsWrapper,
                     const Ptr<vkcom::OpBase>& op,
                     const std::vector<Ptr<Back...",1,src\op_vkcom.hpp,cv.dnn,18,dnn,1
446284,NAMESPACE_BLOCK,<empty>,,src\op_webnn.cpp,src\op_webnn.cpp:<global>,,<global>,1
446288,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

#ifdef HAVE_WEBNN

CV__DNN_INLINE_NS_BEGIN


void Net::Impl::addWebnnOutputs(LayerData &ld)
{
    CV_TRACE_FUNCTION();

    Ptr<WebnnNet> layerNet;
    auto it = ld.backendNodes.find(preferableBackend);
    if (it != ld.backendNodes.end())
    {
        Ptr<BackendNode> node = it->second;
        if (!node.empty())
        {
            Ptr<WebnnBackendNode> webnnNode = node.dynamicCast<WebnnBackendNode>();
            CV_Assert(!webnnNode.empty()); CV_Assert(!webnnNode->net.empty());
            layerNet = webnnNode->net;
        }
    }

    for (int i = 0; i < ld.inputBlobsId.size(); ++i)
    {
        LayerData &inpLd = layers[ld.inputBlobsId[i].lid];
        Ptr<BackendNode> inpNode = inpLd.backendNodes[preferableBackend];
        if (!inpNode.empty())
        {
            Ptr<WebnnBackendNode> webnnInpNode = inpNode.dynamicCast<WebnnBackendNode>();
            CV_Assert(!webnnInpNode.empty()); CV_Assert(!webnnInpNode->net.empty());
            ...",1,src\op_webnn.cpp,cv,19,cv,1
446289,NAMESPACE_BLOCK,"namespace dnn {

#ifdef HAVE_WEBNN

CV__DNN_INLINE_NS_BEGIN


void Net::Impl::addWebnnOutputs(LayerData &ld)
{
    CV_TRACE_FUNCTION();

    Ptr<WebnnNet> layerNet;
    auto it = ld.backendNodes.find(preferableBackend);
    if (it != ld.backendNodes.end())
    {
        Ptr<BackendNode> node = it->second;
        if (!node.empty())
        {
            Ptr<WebnnBackendNode> webnnNode = node.dynamicCast<WebnnBackendNode>();
            CV_Assert(!webnnNode.empty()); CV_Assert(!webnnNode->net.empty());
            layerNet = webnnNode->net;
        }
    }

    for (int i = 0; i < ld.inputBlobsId.size(); ++i)
    {
        LayerData &inpLd = layers[ld.inputBlobsId[i].lid];
        Ptr<BackendNode> inpNode = inpLd.backendNodes[preferableBackend];
        if (!inpNode.empty())
        {
            Ptr<WebnnBackendNode> webnnInpNode = inpNode.dynamicCast<WebnnBackendNode>();
            CV_Assert(!webnnInpNode.empty()); CV_Assert(!webnnInpNode->net.empty());
            if (layerNet !=...",16,src\op_webnn.cpp,cv.dnn,19,dnn,1
446326,NAMESPACE_BLOCK,<empty>,,src\op_webnn.hpp,src\op_webnn.hpp:<global>,,<global>,1
446330,NAMESPACE_BLOCK,"namespace cv { namespace dnn {

constexpr bool haveWebnn() {
#ifdef HAVE_WEBNN
        return true;
#else
        return false;
#endif
}

#ifdef HAVE_WEBNN

class WebnnBackendNode;
class WebnnBackendWrapper;

namespace webnn {
inline std::vector<int32_t> getShape(const Mat& mat)
{
    std::vector<int32_t> result(mat.dims);
    for (int i = 0; i < mat.dims; i++)
        result[i] = (int32_t)mat.size[i];
    return result;
}

ml::Operand BuildConstant(const ml::GraphBuilder& builder,
                              const std::vector<int32_t>& dimensions,
                              const void* value,
                              size_t size,
                              ml::OperandType type);

struct Pool2dOptions {
    public:
        std::vector<int32_t> windowDimensions;
        std::vector<int32_t> padding;
        std::vector<int32_t> strides;
        std::vector<int32_t> dilations;
        ml::AutoPad autoPad = ml::AutoPad::Explicit;
        ml::InputOperandLayout layout = ml:...",1,src\op_webnn.hpp,cv,30,cv,1
446331,NAMESPACE_BLOCK,"namespace dnn {

constexpr bool haveWebnn() {
#ifdef HAVE_WEBNN
        return true;
#else
        return false;
#endif
}

#ifdef HAVE_WEBNN

class WebnnBackendNode;
class WebnnBackendWrapper;

namespace webnn {
inline std::vector<int32_t> getShape(const Mat& mat)
{
    std::vector<int32_t> result(mat.dims);
    for (int i = 0; i < mat.dims; i++)
        result[i] = (int32_t)mat.size[i];
    return result;
}

ml::Operand BuildConstant(const ml::GraphBuilder& builder,
                              const std::vector<int32_t>& dimensions,
                              const void* value,
                              size_t size,
                              ml::OperandType type);

struct Pool2dOptions {
    public:
        std::vector<int32_t> windowDimensions;
        std::vector<int32_t> padding;
        std::vector<int32_t> strides;
        std::vector<int32_t> dilations;
        ml::AutoPad autoPad = ml::AutoPad::Explicit;
        ml::InputOperandLayout layout = ml::InputOperandLa...",16,src\op_webnn.hpp,cv.dnn,30,dnn,1
446352,NAMESPACE_BLOCK,<empty>,,src\plugin_api.hpp,src\plugin_api.hpp:<global>,,<global>,1
446369,NAMESPACE_BLOCK,<empty>,,src\plugin_wrapper.impl.hpp,src\plugin_wrapper.impl.hpp:<global>,,<global>,1
446373,NAMESPACE_BLOCK,"namespace cv { namespace impl {

using namespace cv::dnn_backend;

#if OPENCV_HAVE_FILESYSTEM_SUPPORT && defined(ENABLE_PLUGINS)

using namespace cv::plugin::impl;  // plugin_loader.hpp

class PluginDNNBackend CV_FINAL: public std::enable_shared_from_this<PluginDNNBackend>
{
protected:
    void initPluginAPI()
    {
        const char* init_name = ""opencv_dnn_plugin_init_v0"";
        FN_opencv_dnn_plugin_init_t fn_init = reinterpret_cast<FN_opencv_dnn_plugin_init_t>(lib_->getSymbol(init_name));
        if (fn_init)
        {
            CV_LOG_DEBUG(NULL, ""Found entry: '"" << init_name << ""'"");
            for (int supported_api_version = API_VERSION; supported_api_version >= 0; supported_api_version--)
            {
                plugin_api_ = fn_init(ABI_VERSION, supported_api_version, NULL);
                if (plugin_api_)
                    break;
            }
            if (!plugin_api_)
            {
                CV_LOG_INFO(NULL, ""DNN: plugin is incompatible (can't be...",1,src\plugin_wrapper.impl.hpp,cv,14,cv,1
446374,NAMESPACE_BLOCK,"namespace impl {

using namespace cv::dnn_backend;

#if OPENCV_HAVE_FILESYSTEM_SUPPORT && defined(ENABLE_PLUGINS)

using namespace cv::plugin::impl;  // plugin_loader.hpp

class PluginDNNBackend CV_FINAL: public std::enable_shared_from_this<PluginDNNBackend>
{
protected:
    void initPluginAPI()
    {
        const char* init_name = ""opencv_dnn_plugin_init_v0"";
        FN_opencv_dnn_plugin_init_t fn_init = reinterpret_cast<FN_opencv_dnn_plugin_init_t>(lib_->getSymbol(init_name));
        if (fn_init)
        {
            CV_LOG_DEBUG(NULL, ""Found entry: '"" << init_name << ""'"");
            for (int supported_api_version = API_VERSION; supported_api_version >= 0; supported_api_version--)
            {
                plugin_api_ = fn_init(ABI_VERSION, supported_api_version, NULL);
                if (plugin_api_)
                    break;
            }
            if (!plugin_api_)
            {
                CV_LOG_INFO(NULL, ""DNN: plugin is incompatible (can't be initialized): ...",16,src\plugin_wrapper.impl.hpp,cv.impl,14,impl,1
446375,NAMESPACE_BLOCK,"namespace dnn_backend {


std::shared_ptr<IDNNBackendFactory> createPluginDNNBackendFactory(const std::string& baseName)
{
#if OPENCV_HAVE_FILESYSTEM_SUPPORT && defined(ENABLE_PLUGINS)
    const std::string baseName_u = toUpperCase(baseName);
    AutoLock lock(getInitializationMutex());
    static std::map<std::string, std::shared_ptr<IDNNBackendFactory>> g_plugins_cache;
    auto it = g_plugins_cache.find(baseName_u);
    if (it == g_plugins_cache.end())
    {
        auto factory = std::make_shared<impl::PluginDNNBackendFactory>(baseName);
        g_plugins_cache.insert(std::pair<std::string, std::shared_ptr<IDNNBackendFactory>>(baseName_u, factory));
        return factory;
    }
    return it->second;
#else
    CV_UNUSED(baseName);
    return std::shared_ptr<IDNNBackendFactory>();
#endif
}


cv::dnn_backend::NetworkBackend& createPluginDNNNetworkBackend(const std::string& baseName)
{
    auto factory = dnn_backend::createPluginDNNBackendFactory(baseName);
    if (!factory)
    {...",1,src\plugin_wrapper.impl.hpp,cv.dnn_backend,279,dnn_backend,2
446482,NAMESPACE_BLOCK,<empty>,,src\precomp.hpp,src\precomp.hpp:<global>,,<global>,1
446511,NAMESPACE_BLOCK,<empty>,,src\registry.cpp,src\registry.cpp:<global>,,<global>,1
446515,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN


class BackendRegistry
{
public:
    typedef std::vector< std::pair<Backend, Target> > BackendsList;
    const BackendsList & getBackends() const { return backends; }
    static BackendRegistry & getRegistry()
    {
        static BackendRegistry impl;
        return impl;
    }


private:
    BackendRegistry()
    {
#ifdef HAVE_HALIDE
        backends.push_back(std::make_pair(DNN_BACKEND_HALIDE, DNN_TARGET_CPU));
#ifdef HAVE_OPENCL
        if (cv::ocl::useOpenCL())
            backends.push_back(std::make_pair(DNN_BACKEND_HALIDE, DNN_TARGET_OPENCL));
#endif
#endif  // HAVE_HALIDE

        bool haveBackendOpenVINO = false;
#ifdef HAVE_INF_ENGINE
        haveBackendOpenVINO = true;
#elif defined(ENABLE_PLUGINS)
        {
            auto factory = dnn_backend::createPluginDNNBackendFactory(""openvino"");
            if (factory)
            {
                auto backend = factory->createNetworkBackend();
                if (backe...",1,src\registry.cpp,cv,21,cv,1
446516,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN


class BackendRegistry
{
public:
    typedef std::vector< std::pair<Backend, Target> > BackendsList;
    const BackendsList & getBackends() const { return backends; }
    static BackendRegistry & getRegistry()
    {
        static BackendRegistry impl;
        return impl;
    }


private:
    BackendRegistry()
    {
#ifdef HAVE_HALIDE
        backends.push_back(std::make_pair(DNN_BACKEND_HALIDE, DNN_TARGET_CPU));
#ifdef HAVE_OPENCL
        if (cv::ocl::useOpenCL())
            backends.push_back(std::make_pair(DNN_BACKEND_HALIDE, DNN_TARGET_OPENCL));
#endif
#endif  // HAVE_HALIDE

        bool haveBackendOpenVINO = false;
#ifdef HAVE_INF_ENGINE
        haveBackendOpenVINO = true;
#elif defined(ENABLE_PLUGINS)
        {
            auto factory = dnn_backend::createPluginDNNBackendFactory(""openvino"");
            if (factory)
            {
                auto backend = factory->createNetworkBackend();
                if (backend)
           ...",1,src\registry.cpp,cv.dnn,22,dnn,1
446527,NAMESPACE_BLOCK,<empty>,,src\tensorflow\tf_graph_simplifier.cpp,src\tensorflow\tf_graph_simplifier.cpp:<global>,,<global>,1
446534,NAMESPACE_BLOCK,<empty>,,src\tensorflow\tf_graph_simplifier.hpp,src\tensorflow\tf_graph_simplifier.hpp:<global>,,<global>,1
446563,NAMESPACE_BLOCK,<empty>,,src\tensorflow\tf_importer.cpp,src\tensorflow\tf_importer.cpp:<global>,,<global>,1
446567,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

extern bool DNN_DIAGNOSTICS_RUN;

#ifdef HAVE_PROTOBUF

using ::google::protobuf::RepeatedField;
using ::google::protobuf::RepeatedPtrField;
using ::google::protobuf::Message;
using ::google::protobuf::Descriptor;
using ::google::protobuf::FieldDescriptor;
using ::google::protobuf::Reflection;

namespace
{

static int toNCHW(int idx)
{
    CV_Assert(-4 <= idx && idx < 4);
    if (idx == 0) return 0;
    else if (idx > 0) return idx % 3 + 1;
    else return (4 + idx) % 3 + 1;
}

static int toNCDHW(int idx)
{
    CV_Assert(-5 <= idx && idx < 5);
    if (idx == 0) return 0;
    else if (idx > 0) return idx % 4 + 1;
    else return (5 + idx) % 4 + 1;
}

typedef std::vector<std::pair<String, int> > StrIntVector;

struct Pin
{
    Pin(const std::string &_name, int _blobIndex = 0) :
        name(_name), blobIndex(_blobIndex) {}

    Pin() :
        name(""""), blobIndex(-1) {}

    std::string name;
    int blobIndex;
};

void blobShape...",1,src\tensorflow\tf_importer.cpp,cv,33,cv,1
446568,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

extern bool DNN_DIAGNOSTICS_RUN;

#ifdef HAVE_PROTOBUF

using ::google::protobuf::RepeatedField;
using ::google::protobuf::RepeatedPtrField;
using ::google::protobuf::Message;
using ::google::protobuf::Descriptor;
using ::google::protobuf::FieldDescriptor;
using ::google::protobuf::Reflection;

namespace
{

static int toNCHW(int idx)
{
    CV_Assert(-4 <= idx && idx < 4);
    if (idx == 0) return 0;
    else if (idx > 0) return idx % 3 + 1;
    else return (4 + idx) % 3 + 1;
}

static int toNCDHW(int idx)
{
    CV_Assert(-5 <= idx && idx < 5);
    if (idx == 0) return 0;
    else if (idx > 0) return idx % 4 + 1;
    else return (5 + idx) % 4 + 1;
}

typedef std::vector<std::pair<String, int> > StrIntVector;

struct Pin
{
    Pin(const std::string &_name, int _blobIndex = 0) :
        name(_name), blobIndex(_blobIndex) {}

    Pin() :
        name(""""), blobIndex(-1) {}

    std::string name;
    int blobIndex;
};

void blobShapeFromTensor(cons...",1,src\tensorflow\tf_importer.cpp,cv.dnn,34,dnn,1
446651,NAMESPACE_BLOCK,<empty>,,src\tensorflow\tf_io.cpp,src\tensorflow\tf_io.cpp:<global>,,<global>,1
446664,NAMESPACE_BLOCK,<empty>,,src\tensorflow\tf_io.hpp,src\tensorflow\tf_io.hpp:<global>,,<global>,1
446669,NAMESPACE_BLOCK,<empty>,,src\tflite\builtin_op_data.hpp,src\tflite\builtin_op_data.hpp:<global>,,<global>,1
446734,NAMESPACE_BLOCK,<empty>,,src\tflite\tflite_importer.cpp,src\tflite\tflite_importer.cpp:<global>,,<global>,1
446738,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_FLATBUFFERS

using namespace opencv_tflite;

class TFLiteImporter {
public:
    TFLiteImporter(Net& net, const char* modelBuffer, size_t bufSize);

private:
    const opencv_tflite::Model* model;
    const flatbuffers::Vector<flatbuffers::Offset<opencv_tflite::Tensor> >* modelTensors;
    std::map<int, Mat> allTensors;
    Net& dstNet;

    // This is a vector of pairs (layerId, outputId) where we iterate over
    // indices from TFLite notation and get created OpenCV layers.
    std::map<int, std::pair<int, int> > layerIds;

    // Tracking of layouts for layers outputs.
    std::vector<DataLayout> layouts;

    void populateNet();

    // Wrap TFLite Tensor to OpenCV Mat without data copying
    Mat parseTensor(const Tensor& tensor);

    typedef void (TFLiteImporter::*TFLiteImporterNodeParser)(const Operator&, const std::string&, LayerParams&);
    typedef std::map<std::string, TFLiteImporterNodeParser> DispatchM...",1,src\tflite\tflite_importer.cpp,cv,17,cv,1
446739,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

#ifdef HAVE_FLATBUFFERS

using namespace opencv_tflite;

class TFLiteImporter {
public:
    TFLiteImporter(Net& net, const char* modelBuffer, size_t bufSize);

private:
    const opencv_tflite::Model* model;
    const flatbuffers::Vector<flatbuffers::Offset<opencv_tflite::Tensor> >* modelTensors;
    std::map<int, Mat> allTensors;
    Net& dstNet;

    // This is a vector of pairs (layerId, outputId) where we iterate over
    // indices from TFLite notation and get created OpenCV layers.
    std::map<int, std::pair<int, int> > layerIds;

    // Tracking of layouts for layers outputs.
    std::vector<DataLayout> layouts;

    void populateNet();

    // Wrap TFLite Tensor to OpenCV Mat without data copying
    Mat parseTensor(const Tensor& tensor);

    typedef void (TFLiteImporter::*TFLiteImporterNodeParser)(const Operator&, const std::string&, LayerParams&);
    typedef std::map<std::string, TFLiteImporterNodeParser> DispatchMap;

    const ...",1,src\tflite\tflite_importer.cpp,cv.dnn,18,dnn,1
446788,NAMESPACE_BLOCK,<empty>,,src\torch\THDiskFile.cpp,src\torch\THDiskFile.cpp:<global>,,<global>,1
446792,NAMESPACE_BLOCK,"namespace TH
{

typedef struct THDiskFile__
{
    THFile file;

    FILE *handle;
    int isNativeEncoding;
    int longSize;

} THDiskFile;

static int THDiskFile_isOpened(THFile *self)
{
  THDiskFile *dfself = (THDiskFile*)self;
  return (dfself->handle != NULL);
}

/* workaround mac osx lion ***insane*** fread bug */
#ifdef __APPLE__
static size_t fread__(void *ptr, size_t size, size_t nitems, FILE *stream)
{
  size_t nread = 0;
  while(!feof(stream) && !ferror(stream) && (nread < nitems))
    nread += fread((char*)ptr+nread*size, size, std::min<size_t>(2147483648UL/size, nitems-nread), stream);
  return nread;
}
#else
#define fread__ fread
#endif

#define READ_WRITE_METHODS(TYPE, TYPEC, ASCII_READ_ELEM, ASCII_WRITE_ELEM) \
  static long THDiskFile_read##TYPEC(THFile *self, TYPE *data, long n)  \
  {                                                                     \
    THDiskFile *dfself = (THDiskFile*)(self);                           \
    long nread = 0L;                  ...",1,src\torch\THDiskFile.cpp,TH,6,TH,1
449355,NAMESPACE_BLOCK,<empty>,,src\torch\THDiskFile.hpp,src\torch\THDiskFile.hpp:<global>,,<global>,1
449359,NAMESPACE_BLOCK,"namespace TH
{

TH_API THFile *THDiskFile_new(const std::string &name, const char *mode, int isQuiet);

TH_API int THDiskFile_isLittleEndianCPU(void);
TH_API int THDiskFile_isBigEndianCPU(void);
TH_API void THDiskFile_nativeEndianEncoding(THFile *self);
TH_API void THDiskFile_littleEndianEncoding(THFile *self);
TH_API void THDiskFile_bigEndianEncoding(THFile *self);
TH_API void THDiskFile_longSize(THFile *self, int size);
TH_API void THDiskFile_noBuffer(THFile *self);

}",1,src\torch\THDiskFile.hpp,TH,7,TH,1
449410,NAMESPACE_BLOCK,<empty>,,src\torch\THFile.cpp,src\torch\THFile.cpp:<global>,,<global>,1
449414,NAMESPACE_BLOCK,"namespace TH {

#define IMPLEMENT_THFILE_RW(TYPEC, TYPE)                          \
  long THFile_read##TYPEC##Raw(THFile *self, TYPE *data, long n)  \
  {                                                               \
    return (*self->vtable->read##TYPEC)(self, data, n);           \
  }

IMPLEMENT_THFILE_RW(Byte, unsigned char)
IMPLEMENT_THFILE_RW(Char, char)
IMPLEMENT_THFILE_RW(Short, short)
IMPLEMENT_THFILE_RW(Int, int)
IMPLEMENT_THFILE_RW(Long, int64)
IMPLEMENT_THFILE_RW(Float, float)
IMPLEMENT_THFILE_RW(Double, double)

long THFile_readStringRaw(THFile *self, const char *format, char **str_)
{
  return self->vtable->readString(self, format, str_);
}

void THFile_seek(THFile *self, long position)
{
  self->vtable->seek(self, position);
}

void THFile_seekEnd(THFile *self)
{
  self->vtable->seekEnd(self);
}

long THFile_position(THFile *self)
{
  return self->vtable->position(self);
}

void THFile_close(THFile *self)
{
  self->vtable->close(self);
}

void THFile_free(THFile *s...",1,src\torch\THFile.cpp,TH,5,TH,1
449854,NAMESPACE_BLOCK,<empty>,,src\torch\THFile.hpp,src\torch\THFile.hpp:<global>,,<global>,1
449858,NAMESPACE_BLOCK,"namespace TH
{
typedef struct THFile__ THFile;

TH_API int THFile_isOpened(THFile *self);
TH_API int THFile_isQuiet(THFile *self);
TH_API int THFile_isReadable(THFile *self);
TH_API int THFile_isWritable(THFile *self);
TH_API int THFile_isBinary(THFile *self);
TH_API int THFile_isAutoSpacing(THFile *self);
TH_API int THFile_hasError(THFile *self);

TH_API void THFile_binary(THFile *self);
TH_API void THFile_ascii(THFile *self);
TH_API void THFile_autoSpacing(THFile *self);
TH_API void THFile_noAutoSpacing(THFile *self);
TH_API void THFile_quiet(THFile *self);
TH_API void THFile_pedantic(THFile *self);
TH_API void THFile_clearError(THFile *self);

/* scalar */
TH_API unsigned char THFile_readByteScalar(THFile *self);
TH_API char THFile_readCharScalar(THFile *self);
TH_API short THFile_readShortScalar(THFile *self);
TH_API int THFile_readIntScalar(THFile *self);
TH_API int64 THFile_readLongScalar(THFile *self);
TH_API float THFile_readFloatScalar(THFile *self);
TH_API double THFile_re...",1,src\torch\THFile.hpp,TH,8,TH,1
450049,NAMESPACE_BLOCK,<empty>,,src\torch\THFilePrivate.hpp,src\torch\THFilePrivate.hpp:<global>,,<global>,1
450053,NAMESPACE_BLOCK,"namespace TH {

struct THFile__
{
    struct THFileVTable *vtable;

    int isQuiet;
    int isReadable;
    int isWritable;
    int isBinary;
    int isAutoSpacing;
    int hasError;
};

/* virtual table definition */

struct THFileVTable
{
    int (*isOpened)(THFile *self);

    long (*readByte)(THFile *self, unsigned char *data, long n);
    long (*readChar)(THFile *self, char *data, long n);
    long (*readShort)(THFile *self, short *data, long n);
    long (*readInt)(THFile *self, int *data, long n);
    long (*readLong)(THFile *self, int64 *data, long n);
    long (*readFloat)(THFile *self, float *data, long n);
    long (*readDouble)(THFile *self, double *data, long n);
    long (*readString)(THFile *self, const char *format, char **str_);

    void (*seek)(THFile *self, long position);
    void (*seekEnd)(THFile *self);
    long (*position)(THFile *self);
    void (*close)(THFile *self);
    void (*free)(THFile *self);
};

}",1,src\torch\THFilePrivate.hpp,TH,1,TH,1
450156,NAMESPACE_BLOCK,<empty>,,src\torch\THGeneral.cpp,src\torch\THGeneral.cpp:<global>,,<global>,1
450177,NAMESPACE_BLOCK,<empty>,,src\torch\THGeneral.hpp,src\torch\THGeneral.hpp:<global>,,<global>,1
450200,NAMESPACE_BLOCK,<empty>,,src\torch\torch_importer.cpp,src\torch\torch_importer.cpp:<global>,,<global>,1
450204,NAMESPACE_BLOCK,"namespace cv {
namespace dnn {
CV__DNN_INLINE_NS_BEGIN

using namespace TH;

//#ifdef NDEBUG
static bool dbgPrint = false;
//#else
//static bool dbgPrint = true;
//#endif

enum LuaType
{
    TYPE_NIL      = 0,
    TYPE_NUMBER   = 1,
    TYPE_STRING   = 2,
    TYPE_TABLE    = 3,
    TYPE_TORCH    = 4,
    TYPE_BOOLEAN  = 5,
    TYPE_FUNCTION = 6,
    TYPE_RECUR_FUNCTION = 8,
    LEGACY_TYPE_RECUR_FUNCTION = 7
};

// We use OpenCV's types to manage CV_ELEM_SIZE.
enum TorchType
{
    TYPE_DOUBLE = CV_64F,
    TYPE_FLOAT  = CV_32F,
    TYPE_BYTE   = CV_8U,
    TYPE_CHAR   = CV_8S,
    TYPE_SHORT  = CV_16S,
    TYPE_INT    = CV_32S,
    TYPE_LONG   = CV_32SC2
};

template<typename T>
static String toString(const T &v)
{
    std::ostringstream ss;
    ss << v;
    return ss.str();
}

static inline bool startsWith(const String &str, const char *substr)
{
    return str.find(substr) == 0;
}

static inline bool endsWith(const String &str, const char *substr)
{
    return str.rfind(substr) ==...",1,src\torch\torch_importer.cpp,cv,55,cv,1
450205,NAMESPACE_BLOCK,"namespace dnn {
CV__DNN_INLINE_NS_BEGIN

using namespace TH;

//#ifdef NDEBUG
static bool dbgPrint = false;
//#else
//static bool dbgPrint = true;
//#endif

enum LuaType
{
    TYPE_NIL      = 0,
    TYPE_NUMBER   = 1,
    TYPE_STRING   = 2,
    TYPE_TABLE    = 3,
    TYPE_TORCH    = 4,
    TYPE_BOOLEAN  = 5,
    TYPE_FUNCTION = 6,
    TYPE_RECUR_FUNCTION = 8,
    LEGACY_TYPE_RECUR_FUNCTION = 7
};

// We use OpenCV's types to manage CV_ELEM_SIZE.
enum TorchType
{
    TYPE_DOUBLE = CV_64F,
    TYPE_FLOAT  = CV_32F,
    TYPE_BYTE   = CV_8U,
    TYPE_CHAR   = CV_8S,
    TYPE_SHORT  = CV_16S,
    TYPE_INT    = CV_32S,
    TYPE_LONG   = CV_32SC2
};

template<typename T>
static String toString(const T &v)
{
    std::ostringstream ss;
    ss << v;
    return ss.str();
}

static inline bool startsWith(const String &str, const char *substr)
{
    return str.find(substr) == 0;
}

static inline bool endsWith(const String &str, const char *substr)
{
    return str.rfind(substr) == str.length() -...",1,src\torch\torch_importer.cpp,cv.dnn,56,dnn,1
455239,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\buffer.hpp,src\vkcom\include\buffer.hpp:<global>,,<global>,1
455243,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

class Buffer
{
public:
    Buffer(VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    Buffer(size_t size_in_bytes, const char* data, VkBufferUsageFlags usageFlags = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    ~Buffer();
    VkDeviceMemory getVkMemory() { return memory_; }
    VkBuffer getVkBuffer() { return buffer_; }

private:
    bool init(size_t size_in_bytes, const char* data);
    VkBufferUsageFlags usageFlag_;
    VkBuffer buffer_;
    VkDeviceMemory memory_;
};

#endif // HAVE_VULKAN

}}}",1,src\vkcom\include\buffer.hpp,cv,15,cv,1
455244,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

class Buffer
{
public:
    Buffer(VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    Buffer(size_t size_in_bytes, const char* data, VkBufferUsageFlags usageFlags = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    ~Buffer();
    VkDeviceMemory getVkMemory() { return memory_; }
    VkBuffer getVkBuffer() { return buffer_; }

private:
    bool init(size_t size_in_bytes, const char* data);
    VkBufferUsageFlags usageFlag_;
    VkBuffer buffer_;
    VkDeviceMemory memory_;
};

#endif // HAVE_VULKAN

}}",16,src\vkcom\include\buffer.hpp,cv.dnn,15,dnn,1
455245,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

class Buffer
{
public:
    Buffer(VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    Buffer(size_t size_in_bytes, const char* data, VkBufferUsageFlags usageFlags = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    ~Buffer();
    VkDeviceMemory getVkMemory() { return memory_; }
    VkBuffer getVkBuffer() { return buffer_; }

private:
    bool init(size_t size_in_bytes, const char* data);
    VkBufferUsageFlags usageFlag_;
    VkBuffer buffer_;
    VkDeviceMemory memory_;
};

#endif // HAVE_VULKAN

}",32,src\vkcom\include\buffer.hpp,cv.dnn.vkcom,15,vkcom,1
455253,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\command.hpp,src\vkcom\include\command.hpp:<global>,,<global>,1
455257,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

class CommandPool;
// CommandBuffer will record and dispatch the VkCommand, it was allocated from CommandPool.
class CommandBuffer
{
public:
    ~CommandBuffer();

    void beginRecord(VkCommandBufferUsageFlags flag = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT);
    void endRecord();

    enum BarrierType {
        READ_WRITE = 0,
        WRITE_WRITE = 1,
    };
    void barrierSource(VkBuffer source, size_t start, size_t size, BarrierType type = READ_WRITE) const;

    VkCommandBuffer get()
    {
        return cmdBuffer;
    }

private:
    friend class CommandPool;
    CommandBuffer(CommandPool* pool);

    CommandPool* cmdPool;
    VkCommandBuffer cmdBuffer;
    // If is true, the deconstructor will release the instance, otherwise, re-use it.
    bool needRelease = true;
};

class CommandPool
{
public:
    static Ptr<CommandPool> create(const VkQueue& q, uint32_t _queueFamilyIndex);

    void operator=(const ...",1,src\vkcom\include\command.hpp,cv,15,cv,1
455258,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

class CommandPool;
// CommandBuffer will record and dispatch the VkCommand, it was allocated from CommandPool.
class CommandBuffer
{
public:
    ~CommandBuffer();

    void beginRecord(VkCommandBufferUsageFlags flag = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT);
    void endRecord();

    enum BarrierType {
        READ_WRITE = 0,
        WRITE_WRITE = 1,
    };
    void barrierSource(VkBuffer source, size_t start, size_t size, BarrierType type = READ_WRITE) const;

    VkCommandBuffer get()
    {
        return cmdBuffer;
    }

private:
    friend class CommandPool;
    CommandBuffer(CommandPool* pool);

    CommandPool* cmdPool;
    VkCommandBuffer cmdBuffer;
    // If is true, the deconstructor will release the instance, otherwise, re-use it.
    bool needRelease = true;
};

class CommandPool
{
public:
    static Ptr<CommandPool> create(const VkQueue& q, uint32_t _queueFamilyIndex);

    void operator=(const CommandPool &) ...",16,src\vkcom\include\command.hpp,cv.dnn,15,dnn,1
455259,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

class CommandPool;
// CommandBuffer will record and dispatch the VkCommand, it was allocated from CommandPool.
class CommandBuffer
{
public:
    ~CommandBuffer();

    void beginRecord(VkCommandBufferUsageFlags flag = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT);
    void endRecord();

    enum BarrierType {
        READ_WRITE = 0,
        WRITE_WRITE = 1,
    };
    void barrierSource(VkBuffer source, size_t start, size_t size, BarrierType type = READ_WRITE) const;

    VkCommandBuffer get()
    {
        return cmdBuffer;
    }

private:
    friend class CommandPool;
    CommandBuffer(CommandPool* pool);

    CommandPool* cmdPool;
    VkCommandBuffer cmdBuffer;
    // If is true, the deconstructor will release the instance, otherwise, re-use it.
    bool needRelease = true;
};

class CommandPool
{
public:
    static Ptr<CommandPool> create(const VkQueue& q, uint32_t _queueFamilyIndex);

    void operator=(const CommandPool &) = delete;
    Co...",32,src\vkcom\include\command.hpp,cv.dnn.vkcom,15,vkcom,1
455269,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\context.hpp,src\vkcom\include\context.hpp:<global>,,<global>,1
455273,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// NOTE: Manually set true to enable ValidationLayers, default is false.
const bool enableValidationLayers = false;

enum GPU_TYPE {
    GPU_TYPE_NOFOUND = -1,
    GPU_TYPE_DISCRETE = 0,
    GPU_TYPE_INTEGRATED = 1,
    GPU_TYPE_VIRTUAL = 2,
    GPU_TYPE_CPU_ONLY = 3,
};

// GPUInfo will parse GPU hardware information and save it in param.
struct GPUInfo
{
    // memory properties
    VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;

    // basic info
    GPU_TYPE type; // cpu, integrated GPU, discrete GPU.
    uint32_t apiVersion;
    uint32_t driverVersion;
    uint32_t vendorId;
    uint32_t deviceId;
    char deviceName[VK_MAX_PHYSICAL_DEVICE_NAME_SIZE];
    uint8_t pipelineCacheUUID[VK_UUID_SIZE];

    // hardware limit
    uint32_t maxSharedMemorySize;
    uint32_t maxWorkgroupCount_x;
    uint32_t maxWorkgroupCount_y;
    uint32_t maxWorkgroupCount_z;
    uint32_t maxWorkgroup_invocations;
 ...",1,src\vkcom\include\context.hpp,cv,37,cv,1
455274,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// NOTE: Manually set true to enable ValidationLayers, default is false.
const bool enableValidationLayers = false;

enum GPU_TYPE {
    GPU_TYPE_NOFOUND = -1,
    GPU_TYPE_DISCRETE = 0,
    GPU_TYPE_INTEGRATED = 1,
    GPU_TYPE_VIRTUAL = 2,
    GPU_TYPE_CPU_ONLY = 3,
};

// GPUInfo will parse GPU hardware information and save it in param.
struct GPUInfo
{
    // memory properties
    VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;

    // basic info
    GPU_TYPE type; // cpu, integrated GPU, discrete GPU.
    uint32_t apiVersion;
    uint32_t driverVersion;
    uint32_t vendorId;
    uint32_t deviceId;
    char deviceName[VK_MAX_PHYSICAL_DEVICE_NAME_SIZE];
    uint8_t pipelineCacheUUID[VK_UUID_SIZE];

    // hardware limit
    uint32_t maxSharedMemorySize;
    uint32_t maxWorkgroupCount_x;
    uint32_t maxWorkgroupCount_y;
    uint32_t maxWorkgroupCount_z;
    uint32_t maxWorkgroup_invocations;
    uint32_t max...",16,src\vkcom\include\context.hpp,cv.dnn,37,dnn,1
455275,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

// NOTE: Manually set true to enable ValidationLayers, default is false.
const bool enableValidationLayers = false;

enum GPU_TYPE {
    GPU_TYPE_NOFOUND = -1,
    GPU_TYPE_DISCRETE = 0,
    GPU_TYPE_INTEGRATED = 1,
    GPU_TYPE_VIRTUAL = 2,
    GPU_TYPE_CPU_ONLY = 3,
};

// GPUInfo will parse GPU hardware information and save it in param.
struct GPUInfo
{
    // memory properties
    VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;

    // basic info
    GPU_TYPE type; // cpu, integrated GPU, discrete GPU.
    uint32_t apiVersion;
    uint32_t driverVersion;
    uint32_t vendorId;
    uint32_t deviceId;
    char deviceName[VK_MAX_PHYSICAL_DEVICE_NAME_SIZE];
    uint8_t pipelineCacheUUID[VK_UUID_SIZE];

    // hardware limit
    uint32_t maxSharedMemorySize;
    uint32_t maxWorkgroupCount_x;
    uint32_t maxWorkgroupCount_y;
    uint32_t maxWorkgroupCount_z;
    uint32_t maxWorkgroup_invocations;
    uint32_t maxWorkgroupSize_x;...",32,src\vkcom\include\context.hpp,cv.dnn.vkcom,37,vkcom,1
455281,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\fence.hpp,src\vkcom\include\fence.hpp:<global>,,<global>,1
455285,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {
#ifdef HAVE_VULKAN
// Used for synchronize and wait
class Fence
{
public:
    Fence();
    ~Fence();

    VkFence get() const;
    VkResult reset() const;
    VkResult wait() const;

private:
    VkFence fence;
};
#endif // HAVE_VULKAN
}}}",1,src\vkcom\include\fence.hpp,cv,14,cv,1
455286,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {
#ifdef HAVE_VULKAN
// Used for synchronize and wait
class Fence
{
public:
    Fence();
    ~Fence();

    VkFence get() const;
    VkResult reset() const;
    VkResult wait() const;

private:
    VkFence fence;
};
#endif // HAVE_VULKAN
}}",16,src\vkcom\include\fence.hpp,cv.dnn,14,dnn,1
455287,NAMESPACE_BLOCK,"namespace vkcom {
#ifdef HAVE_VULKAN
// Used for synchronize and wait
class Fence
{
public:
    Fence();
    ~Fence();

    VkFence get() const;
    VkResult reset() const;
    VkResult wait() const;

private:
    VkFence fence;
};
#endif // HAVE_VULKAN
}",32,src\vkcom\include\fence.hpp,cv.dnn.vkcom,14,vkcom,1
455295,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\op_base.hpp,src\vkcom\include\op_base.hpp:<global>,,<global>,1
455299,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// Forward declare
class Context;

class OpBase
{
public:
    OpBase();
    virtual ~OpBase();
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) = 0;
protected:
    std::vector<VkDescriptorType> destTypes; // Save the input data type.
    std::string shader_name; // the key which is used for retrieve Pipeline from PipelineFactory.
    std::string type_;
    int group_x_;
    int group_y_;
    int group_z_;
};

#endif // HAVE_VULKAN

}}}",1,src\vkcom\include\op_base.hpp,cv,15,cv,1
455300,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// Forward declare
class Context;

class OpBase
{
public:
    OpBase();
    virtual ~OpBase();
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) = 0;
protected:
    std::vector<VkDescriptorType> destTypes; // Save the input data type.
    std::string shader_name; // the key which is used for retrieve Pipeline from PipelineFactory.
    std::string type_;
    int group_x_;
    int group_y_;
    int group_z_;
};

#endif // HAVE_VULKAN

}}",16,src\vkcom\include\op_base.hpp,cv.dnn,15,dnn,1
455301,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

// Forward declare
class Context;

class OpBase
{
public:
    OpBase();
    virtual ~OpBase();
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) = 0;
protected:
    std::vector<VkDescriptorType> destTypes; // Save the input data type.
    std::string shader_name; // the key which is used for retrieve Pipeline from PipelineFactory.
    std::string type_;
    int group_x_;
    int group_y_;
    int group_z_;
};

#endif // HAVE_VULKAN

}",32,src\vkcom\include\op_base.hpp,cv.dnn.vkcom,15,vkcom,1
455307,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\op_conv.hpp,src\vkcom\include\op_conv.hpp:<global>,,<global>,1
455311,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

enum ConvShaderType
{
    kConvShaderTypeGeneric = 0,
    kConvShaderTypeDepthWise = 2, // special branch
    kConvShaderTypeWinograd = 3,
    kConvShaderTest = 4,
};

struct ConvShaderConfig
{
    int local_size_x;
    int local_size_y;
    int local_size_z;
};

// Current Vulkan Convolution layer only support Conv2D.
class OpConv : public OpBase
{
public:
    OpConv(const Mat& weightBlob, const std::vector<float>& biasvec, int activType, const int ngroups, const int K, const int C, const int Hk, const int Wk,
           const int stride_h, const int stride_w, const int dilation_h, const int dilation_w,
           const int pad_left, const int pad_top);
    ~OpConv();

    void firstForward(); // Execute only in the first forward.
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) CV_OVERRIDE;

    std::vector<float> biasCopy;
    Ptr<Tensor> weightTensorPtr;
    Ptr<Tensor> biasTensorP...",1,src\vkcom\include\op_conv.hpp,cv,14,cv,1
455312,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

enum ConvShaderType
{
    kConvShaderTypeGeneric = 0,
    kConvShaderTypeDepthWise = 2, // special branch
    kConvShaderTypeWinograd = 3,
    kConvShaderTest = 4,
};

struct ConvShaderConfig
{
    int local_size_x;
    int local_size_y;
    int local_size_z;
};

// Current Vulkan Convolution layer only support Conv2D.
class OpConv : public OpBase
{
public:
    OpConv(const Mat& weightBlob, const std::vector<float>& biasvec, int activType, const int ngroups, const int K, const int C, const int Hk, const int Wk,
           const int stride_h, const int stride_w, const int dilation_h, const int dilation_w,
           const int pad_left, const int pad_top);
    ~OpConv();

    void firstForward(); // Execute only in the first forward.
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) CV_OVERRIDE;

    std::vector<float> biasCopy;
    Ptr<Tensor> weightTensorPtr;
    Ptr<Tensor> biasTensorPtr;

private:
 ...",16,src\vkcom\include\op_conv.hpp,cv.dnn,14,dnn,1
455313,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

enum ConvShaderType
{
    kConvShaderTypeGeneric = 0,
    kConvShaderTypeDepthWise = 2, // special branch
    kConvShaderTypeWinograd = 3,
    kConvShaderTest = 4,
};

struct ConvShaderConfig
{
    int local_size_x;
    int local_size_y;
    int local_size_z;
};

// Current Vulkan Convolution layer only support Conv2D.
class OpConv : public OpBase
{
public:
    OpConv(const Mat& weightBlob, const std::vector<float>& biasvec, int activType, const int ngroups, const int K, const int C, const int Hk, const int Wk,
           const int stride_h, const int stride_w, const int dilation_h, const int dilation_w,
           const int pad_left, const int pad_top);
    ~OpConv();

    void firstForward(); // Execute only in the first forward.
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) CV_OVERRIDE;

    std::vector<float> biasCopy;
    Ptr<Tensor> weightTensorPtr;
    Ptr<Tensor> biasTensorPtr;

private:
    bool computeG...",32,src\vkcom\include\op_conv.hpp,cv.dnn.vkcom,14,vkcom,1
455319,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\op_matmul.hpp,src\vkcom\include\op_matmul.hpp:<global>,,<global>,1
455323,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

struct MatMulShaderConfig
{
    int local_size_x;
    int local_size_y;
    int local_size_z;
};

// Current Vulkan Convolution layer only support Conv2D.
class OpMatMul : public OpBase
{
public:
    OpMatMul(std::vector<Mat>& matBlobs, const int M, const int K, const int N);

    void firstForward(); // Execute only in the first forward.
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) CV_OVERRIDE;
    Ptr<Tensor> weightTensorPtr;
private:
    bool computeGroupCount();

    const int M, K, N;

    int Hi, Wi;
    int H0, W0;
    int batch;

    MatMulShaderConfig config;
    bool firstForwardFinsh = false;
};

#endif // HAVE_VULKAN

}}}",1,src\vkcom\include\op_matmul.hpp,cv,11,cv,1
455324,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

struct MatMulShaderConfig
{
    int local_size_x;
    int local_size_y;
    int local_size_z;
};

// Current Vulkan Convolution layer only support Conv2D.
class OpMatMul : public OpBase
{
public:
    OpMatMul(std::vector<Mat>& matBlobs, const int M, const int K, const int N);

    void firstForward(); // Execute only in the first forward.
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) CV_OVERRIDE;
    Ptr<Tensor> weightTensorPtr;
private:
    bool computeGroupCount();

    const int M, K, N;

    int Hi, Wi;
    int H0, W0;
    int batch;

    MatMulShaderConfig config;
    bool firstForwardFinsh = false;
};

#endif // HAVE_VULKAN

}}",16,src\vkcom\include\op_matmul.hpp,cv.dnn,11,dnn,1
455325,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

struct MatMulShaderConfig
{
    int local_size_x;
    int local_size_y;
    int local_size_z;
};

// Current Vulkan Convolution layer only support Conv2D.
class OpMatMul : public OpBase
{
public:
    OpMatMul(std::vector<Mat>& matBlobs, const int M, const int K, const int N);

    void firstForward(); // Execute only in the first forward.
    virtual bool forward(std::vector<Tensor>& ins, std::vector<Tensor>& outs) CV_OVERRIDE;
    Ptr<Tensor> weightTensorPtr;
private:
    bool computeGroupCount();

    const int M, K, N;

    int Hi, Wi;
    int H0, W0;
    int batch;

    MatMulShaderConfig config;
    bool firstForwardFinsh = false;
};

#endif // HAVE_VULKAN

}",32,src\vkcom\include\op_matmul.hpp,cv.dnn.vkcom,11,vkcom,1
455337,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\pipeline.hpp,src\vkcom\include\pipeline.hpp:<global>,,<global>,1
455341,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

class Pipeline;
class Descriptor
{
public:
    static Ptr<Descriptor> create(const VkDescriptorPool& pool, const VkDescriptorSet& set,
                                  Pipeline* _pipeline);
    ~Descriptor();

    void writeTensor(Tensor tensor, int bindIndex);
    // the buffer is bond to the device VkMemory.
    void writeBuffer(VkBuffer buffer, int bindIndex, size_t size, VkDeviceSize offset = 0);

    VkDescriptorSet get() const
    {
        return desSet;
    }

private:
    friend class Pipeline;
    Descriptor(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* _pipeline);

    VkDescriptorPool desPool;
    VkDescriptorSet desSet;
    Pipeline* pipeline;
    // If is true, the deconstruct will release the instance, otherwise, re-use it.
    bool needRelease = true;
};

class Pipeline
{
public:
    static Ptr<Pipeline> create(const uint32_t* spv, size_t length, const std::vector<VkDescripto...",1,src\vkcom\include\pipeline.hpp,cv,17,cv,1
455342,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

class Pipeline;
class Descriptor
{
public:
    static Ptr<Descriptor> create(const VkDescriptorPool& pool, const VkDescriptorSet& set,
                                  Pipeline* _pipeline);
    ~Descriptor();

    void writeTensor(Tensor tensor, int bindIndex);
    // the buffer is bond to the device VkMemory.
    void writeBuffer(VkBuffer buffer, int bindIndex, size_t size, VkDeviceSize offset = 0);

    VkDescriptorSet get() const
    {
        return desSet;
    }

private:
    friend class Pipeline;
    Descriptor(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* _pipeline);

    VkDescriptorPool desPool;
    VkDescriptorSet desSet;
    Pipeline* pipeline;
    // If is true, the deconstruct will release the instance, otherwise, re-use it.
    bool needRelease = true;
};

class Pipeline
{
public:
    static Ptr<Pipeline> create(const uint32_t* spv, size_t length, const std::vector<VkDescriptorType>& bufferT...",16,src\vkcom\include\pipeline.hpp,cv.dnn,17,dnn,1
455343,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

class Pipeline;
class Descriptor
{
public:
    static Ptr<Descriptor> create(const VkDescriptorPool& pool, const VkDescriptorSet& set,
                                  Pipeline* _pipeline);
    ~Descriptor();

    void writeTensor(Tensor tensor, int bindIndex);
    // the buffer is bond to the device VkMemory.
    void writeBuffer(VkBuffer buffer, int bindIndex, size_t size, VkDeviceSize offset = 0);

    VkDescriptorSet get() const
    {
        return desSet;
    }

private:
    friend class Pipeline;
    Descriptor(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* _pipeline);

    VkDescriptorPool desPool;
    VkDescriptorSet desSet;
    Pipeline* pipeline;
    // If is true, the deconstruct will release the instance, otherwise, re-use it.
    bool needRelease = true;
};

class Pipeline
{
public:
    static Ptr<Pipeline> create(const uint32_t* spv, size_t length, const std::vector<VkDescriptorType>& bufferTypes,
          ...",32,src\vkcom\include\pipeline.hpp,cv.dnn.vkcom,17,vkcom,1
455351,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\tensor.hpp,src\vkcom\include\tensor.hpp:<global>,,<global>,1
455355,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

class Buffer;

class Tensor
{
public:
    Tensor(Format fmt = kFormatFp32, VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    Tensor(const char* data, std::vector<int>& shape, Format fmt = kFormatFp32,
           VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    void* map();
    void unMap();
    Shape getShape() const;
    int dimSize(const int dim) const;
    int dimNum() const;
    int count(const int start_axis = 0, const int end_axis = -1) const;

    // Change shape and format to as passed in.
    // Copy data if data != NULL
    // Allocate new internal buffer if new size > old size or alloc flag is true
    Tensor reshape(const char* data, const std::vector<int>& shape, bool alloc = false, Format fmt = kFormatInvalid);

    void setTo(float val);
    int getFormat() const;
    size_t size() const { return size_in_byte_; }
    bool isEmpty() { return size_in_byte_ ...",1,src\vkcom\include\tensor.hpp,cv,14,cv,1
455356,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

class Buffer;

class Tensor
{
public:
    Tensor(Format fmt = kFormatFp32, VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    Tensor(const char* data, std::vector<int>& shape, Format fmt = kFormatFp32,
           VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    void* map();
    void unMap();
    Shape getShape() const;
    int dimSize(const int dim) const;
    int dimNum() const;
    int count(const int start_axis = 0, const int end_axis = -1) const;

    // Change shape and format to as passed in.
    // Copy data if data != NULL
    // Allocate new internal buffer if new size > old size or alloc flag is true
    Tensor reshape(const char* data, const std::vector<int>& shape, bool alloc = false, Format fmt = kFormatInvalid);

    void setTo(float val);
    int getFormat() const;
    size_t size() const { return size_in_byte_; }
    bool isEmpty() { return size_in_byte_ == 0 ? true : f...",16,src\vkcom\include\tensor.hpp,cv.dnn,14,dnn,1
455357,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

class Buffer;

class Tensor
{
public:
    Tensor(Format fmt = kFormatFp32, VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    Tensor(const char* data, std::vector<int>& shape, Format fmt = kFormatFp32,
           VkBufferUsageFlags usageFlag = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT);
    void* map();
    void unMap();
    Shape getShape() const;
    int dimSize(const int dim) const;
    int dimNum() const;
    int count(const int start_axis = 0, const int end_axis = -1) const;

    // Change shape and format to as passed in.
    // Copy data if data != NULL
    // Allocate new internal buffer if new size > old size or alloc flag is true
    Tensor reshape(const char* data, const std::vector<int>& shape, bool alloc = false, Format fmt = kFormatInvalid);

    void setTo(float val);
    int getFormat() const;
    size_t size() const { return size_in_byte_; }
    bool isEmpty() { return size_in_byte_ == 0 ? true : false; }
    void...",32,src\vkcom\include\tensor.hpp,cv.dnn.vkcom,14,vkcom,1
455373,NAMESPACE_BLOCK,<empty>,,src\vkcom\include\vkcom.hpp,src\vkcom\include\vkcom.hpp:<global>,,<global>,1
455377,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

enum Format{
    kFormatInvalid = -1,
    kFormatFp16,
    kFormatFp32,
    kFormatFp64,
    kFormatInt32,
    kFormatNum
};

enum OpType {
    kOpTypeNull = -1,
    kOpTypeConv,
    kOpTypeMatMul,
};

enum FusedActivationType {
    kFusedActivUnsupport = -1,
    kFusedActivNone = 0,
    kFusedActivRelu = 1,
    kFusedActivRelu6 = 2,
};
typedef std::vector<int> Shape;

bool isAvailable();

#endif // HAVE_VULKAN

}}}",1,src\vkcom\include\vkcom.hpp,cv,13,cv,1
455378,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

enum Format{
    kFormatInvalid = -1,
    kFormatFp16,
    kFormatFp32,
    kFormatFp64,
    kFormatInt32,
    kFormatNum
};

enum OpType {
    kOpTypeNull = -1,
    kOpTypeConv,
    kOpTypeMatMul,
};

enum FusedActivationType {
    kFusedActivUnsupport = -1,
    kFusedActivNone = 0,
    kFusedActivRelu = 1,
    kFusedActivRelu6 = 2,
};
typedef std::vector<int> Shape;

bool isAvailable();

#endif // HAVE_VULKAN

}}",16,src\vkcom\include\vkcom.hpp,cv.dnn,13,dnn,1
455379,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

enum Format{
    kFormatInvalid = -1,
    kFormatFp16,
    kFormatFp32,
    kFormatFp64,
    kFormatInt32,
    kFormatNum
};

enum OpType {
    kOpTypeNull = -1,
    kOpTypeConv,
    kOpTypeMatMul,
};

enum FusedActivationType {
    kFusedActivUnsupport = -1,
    kFusedActivNone = 0,
    kFusedActivRelu = 1,
    kFusedActivRelu6 = 2,
};
typedef std::vector<int> Shape;

bool isAvailable();

#endif // HAVE_VULKAN

}",32,src\vkcom\include\vkcom.hpp,cv.dnn.vkcom,13,vkcom,1
455383,NAMESPACE_BLOCK,<empty>,,src\vkcom\shader\conv_1x1_fast_spv.cpp,src\vkcom\shader\conv_1x1_fast_spv.cpp:<global>,,<global>,1
455387,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

extern const unsigned int conv_1x1_fast_spv[3134] = {
    0x07230203,0x00010000,0x0008000b,0x00000205,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002f,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00030005,0x00000008,0x0000004d,0x00050005,
    0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,0x00000000,0x00006948,
    0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,0x00000002,0x00003048,
    0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,0x00000004,0x69727473,
    0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,0x775f6564,0x00000000,
    0x00050006,0x00000009,0x00000006,0x5f646170,0x000000...",1,src\vkcom\shader\conv_1x1_fast_spv.cpp,cv,7,cv,1
455388,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

extern const unsigned int conv_1x1_fast_spv[3134] = {
    0x07230203,0x00010000,0x0008000b,0x00000205,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002f,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00030005,0x00000008,0x0000004d,0x00050005,
    0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,0x00000000,0x00006948,
    0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,0x00000002,0x00003048,
    0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,0x00000004,0x69727473,
    0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,0x775f6564,0x00000000,
    0x00050006,0x00000009,0x00000006,0x5f646170,0x00000068,0x00050006,0...",16,src\vkcom\shader\conv_1x1_fast_spv.cpp,cv.dnn,7,dnn,1
455389,NAMESPACE_BLOCK,"namespace vkcom {

extern const unsigned int conv_1x1_fast_spv[3134] = {
    0x07230203,0x00010000,0x0008000b,0x00000205,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002f,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00030005,0x00000008,0x0000004d,0x00050005,
    0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,0x00000000,0x00006948,
    0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,0x00000002,0x00003048,
    0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,0x00000004,0x69727473,
    0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,0x775f6564,0x00000000,
    0x00050006,0x00000009,0x00000006,0x5f646170,0x00000068,0x00050006,0x00000009,0x0000...",32,src\vkcom\shader\conv_1x1_fast_spv.cpp,cv.dnn.vkcom,7,vkcom,1
456398,NAMESPACE_BLOCK,<empty>,,src\vkcom\shader\conv_depthwise_3x3_spv.cpp,src\vkcom\shader\conv_depthwise_3x3_spv.cpp:<global>,,<global>,1
456402,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

extern const unsigned int conv_depthwise_3x3_spv[1977] = {
    0x07230203,0x00010000,0x0008000b,0x0000012f,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002e,0x00060010,
    0x00000004,0x00000011,0x00000040,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00050005,0x00000008,0x7074756f,0x6c507475,
    0x00006e61,0x00050005,0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,
    0x00000000,0x00006948,0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,
    0x00000002,0x00003048,0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,
    0x00000004,0x69727473,0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,
    0x775f6564,0x00000000,0x00050006,0x00000009,0x0...",1,src\vkcom\shader\conv_depthwise_3x3_spv.cpp,cv,7,cv,1
456403,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

extern const unsigned int conv_depthwise_3x3_spv[1977] = {
    0x07230203,0x00010000,0x0008000b,0x0000012f,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002e,0x00060010,
    0x00000004,0x00000011,0x00000040,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00050005,0x00000008,0x7074756f,0x6c507475,
    0x00006e61,0x00050005,0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,
    0x00000000,0x00006948,0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,
    0x00000002,0x00003048,0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,
    0x00000004,0x69727473,0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,
    0x775f6564,0x00000000,0x00050006,0x00000009,0x00000006,0x5f646...",16,src\vkcom\shader\conv_depthwise_3x3_spv.cpp,cv.dnn,7,dnn,1
456404,NAMESPACE_BLOCK,"namespace vkcom {

extern const unsigned int conv_depthwise_3x3_spv[1977] = {
    0x07230203,0x00010000,0x0008000b,0x0000012f,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002e,0x00060010,
    0x00000004,0x00000011,0x00000040,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00050005,0x00000008,0x7074756f,0x6c507475,
    0x00006e61,0x00050005,0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,
    0x00000000,0x00006948,0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,
    0x00000002,0x00003048,0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,
    0x00000004,0x69727473,0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,
    0x775f6564,0x00000000,0x00050006,0x00000009,0x00000006,0x5f646170,0x00000068,0...",32,src\vkcom\shader\conv_depthwise_3x3_spv.cpp,cv.dnn.vkcom,7,vkcom,1
457413,NAMESPACE_BLOCK,<empty>,,src\vkcom\shader\conv_depthwise_spv.cpp,src\vkcom\shader\conv_depthwise_spv.cpp:<global>,,<global>,1
457417,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

extern const unsigned int conv_depthwise_spv[2092] = {
    0x07230203,0x00010000,0x0008000b,0x00000143,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000026,0x00000034,0x00060010,
    0x00000004,0x00000011,0x00000040,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00050005,0x00000008,0x7074756f,0x6c507475,
    0x00006e61,0x00050005,0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,
    0x00000000,0x00006948,0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,
    0x00000002,0x00003048,0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,
    0x00000004,0x69727473,0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,
    0x775f6564,0x00000000,0x00050006,0x00000009,0x00000...",1,src\vkcom\shader\conv_depthwise_spv.cpp,cv,7,cv,1
457418,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

extern const unsigned int conv_depthwise_spv[2092] = {
    0x07230203,0x00010000,0x0008000b,0x00000143,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000026,0x00000034,0x00060010,
    0x00000004,0x00000011,0x00000040,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00050005,0x00000008,0x7074756f,0x6c507475,
    0x00006e61,0x00050005,0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,
    0x00000000,0x00006948,0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,
    0x00000002,0x00003048,0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,
    0x00000004,0x69727473,0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,
    0x775f6564,0x00000000,0x00050006,0x00000009,0x00000006,0x5f646170,...",16,src\vkcom\shader\conv_depthwise_spv.cpp,cv.dnn,7,dnn,1
457419,NAMESPACE_BLOCK,"namespace vkcom {

extern const unsigned int conv_depthwise_spv[2092] = {
    0x07230203,0x00010000,0x0008000b,0x00000143,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000026,0x00000034,0x00060010,
    0x00000004,0x00000011,0x00000040,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00050005,0x00000008,0x7074756f,0x6c507475,
    0x00006e61,0x00050005,0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,
    0x00000000,0x00006948,0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,
    0x00000002,0x00003048,0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,
    0x00000004,0x69727473,0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,
    0x775f6564,0x00000000,0x00050006,0x00000009,0x00000006,0x5f646170,0x00000068,0x000...",32,src\vkcom\shader\conv_depthwise_spv.cpp,cv.dnn.vkcom,7,vkcom,1
458428,NAMESPACE_BLOCK,<empty>,,src\vkcom\shader\conv_implicit_gemm_spv.cpp,src\vkcom\shader\conv_implicit_gemm_spv.cpp:<global>,,<global>,1
458432,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

extern const unsigned int conv_implicit_gemm_spv[3565] = {
    0x07230203,0x00010000,0x0008000b,0x00000257,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002f,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00030005,0x00000008,0x0000004d,0x00050005,
    0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,0x00000000,0x00006948,
    0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,0x00000002,0x00003048,
    0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,0x00000004,0x69727473,
    0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,0x775f6564,0x00000000,
    0x00050006,0x00000009,0x00000006,0x5f646170,0x0...",1,src\vkcom\shader\conv_implicit_gemm_spv.cpp,cv,7,cv,1
458433,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

extern const unsigned int conv_implicit_gemm_spv[3565] = {
    0x07230203,0x00010000,0x0008000b,0x00000257,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002f,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00030005,0x00000008,0x0000004d,0x00050005,
    0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,0x00000000,0x00006948,
    0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,0x00000002,0x00003048,
    0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,0x00000004,0x69727473,
    0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,0x775f6564,0x00000000,
    0x00050006,0x00000009,0x00000006,0x5f646170,0x00000068,0x00050...",16,src\vkcom\shader\conv_implicit_gemm_spv.cpp,cv.dnn,7,dnn,1
458434,NAMESPACE_BLOCK,"namespace vkcom {

extern const unsigned int conv_implicit_gemm_spv[3565] = {
    0x07230203,0x00010000,0x0008000b,0x00000257,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x00000020,0x0000002f,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00030005,0x00000008,0x0000004d,0x00050005,
    0x00000009,0x68737570,0x636f6c42,0x0000006b,0x00040006,0x00000009,0x00000000,0x00006948,
    0x00040006,0x00000009,0x00000001,0x00006957,0x00040006,0x00000009,0x00000002,0x00003048,
    0x00040006,0x00000009,0x00000003,0x00003057,0x00060006,0x00000009,0x00000004,0x69727473,
    0x685f6564,0x00000000,0x00060006,0x00000009,0x00000005,0x69727473,0x775f6564,0x00000000,
    0x00050006,0x00000009,0x00000006,0x5f646170,0x00000068,0x00050006,0x00000009,0...",32,src\vkcom\shader\conv_implicit_gemm_spv.cpp,cv.dnn.vkcom,7,vkcom,1
459443,NAMESPACE_BLOCK,<empty>,,src\vkcom\shader\gemm_spv.cpp,src\vkcom\shader\gemm_spv.cpp:<global>,,<global>,1
459447,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

extern const unsigned int gemm_spv[2902] = {
    0x07230203,0x00010000,0x0008000b,0x000001ff,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x0000000c,0x0000001b,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00040005,0x00000008,0x646e496d,0x00007865,
    0x00060005,0x0000000c,0x575f6c67,0x476b726f,0x70756f72,0x00004449,0x00040005,0x00000014,
    0x646e496e,0x00007865,0x00040005,0x0000001a,0x61636f6c,0x00785f6c,0x00080005,0x0000001b,
    0x4c5f6c67,0x6c61636f,0x6f766e49,0x69746163,0x44496e6f,0x00000000,0x00040005,0x00000021,
    0x61636f6c,0x00795f6c,0x00050005,0x00000026,0x6f6c5f61,0x5f6c6163,0x00000078,0x00050005,
    0x0000002c,0x6f6c5f61,0x5f6c6163,0x00000079,0x00050005,0x0000...",1,src\vkcom\shader\gemm_spv.cpp,cv,7,cv,1
459448,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

extern const unsigned int gemm_spv[2902] = {
    0x07230203,0x00010000,0x0008000b,0x000001ff,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x0000000c,0x0000001b,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00040005,0x00000008,0x646e496d,0x00007865,
    0x00060005,0x0000000c,0x575f6c67,0x476b726f,0x70756f72,0x00004449,0x00040005,0x00000014,
    0x646e496e,0x00007865,0x00040005,0x0000001a,0x61636f6c,0x00785f6c,0x00080005,0x0000001b,
    0x4c5f6c67,0x6c61636f,0x6f766e49,0x69746163,0x44496e6f,0x00000000,0x00040005,0x00000021,
    0x61636f6c,0x00795f6c,0x00050005,0x00000026,0x6f6c5f61,0x5f6c6163,0x00000078,0x00050005,
    0x0000002c,0x6f6c5f61,0x5f6c6163,0x00000079,0x00050005,0x00000031,0x6f6c5f62...",16,src\vkcom\shader\gemm_spv.cpp,cv.dnn,7,dnn,1
459449,NAMESPACE_BLOCK,"namespace vkcom {

extern const unsigned int gemm_spv[2902] = {
    0x07230203,0x00010000,0x0008000b,0x000001ff,0x00000000,0x00020011,0x00000001,0x0006000b,
    0x00000001,0x4c534c47,0x6474732e,0x3035342e,0x00000000,0x0003000e,0x00000000,0x00000001,
    0x0007000f,0x00000005,0x00000004,0x6e69616d,0x00000000,0x0000000c,0x0000001b,0x00060010,
    0x00000004,0x00000011,0x00000100,0x00000001,0x00000001,0x00030003,0x00000002,0x000001c2,
    0x00040005,0x00000004,0x6e69616d,0x00000000,0x00040005,0x00000008,0x646e496d,0x00007865,
    0x00060005,0x0000000c,0x575f6c67,0x476b726f,0x70756f72,0x00004449,0x00040005,0x00000014,
    0x646e496e,0x00007865,0x00040005,0x0000001a,0x61636f6c,0x00785f6c,0x00080005,0x0000001b,
    0x4c5f6c67,0x6c61636f,0x6f766e49,0x69746163,0x44496e6f,0x00000000,0x00040005,0x00000021,
    0x61636f6c,0x00795f6c,0x00050005,0x00000026,0x6f6c5f61,0x5f6c6163,0x00000078,0x00050005,
    0x0000002c,0x6f6c5f61,0x5f6c6163,0x00000079,0x00050005,0x00000031,0x6f6c5f62,0x5f6c6163,
   ...",32,src\vkcom\shader\gemm_spv.cpp,cv.dnn.vkcom,7,vkcom,1
460460,NAMESPACE_BLOCK,<empty>,,src\vkcom\shader\spv_shader.cpp,src\vkcom\shader\spv_shader.cpp:<global>,,<global>,1
460464,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

std::map<std::string, std::pair<const unsigned int *, size_t> > SPVMaps;

void initSPVMaps()
{
    SPVMaps.insert(std::make_pair(""conv_1x1_fast_spv"", std::make_pair(conv_1x1_fast_spv, 3134)));
    SPVMaps.insert(std::make_pair(""gemm_spv"", std::make_pair(gemm_spv, 2902)));
    SPVMaps.insert(std::make_pair(""conv_depthwise_3x3_spv"", std::make_pair(conv_depthwise_3x3_spv, 1977)));
    SPVMaps.insert(std::make_pair(""conv_implicit_gemm_spv"", std::make_pair(conv_implicit_gemm_spv, 3565)));
    SPVMaps.insert(std::make_pair(""conv_depthwise_spv"", std::make_pair(conv_depthwise_spv, 2092)));
}

}}}",1,src\vkcom\shader\spv_shader.cpp,cv,8,cv,1
460465,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

std::map<std::string, std::pair<const unsigned int *, size_t> > SPVMaps;

void initSPVMaps()
{
    SPVMaps.insert(std::make_pair(""conv_1x1_fast_spv"", std::make_pair(conv_1x1_fast_spv, 3134)));
    SPVMaps.insert(std::make_pair(""gemm_spv"", std::make_pair(gemm_spv, 2902)));
    SPVMaps.insert(std::make_pair(""conv_depthwise_3x3_spv"", std::make_pair(conv_depthwise_3x3_spv, 1977)));
    SPVMaps.insert(std::make_pair(""conv_implicit_gemm_spv"", std::make_pair(conv_implicit_gemm_spv, 3565)));
    SPVMaps.insert(std::make_pair(""conv_depthwise_spv"", std::make_pair(conv_depthwise_spv, 2092)));
}

}}",16,src\vkcom\shader\spv_shader.cpp,cv.dnn,8,dnn,1
460466,NAMESPACE_BLOCK,"namespace vkcom {

std::map<std::string, std::pair<const unsigned int *, size_t> > SPVMaps;

void initSPVMaps()
{
    SPVMaps.insert(std::make_pair(""conv_1x1_fast_spv"", std::make_pair(conv_1x1_fast_spv, 3134)));
    SPVMaps.insert(std::make_pair(""gemm_spv"", std::make_pair(gemm_spv, 2902)));
    SPVMaps.insert(std::make_pair(""conv_depthwise_3x3_spv"", std::make_pair(conv_depthwise_3x3_spv, 1977)));
    SPVMaps.insert(std::make_pair(""conv_implicit_gemm_spv"", std::make_pair(conv_implicit_gemm_spv, 3565)));
    SPVMaps.insert(std::make_pair(""conv_depthwise_spv"", std::make_pair(conv_depthwise_spv, 2092)));
}

}",32,src\vkcom\shader\spv_shader.cpp,cv.dnn.vkcom,8,vkcom,1
460548,NAMESPACE_BLOCK,<empty>,,src\vkcom\shader\spv_shader.hpp,src\vkcom\shader\spv_shader.hpp:<global>,,<global>,1
460552,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

extern const unsigned int conv_1x1_fast_spv[3134];
extern const unsigned int gemm_spv[2902];
extern const unsigned int conv_depthwise_3x3_spv[1977];
extern const unsigned int conv_implicit_gemm_spv[3565];
extern const unsigned int conv_depthwise_spv[2092];

extern std::map<std::string, std::pair<const unsigned int *, size_t> > SPVMaps;

void initSPVMaps();

}}}",1,src\vkcom\shader\spv_shader.hpp,cv,9,cv,1
460553,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

extern const unsigned int conv_1x1_fast_spv[3134];
extern const unsigned int gemm_spv[2902];
extern const unsigned int conv_depthwise_3x3_spv[1977];
extern const unsigned int conv_implicit_gemm_spv[3565];
extern const unsigned int conv_depthwise_spv[2092];

extern std::map<std::string, std::pair<const unsigned int *, size_t> > SPVMaps;

void initSPVMaps();

}}",16,src\vkcom\shader\spv_shader.hpp,cv.dnn,9,dnn,1
460554,NAMESPACE_BLOCK,"namespace vkcom {

extern const unsigned int conv_1x1_fast_spv[3134];
extern const unsigned int gemm_spv[2902];
extern const unsigned int conv_depthwise_3x3_spv[1977];
extern const unsigned int conv_implicit_gemm_spv[3565];
extern const unsigned int conv_depthwise_spv[2092];

extern std::map<std::string, std::pair<const unsigned int *, size_t> > SPVMaps;

void initSPVMaps();

}",32,src\vkcom\shader\spv_shader.hpp,cv.dnn.vkcom,9,vkcom,1
460582,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\buffer.cpp,src\vkcom\src\buffer.cpp:<global>,,<global>,1
460586,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

static uint32_t findMemoryType(uint32_t memoryTypeBits, VkMemoryPropertyFlags properties)
{
    for (uint32_t i = 0; i < physicalDeviceMemoryProperties.memoryTypeCount; ++i)
    {
        if ((memoryTypeBits & (1 << i)) &&
                ((physicalDeviceMemoryProperties.memoryTypes[i].propertyFlags & properties) == properties))
            return i;
    }
    return uint32_t(-1);
}

Buffer::Buffer(VkBufferUsageFlags usageFlag) : usageFlag_(usageFlag), buffer_(VK_NULL_HANDLE), memory_(VK_NULL_HANDLE)
{
}

bool Buffer::init(size_t size_in_bytes, const char* data)
{
    if (buffer_ != VK_NULL_HANDLE)
    {
        CV_LOG_WARNING(NULL, ""Warn: Buffer object already inited!"");
        return false;
    }

    VkBufferCreateInfo bufferCreateInfo = {};
    bufferCreateInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
    bufferCreateInfo.size = (VkDeviceSize)size_in_bytes;
    bufferCreateInfo.usage = usageFlag_;
    bu...",1,src\vkcom\src\buffer.cpp,cv,12,cv,1
460587,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

static uint32_t findMemoryType(uint32_t memoryTypeBits, VkMemoryPropertyFlags properties)
{
    for (uint32_t i = 0; i < physicalDeviceMemoryProperties.memoryTypeCount; ++i)
    {
        if ((memoryTypeBits & (1 << i)) &&
                ((physicalDeviceMemoryProperties.memoryTypes[i].propertyFlags & properties) == properties))
            return i;
    }
    return uint32_t(-1);
}

Buffer::Buffer(VkBufferUsageFlags usageFlag) : usageFlag_(usageFlag), buffer_(VK_NULL_HANDLE), memory_(VK_NULL_HANDLE)
{
}

bool Buffer::init(size_t size_in_bytes, const char* data)
{
    if (buffer_ != VK_NULL_HANDLE)
    {
        CV_LOG_WARNING(NULL, ""Warn: Buffer object already inited!"");
        return false;
    }

    VkBufferCreateInfo bufferCreateInfo = {};
    bufferCreateInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
    bufferCreateInfo.size = (VkDeviceSize)size_in_bytes;
    bufferCreateInfo.usage = usageFlag_;
    bufferCreateInfo....",16,src\vkcom\src\buffer.cpp,cv.dnn,12,dnn,1
460588,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

static uint32_t findMemoryType(uint32_t memoryTypeBits, VkMemoryPropertyFlags properties)
{
    for (uint32_t i = 0; i < physicalDeviceMemoryProperties.memoryTypeCount; ++i)
    {
        if ((memoryTypeBits & (1 << i)) &&
                ((physicalDeviceMemoryProperties.memoryTypes[i].propertyFlags & properties) == properties))
            return i;
    }
    return uint32_t(-1);
}

Buffer::Buffer(VkBufferUsageFlags usageFlag) : usageFlag_(usageFlag), buffer_(VK_NULL_HANDLE), memory_(VK_NULL_HANDLE)
{
}

bool Buffer::init(size_t size_in_bytes, const char* data)
{
    if (buffer_ != VK_NULL_HANDLE)
    {
        CV_LOG_WARNING(NULL, ""Warn: Buffer object already inited!"");
        return false;
    }

    VkBufferCreateInfo bufferCreateInfo = {};
    bufferCreateInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
    bufferCreateInfo.size = (VkDeviceSize)size_in_bytes;
    bufferCreateInfo.usage = usageFlag_;
    bufferCreateInfo.sharingMode = VK...",32,src\vkcom\src\buffer.cpp,cv.dnn.vkcom,12,vkcom,1
460596,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\command.cpp,src\vkcom\src\command.cpp:<global>,,<global>,1
460600,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// *********************** CommandBuffer ********************
CommandBuffer::CommandBuffer(CommandPool* pool) : cmdPool(pool)
{
    CV_Assert(cmdPool);
    if (pool->bufferQueue.empty())
    {
        VkCommandBufferAllocateInfo cmdBufferCreateInfo {
                /* .sType              = */ VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
                /* .pNext              = */ nullptr,
                /* .commandPool        = */ cmdPool->get(),
                /* .level              = */ VK_COMMAND_BUFFER_LEVEL_PRIMARY,
                /* .commandBufferCount = */ 1,
        };
        vkAllocateCommandBuffers(kDevice, &cmdBufferCreateInfo, &cmdBuffer);
    }
    else
    {
        cmdBuffer = pool->bufferQueue.front();
        pool-> bufferQueue.pop();
    }
}

void CommandBuffer::barrierSource(VkBuffer source, size_t start, size_t size, BarrierType type) const
{
    VkBufferMemoryBarrier barrier;
    barri...",1,src\vkcom\src\command.cpp,cv,23,cv,1
460601,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// *********************** CommandBuffer ********************
CommandBuffer::CommandBuffer(CommandPool* pool) : cmdPool(pool)
{
    CV_Assert(cmdPool);
    if (pool->bufferQueue.empty())
    {
        VkCommandBufferAllocateInfo cmdBufferCreateInfo {
                /* .sType              = */ VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
                /* .pNext              = */ nullptr,
                /* .commandPool        = */ cmdPool->get(),
                /* .level              = */ VK_COMMAND_BUFFER_LEVEL_PRIMARY,
                /* .commandBufferCount = */ 1,
        };
        vkAllocateCommandBuffers(kDevice, &cmdBufferCreateInfo, &cmdBuffer);
    }
    else
    {
        cmdBuffer = pool->bufferQueue.front();
        pool-> bufferQueue.pop();
    }
}

void CommandBuffer::barrierSource(VkBuffer source, size_t start, size_t size, BarrierType type) const
{
    VkBufferMemoryBarrier barrier;
    barrier.sType       ...",16,src\vkcom\src\command.cpp,cv.dnn,23,dnn,1
460602,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

// *********************** CommandBuffer ********************
CommandBuffer::CommandBuffer(CommandPool* pool) : cmdPool(pool)
{
    CV_Assert(cmdPool);
    if (pool->bufferQueue.empty())
    {
        VkCommandBufferAllocateInfo cmdBufferCreateInfo {
                /* .sType              = */ VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
                /* .pNext              = */ nullptr,
                /* .commandPool        = */ cmdPool->get(),
                /* .level              = */ VK_COMMAND_BUFFER_LEVEL_PRIMARY,
                /* .commandBufferCount = */ 1,
        };
        vkAllocateCommandBuffers(kDevice, &cmdBufferCreateInfo, &cmdBuffer);
    }
    else
    {
        cmdBuffer = pool->bufferQueue.front();
        pool-> bufferQueue.pop();
    }
}

void CommandBuffer::barrierSource(VkBuffer source, size_t start, size_t size, BarrierType type) const
{
    VkBufferMemoryBarrier barrier;
    barrier.sType               = VK_STR...",32,src\vkcom\src\command.cpp,cv.dnn.vkcom,23,vkcom,1
460612,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\context.cpp,src\vkcom\src\context.cpp:<global>,,<global>,1
460616,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// Global Variable
VkQueue kQueue = VK_NULL_HANDLE;
VkDevice kDevice = VK_NULL_HANDLE; // It was used almost everywhere.
VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;
cv::Mutex kContextMtx;
Ptr<CommandPool> cmdPoolPtr;
Ptr<PipelineFactory> pipelineFactoryPtr;

int support_VK_KHR_external_memory_capabilities = 0;
int support_VK_KHR_get_physical_device_properties2 = 0;
int support_VK_KHR_get_surface_capabilities2 = 0;
int support_VK_KHR_portability_enumeration = 0;
int support_VK_KHR_surface = 0;
int support_VK_EXT_debug_report = 0;

#if defined(__ANDROID_API__) && __ANDROID_API__ >= 26
int support_VK_KHR_android_surface = 0;
#endif // __ANDROID_API__ >= 26

static uint32_t findDeviceComputeQueue(const std::vector<VkQueueFamilyProperties>& queueFamilyProperties)
{
    // first try, compute only queue
    for (uint32_t i = 0; i < queueFamilyProperties.size(); i++)
    {
        const VkQueueFamilyP...",1,src\vkcom\src\context.cpp,cv,30,cv,1
460617,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// Global Variable
VkQueue kQueue = VK_NULL_HANDLE;
VkDevice kDevice = VK_NULL_HANDLE; // It was used almost everywhere.
VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;
cv::Mutex kContextMtx;
Ptr<CommandPool> cmdPoolPtr;
Ptr<PipelineFactory> pipelineFactoryPtr;

int support_VK_KHR_external_memory_capabilities = 0;
int support_VK_KHR_get_physical_device_properties2 = 0;
int support_VK_KHR_get_surface_capabilities2 = 0;
int support_VK_KHR_portability_enumeration = 0;
int support_VK_KHR_surface = 0;
int support_VK_EXT_debug_report = 0;

#if defined(__ANDROID_API__) && __ANDROID_API__ >= 26
int support_VK_KHR_android_surface = 0;
#endif // __ANDROID_API__ >= 26

static uint32_t findDeviceComputeQueue(const std::vector<VkQueueFamilyProperties>& queueFamilyProperties)
{
    // first try, compute only queue
    for (uint32_t i = 0; i < queueFamilyProperties.size(); i++)
    {
        const VkQueueFamilyProperties& queu...",16,src\vkcom\src\context.cpp,cv.dnn,30,dnn,1
460618,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

// Global Variable
VkQueue kQueue = VK_NULL_HANDLE;
VkDevice kDevice = VK_NULL_HANDLE; // It was used almost everywhere.
VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;
cv::Mutex kContextMtx;
Ptr<CommandPool> cmdPoolPtr;
Ptr<PipelineFactory> pipelineFactoryPtr;

int support_VK_KHR_external_memory_capabilities = 0;
int support_VK_KHR_get_physical_device_properties2 = 0;
int support_VK_KHR_get_surface_capabilities2 = 0;
int support_VK_KHR_portability_enumeration = 0;
int support_VK_KHR_surface = 0;
int support_VK_EXT_debug_report = 0;

#if defined(__ANDROID_API__) && __ANDROID_API__ >= 26
int support_VK_KHR_android_surface = 0;
#endif // __ANDROID_API__ >= 26

static uint32_t findDeviceComputeQueue(const std::vector<VkQueueFamilyProperties>& queueFamilyProperties)
{
    // first try, compute only queue
    for (uint32_t i = 0; i < queueFamilyProperties.size(); i++)
    {
        const VkQueueFamilyProperties& queueFamilyProperty ...",32,src\vkcom\src\context.cpp,cv.dnn.vkcom,30,vkcom,1
460626,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\fence.cpp,src\vkcom\src\fence.cpp:<global>,,<global>,1
460630,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {
#ifdef HAVE_VULKAN

Fence::Fence()
{
    VkFenceCreateInfo fci{
            /* .sType = */ VK_STRUCTURE_TYPE_FENCE_CREATE_INFO,
            /* .pNext = */ nullptr,
            /* .flags = */ 0,
    };
    vkCreateFence(kDevice, &fci, nullptr, &fence);
}

VkFence Fence::get() const
{
    return fence;
}

VkResult Fence::reset() const
{
    return vkResetFences(kDevice, 1, &fence);
}

VkResult Fence::wait() const
{
    auto status = VK_TIMEOUT;

    do {
        status = vkWaitForFences(kDevice, 1, &fence, VK_TRUE, 5000000000);
    } while (status == VK_TIMEOUT);

    return status;
}

Fence::~Fence()
{
    vkDestroyFence(kDevice, fence, nullptr);
}

#endif // HAVE_VULKAN
}}}",1,src\vkcom\src\fence.cpp,cv,9,cv,1
460631,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {
#ifdef HAVE_VULKAN

Fence::Fence()
{
    VkFenceCreateInfo fci{
            /* .sType = */ VK_STRUCTURE_TYPE_FENCE_CREATE_INFO,
            /* .pNext = */ nullptr,
            /* .flags = */ 0,
    };
    vkCreateFence(kDevice, &fci, nullptr, &fence);
}

VkFence Fence::get() const
{
    return fence;
}

VkResult Fence::reset() const
{
    return vkResetFences(kDevice, 1, &fence);
}

VkResult Fence::wait() const
{
    auto status = VK_TIMEOUT;

    do {
        status = vkWaitForFences(kDevice, 1, &fence, VK_TRUE, 5000000000);
    } while (status == VK_TIMEOUT);

    return status;
}

Fence::~Fence()
{
    vkDestroyFence(kDevice, fence, nullptr);
}

#endif // HAVE_VULKAN
}}",16,src\vkcom\src\fence.cpp,cv.dnn,9,dnn,1
460632,NAMESPACE_BLOCK,"namespace vkcom {
#ifdef HAVE_VULKAN

Fence::Fence()
{
    VkFenceCreateInfo fci{
            /* .sType = */ VK_STRUCTURE_TYPE_FENCE_CREATE_INFO,
            /* .pNext = */ nullptr,
            /* .flags = */ 0,
    };
    vkCreateFence(kDevice, &fci, nullptr, &fence);
}

VkFence Fence::get() const
{
    return fence;
}

VkResult Fence::reset() const
{
    return vkResetFences(kDevice, 1, &fence);
}

VkResult Fence::wait() const
{
    auto status = VK_TIMEOUT;

    do {
        status = vkWaitForFences(kDevice, 1, &fence, VK_TRUE, 5000000000);
    } while (status == VK_TIMEOUT);

    return status;
}

Fence::~Fence()
{
    vkDestroyFence(kDevice, fence, nullptr);
}

#endif // HAVE_VULKAN
}",32,src\vkcom\src\fence.cpp,cv.dnn.vkcom,9,vkcom,1
460638,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\internal.cpp,src\vkcom\src\internal.cpp:<global>,,<global>,1
460642,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {
#ifdef HAVE_VULKAN

bool checkFormat(Format fmt)
{
    return (fmt > -1 && fmt < kFormatNum) ? true : false;
}

size_t elementSize(Format fmt)
{
    if (fmt == kFormatFp32 || fmt == kFormatInt32)
    {
        return 4;
    }
    else if (fmt >= 0 && fmt < kFormatNum)
    {
        CV_LOG_WARNING(NULL, format(""Unsupported format %d"", fmt));
    }
    else
    {
        CV_Error(Error::StsError, format(""Invalid format %d"", fmt));
    }
    return 0;
}

int shapeCount(const Shape& shape, int start, int end)
{
    if (start == -1) start = 0;
    if (end == -1) end = (int)shape.size();

    if (shape.empty())
        return 0;

    int elems = 1;
    assert(start <= (int)shape.size() &&
           end <= (int)shape.size() &&
           start <= end);
    for(int i = start; i < end; i++)
    {
        elems *= shape[i];
    }
    return elems;
}

#endif // HAVE_VULKAN

}}}",1,src\vkcom\src\internal.cpp,cv,8,cv,1
460643,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {
#ifdef HAVE_VULKAN

bool checkFormat(Format fmt)
{
    return (fmt > -1 && fmt < kFormatNum) ? true : false;
}

size_t elementSize(Format fmt)
{
    if (fmt == kFormatFp32 || fmt == kFormatInt32)
    {
        return 4;
    }
    else if (fmt >= 0 && fmt < kFormatNum)
    {
        CV_LOG_WARNING(NULL, format(""Unsupported format %d"", fmt));
    }
    else
    {
        CV_Error(Error::StsError, format(""Invalid format %d"", fmt));
    }
    return 0;
}

int shapeCount(const Shape& shape, int start, int end)
{
    if (start == -1) start = 0;
    if (end == -1) end = (int)shape.size();

    if (shape.empty())
        return 0;

    int elems = 1;
    assert(start <= (int)shape.size() &&
           end <= (int)shape.size() &&
           start <= end);
    for(int i = start; i < end; i++)
    {
        elems *= shape[i];
    }
    return elems;
}

#endif // HAVE_VULKAN

}}",16,src\vkcom\src\internal.cpp,cv.dnn,8,dnn,1
460644,NAMESPACE_BLOCK,"namespace vkcom {
#ifdef HAVE_VULKAN

bool checkFormat(Format fmt)
{
    return (fmt > -1 && fmt < kFormatNum) ? true : false;
}

size_t elementSize(Format fmt)
{
    if (fmt == kFormatFp32 || fmt == kFormatInt32)
    {
        return 4;
    }
    else if (fmt >= 0 && fmt < kFormatNum)
    {
        CV_LOG_WARNING(NULL, format(""Unsupported format %d"", fmt));
    }
    else
    {
        CV_Error(Error::StsError, format(""Invalid format %d"", fmt));
    }
    return 0;
}

int shapeCount(const Shape& shape, int start, int end)
{
    if (start == -1) start = 0;
    if (end == -1) end = (int)shape.size();

    if (shape.empty())
        return 0;

    int elems = 1;
    assert(start <= (int)shape.size() &&
           end <= (int)shape.size() &&
           start <= end);
    for(int i = start; i < end; i++)
    {
        elems *= shape[i];
    }
    return elems;
}

#endif // HAVE_VULKAN

}",32,src\vkcom\src\internal.cpp,cv.dnn.vkcom,8,vkcom,1
460676,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\internal.hpp,src\vkcom\src\internal.hpp:<global>,,<global>,1
460680,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN
extern VkQueue kQueue;
extern VkDevice kDevice;
extern cv::Mutex kContextMtx;
extern Ptr<CommandPool> cmdPoolPtr;
extern Ptr<PipelineFactory> pipelineFactoryPtr;
extern VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;



enum ShapeIdx
{
    kShapeIdxBatch = 0,
    kShapeIdxChannel,
    kShapeIdxHeight,
    kShapeIdxWidth,
};

#define VK_CHECK_RESULT(f) \
{ \
        if (f != VK_SUCCESS) \
        { \
            CV_LOG_ERROR(NULL, ""Vulkan check failed, result = "" << (int)f); \
            CV_Error(Error::StsError, ""Vulkan check failed""); \
        } \
}

#define VKCOM_CHECK_BOOL_RET_VAL(val, ret) \
{ \
    bool res = (val); \
    if (!res) \
    { \
        CV_LOG_WARNING(NULL, ""Check bool failed""); \
        return ret; \
    } \
}

#define VKCOM_CHECK_POINTER_RET_VOID(p) \
{ \
    if (NULL == (p)) \
    { \
        CV_LOG_WARNING(NULL, ""Check pointer failed""); \
        return; \
    } \
}

#defin...",1,src\vkcom\src\internal.hpp,cv,33,cv,1
460681,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN
extern VkQueue kQueue;
extern VkDevice kDevice;
extern cv::Mutex kContextMtx;
extern Ptr<CommandPool> cmdPoolPtr;
extern Ptr<PipelineFactory> pipelineFactoryPtr;
extern VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;



enum ShapeIdx
{
    kShapeIdxBatch = 0,
    kShapeIdxChannel,
    kShapeIdxHeight,
    kShapeIdxWidth,
};

#define VK_CHECK_RESULT(f) \
{ \
        if (f != VK_SUCCESS) \
        { \
            CV_LOG_ERROR(NULL, ""Vulkan check failed, result = "" << (int)f); \
            CV_Error(Error::StsError, ""Vulkan check failed""); \
        } \
}

#define VKCOM_CHECK_BOOL_RET_VAL(val, ret) \
{ \
    bool res = (val); \
    if (!res) \
    { \
        CV_LOG_WARNING(NULL, ""Check bool failed""); \
        return ret; \
    } \
}

#define VKCOM_CHECK_POINTER_RET_VOID(p) \
{ \
    if (NULL == (p)) \
    { \
        CV_LOG_WARNING(NULL, ""Check pointer failed""); \
        return; \
    } \
}

#define VKCOM_CHECK_P...",16,src\vkcom\src\internal.hpp,cv.dnn,33,dnn,1
460682,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN
extern VkQueue kQueue;
extern VkDevice kDevice;
extern cv::Mutex kContextMtx;
extern Ptr<CommandPool> cmdPoolPtr;
extern Ptr<PipelineFactory> pipelineFactoryPtr;
extern VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;



enum ShapeIdx
{
    kShapeIdxBatch = 0,
    kShapeIdxChannel,
    kShapeIdxHeight,
    kShapeIdxWidth,
};

#define VK_CHECK_RESULT(f) \
{ \
        if (f != VK_SUCCESS) \
        { \
            CV_LOG_ERROR(NULL, ""Vulkan check failed, result = "" << (int)f); \
            CV_Error(Error::StsError, ""Vulkan check failed""); \
        } \
}

#define VKCOM_CHECK_BOOL_RET_VAL(val, ret) \
{ \
    bool res = (val); \
    if (!res) \
    { \
        CV_LOG_WARNING(NULL, ""Check bool failed""); \
        return ret; \
    } \
}

#define VKCOM_CHECK_POINTER_RET_VOID(p) \
{ \
    if (NULL == (p)) \
    { \
        CV_LOG_WARNING(NULL, ""Check pointer failed""); \
        return; \
    } \
}

#define VKCOM_CHECK_POINTER_RET_VAL(p...",32,src\vkcom\src\internal.hpp,cv.dnn.vkcom,33,vkcom,1
460690,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\op_base.cpp,src\vkcom\src\op_base.cpp:<global>,,<global>,1
460694,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

OpBase::OpBase()
{
}

OpBase::~OpBase()
{
}

#endif // HAVE_VULKAN

}}}",1,src\vkcom\src\op_base.cpp,cv,12,cv,1
460695,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

OpBase::OpBase()
{
}

OpBase::~OpBase()
{
}

#endif // HAVE_VULKAN

}}",16,src\vkcom\src\op_base.cpp,cv.dnn,12,dnn,1
460696,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

OpBase::OpBase()
{
}

OpBase::~OpBase()
{
}

#endif // HAVE_VULKAN

}",32,src\vkcom\src\op_base.cpp,cv.dnn.vkcom,12,vkcom,1
460704,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\op_conv.cpp,src\vkcom\src\op_conv.cpp:<global>,,<global>,1
460708,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN
#define BLOCK_SIZE 64

#define MAX_GROUP_COUNT_X 65535
#define MAX_GROUP_COUNT_Y 65535
#define MAX_GROUP_COUNT_Z 65535

OpConv::OpConv(const Mat& weightBlob, const std::vector<float>& biasvec, int _activType, const int _ngroups, const int _K,
               const int _C, const int _Hk, const int _Wk, const int _stride_h, const int _stride_w,
               const int _dilation_h, const int _dilation_w, const int _pad_left, const int _pad_top):
               activ((FusedActivationType)_activType), ngroups(_ngroups), K(_K), C(_C), Hk(_Hk), Wk(_Wk), stride_h(_stride_h), stride_w(_stride_w),
               dilation_h(_dilation_h), dilation_w(_dilation_w), pad_left(_pad_left), pad_top(_pad_top)
{
    type_ = kOpTypeConv;
    CV_Assert(!weightBlob.empty());

    Kg = K/ngroups, Cg = max(C/ngroups, 1);
    ksize = Hk * Wk;
    CgHkWk = Cg * ksize;
    fast_1x1 = ksize == 1 && stride_w == 1 && stride_h == 1 && pad_top == 0...",1,src\vkcom\src\op_conv.cpp,cv,9,cv,1
460709,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN
#define BLOCK_SIZE 64

#define MAX_GROUP_COUNT_X 65535
#define MAX_GROUP_COUNT_Y 65535
#define MAX_GROUP_COUNT_Z 65535

OpConv::OpConv(const Mat& weightBlob, const std::vector<float>& biasvec, int _activType, const int _ngroups, const int _K,
               const int _C, const int _Hk, const int _Wk, const int _stride_h, const int _stride_w,
               const int _dilation_h, const int _dilation_w, const int _pad_left, const int _pad_top):
               activ((FusedActivationType)_activType), ngroups(_ngroups), K(_K), C(_C), Hk(_Hk), Wk(_Wk), stride_h(_stride_h), stride_w(_stride_w),
               dilation_h(_dilation_h), dilation_w(_dilation_w), pad_left(_pad_left), pad_top(_pad_top)
{
    type_ = kOpTypeConv;
    CV_Assert(!weightBlob.empty());

    Kg = K/ngroups, Cg = max(C/ngroups, 1);
    ksize = Hk * Wk;
    CgHkWk = Cg * ksize;
    fast_1x1 = ksize == 1 && stride_w == 1 && stride_h == 1 && pad_top == 0 && pad_left ==...",16,src\vkcom\src\op_conv.cpp,cv.dnn,9,dnn,1
460710,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN
#define BLOCK_SIZE 64

#define MAX_GROUP_COUNT_X 65535
#define MAX_GROUP_COUNT_Y 65535
#define MAX_GROUP_COUNT_Z 65535

OpConv::OpConv(const Mat& weightBlob, const std::vector<float>& biasvec, int _activType, const int _ngroups, const int _K,
               const int _C, const int _Hk, const int _Wk, const int _stride_h, const int _stride_w,
               const int _dilation_h, const int _dilation_w, const int _pad_left, const int _pad_top):
               activ((FusedActivationType)_activType), ngroups(_ngroups), K(_K), C(_C), Hk(_Hk), Wk(_Wk), stride_h(_stride_h), stride_w(_stride_w),
               dilation_h(_dilation_h), dilation_w(_dilation_w), pad_left(_pad_left), pad_top(_pad_top)
{
    type_ = kOpTypeConv;
    CV_Assert(!weightBlob.empty());

    Kg = K/ngroups, Cg = max(C/ngroups, 1);
    ksize = Hk * Wk;
    CgHkWk = Cg * ksize;
    fast_1x1 = ksize == 1 && stride_w == 1 && stride_h == 1 && pad_top == 0 && pad_left == 0;

    if (ngr...",32,src\vkcom\src\op_conv.cpp,cv.dnn.vkcom,9,vkcom,1
460718,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\op_matmul.cpp,src\vkcom\src\op_matmul.cpp:<global>,,<global>,1
460722,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

#define KSTRIP_LEN 32
#define BLOCK_SIZE 64

OpMatMul::OpMatMul(std::vector<Mat>& matBlobs, const int _M, const int _K, const int _N) : M(_M), K(_K), N(_N)
{
    // Convert Weight to GPU Tensor.
    type_ = kOpTypeMatMul;
    CV_Assert(matBlobs.empty() || matBlobs.size() == 1);

    if (matBlobs.size() == 1)
    {
        Tensor weightTensor;
        CV_Assert(matBlobs[0].isContinuous() && matBlobs[0].type() == CV_32F);
        std::vector<int> matShape = shape(matBlobs[0]);
        weightTensor.reshape((const char*)matBlobs[0].data, matShape); // This code will copy the src data from Mat to VkBuffer.

        weightTensorPtr = makePtr<Tensor>(weightTensor);
    }
}

void OpMatMul::firstForward()
{
    if (!firstForwardFinsh)
    {
        config.local_size_x = BLOCK_SIZE;
        config.local_size_y = BLOCK_SIZE;
        config.local_size_z = 1;

        computeGroupCount();
        firstForwardFinsh = true;
    ...",1,src\vkcom\src\op_matmul.cpp,cv,9,cv,1
460723,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

#define KSTRIP_LEN 32
#define BLOCK_SIZE 64

OpMatMul::OpMatMul(std::vector<Mat>& matBlobs, const int _M, const int _K, const int _N) : M(_M), K(_K), N(_N)
{
    // Convert Weight to GPU Tensor.
    type_ = kOpTypeMatMul;
    CV_Assert(matBlobs.empty() || matBlobs.size() == 1);

    if (matBlobs.size() == 1)
    {
        Tensor weightTensor;
        CV_Assert(matBlobs[0].isContinuous() && matBlobs[0].type() == CV_32F);
        std::vector<int> matShape = shape(matBlobs[0]);
        weightTensor.reshape((const char*)matBlobs[0].data, matShape); // This code will copy the src data from Mat to VkBuffer.

        weightTensorPtr = makePtr<Tensor>(weightTensor);
    }
}

void OpMatMul::firstForward()
{
    if (!firstForwardFinsh)
    {
        config.local_size_x = BLOCK_SIZE;
        config.local_size_y = BLOCK_SIZE;
        config.local_size_z = 1;

        computeGroupCount();
        firstForwardFinsh = true;
    }
    else
    ...",16,src\vkcom\src\op_matmul.cpp,cv.dnn,9,dnn,1
460724,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

#define KSTRIP_LEN 32
#define BLOCK_SIZE 64

OpMatMul::OpMatMul(std::vector<Mat>& matBlobs, const int _M, const int _K, const int _N) : M(_M), K(_K), N(_N)
{
    // Convert Weight to GPU Tensor.
    type_ = kOpTypeMatMul;
    CV_Assert(matBlobs.empty() || matBlobs.size() == 1);

    if (matBlobs.size() == 1)
    {
        Tensor weightTensor;
        CV_Assert(matBlobs[0].isContinuous() && matBlobs[0].type() == CV_32F);
        std::vector<int> matShape = shape(matBlobs[0]);
        weightTensor.reshape((const char*)matBlobs[0].data, matShape); // This code will copy the src data from Mat to VkBuffer.

        weightTensorPtr = makePtr<Tensor>(weightTensor);
    }
}

void OpMatMul::firstForward()
{
    if (!firstForwardFinsh)
    {
        config.local_size_x = BLOCK_SIZE;
        config.local_size_y = BLOCK_SIZE;
        config.local_size_z = 1;

        computeGroupCount();
        firstForwardFinsh = true;
    }
    else
        return;
}

b...",32,src\vkcom\src\op_matmul.cpp,cv.dnn.vkcom,9,vkcom,1
460732,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\pipeline.cpp,src\vkcom\src\pipeline.cpp:<global>,,<global>,1
460736,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// *********************** Descriptor ********************
Ptr<Descriptor> Descriptor::create(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* pipeline)
{
    return Ptr<Descriptor>(new Descriptor(pool, set, pipeline));
}

Descriptor::Descriptor(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* _pipeline)
: desPool(pool), desSet(set), pipeline(_pipeline)
{
}

void Descriptor::writeTensor(Tensor tensor, int bindIndex)
{
    writeBuffer(tensor.getBuffer()->getVkBuffer(), bindIndex, tensor.size()); // TODO, check if need the size in bit.
}

void Descriptor::writeBuffer(VkBuffer buffer, int bindIndex, size_t size, VkDeviceSize offset)
{
    CV_Assert(pipeline);
    VkWriteDescriptorSet writeSet = {};
    VkDescriptorBufferInfo sourceInfo;
    sourceInfo.buffer        = buffer;
    sourceInfo.offset        = offset;
    sourceInfo.range         = size;

    writeSet.sType           ...",1,src\vkcom\src\pipeline.cpp,cv,23,cv,1
460737,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

// *********************** Descriptor ********************
Ptr<Descriptor> Descriptor::create(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* pipeline)
{
    return Ptr<Descriptor>(new Descriptor(pool, set, pipeline));
}

Descriptor::Descriptor(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* _pipeline)
: desPool(pool), desSet(set), pipeline(_pipeline)
{
}

void Descriptor::writeTensor(Tensor tensor, int bindIndex)
{
    writeBuffer(tensor.getBuffer()->getVkBuffer(), bindIndex, tensor.size()); // TODO, check if need the size in bit.
}

void Descriptor::writeBuffer(VkBuffer buffer, int bindIndex, size_t size, VkDeviceSize offset)
{
    CV_Assert(pipeline);
    VkWriteDescriptorSet writeSet = {};
    VkDescriptorBufferInfo sourceInfo;
    sourceInfo.buffer        = buffer;
    sourceInfo.offset        = offset;
    sourceInfo.range         = size;

    writeSet.sType           = VK_STRUCTURE_...",16,src\vkcom\src\pipeline.cpp,cv.dnn,23,dnn,1
460738,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

// *********************** Descriptor ********************
Ptr<Descriptor> Descriptor::create(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* pipeline)
{
    return Ptr<Descriptor>(new Descriptor(pool, set, pipeline));
}

Descriptor::Descriptor(const VkDescriptorPool& pool, const VkDescriptorSet& set, Pipeline* _pipeline)
: desPool(pool), desSet(set), pipeline(_pipeline)
{
}

void Descriptor::writeTensor(Tensor tensor, int bindIndex)
{
    writeBuffer(tensor.getBuffer()->getVkBuffer(), bindIndex, tensor.size()); // TODO, check if need the size in bit.
}

void Descriptor::writeBuffer(VkBuffer buffer, int bindIndex, size_t size, VkDeviceSize offset)
{
    CV_Assert(pipeline);
    VkWriteDescriptorSet writeSet = {};
    VkDescriptorBufferInfo sourceInfo;
    sourceInfo.buffer        = buffer;
    sourceInfo.offset        = offset;
    sourceInfo.range         = size;

    writeSet.sType           = VK_STRUCTURE_TYPE_WRITE_DESCR...",32,src\vkcom\src\pipeline.cpp,cv.dnn.vkcom,23,vkcom,1
460744,NAMESPACE_BLOCK,<empty>,,src\vkcom\src\tensor.cpp,src\vkcom\src\tensor.cpp:<global>,,<global>,1
460748,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

Tensor::Tensor(Format fmt, VkBufferUsageFlags usageFlag) : size_in_byte_(0), format_(fmt), usageFlag_(usageFlag)
{
}

Tensor::Tensor(const char* data, std::vector<int>& shape, Format fmt, VkBufferUsageFlags usageFlag)
               : size_in_byte_(0), format_(fmt), usageFlag_(usageFlag)
{
    reshape(data, shape);
}

void* Tensor::map()
{
    void *p;

    VK_CHECK_RESULT(vkMapMemory(kDevice, buffer_->getVkMemory(),
                                0, size_in_byte_, 0, (void **)&p));

    return p;
}

void Tensor::unMap()
{
    vkUnmapMemory(kDevice, buffer_->getVkMemory());
}

Shape Tensor::getShape() const
{
    return shape_;
}

int Tensor::count(const int start_axis, const int end_axis) const
{
    return shapeCount(shape_, start_axis, end_axis);
}

int Tensor::dimSize(const int axis) const
{
    CV_Assert(axis >= 0);
    CV_Assert(axis < shape_.size());

    return shape_[axis];
}

int Tensor::dimNum() const
...",1,src\vkcom\src\tensor.cpp,cv,11,cv,1
460749,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN

Tensor::Tensor(Format fmt, VkBufferUsageFlags usageFlag) : size_in_byte_(0), format_(fmt), usageFlag_(usageFlag)
{
}

Tensor::Tensor(const char* data, std::vector<int>& shape, Format fmt, VkBufferUsageFlags usageFlag)
               : size_in_byte_(0), format_(fmt), usageFlag_(usageFlag)
{
    reshape(data, shape);
}

void* Tensor::map()
{
    void *p;

    VK_CHECK_RESULT(vkMapMemory(kDevice, buffer_->getVkMemory(),
                                0, size_in_byte_, 0, (void **)&p));

    return p;
}

void Tensor::unMap()
{
    vkUnmapMemory(kDevice, buffer_->getVkMemory());
}

Shape Tensor::getShape() const
{
    return shape_;
}

int Tensor::count(const int start_axis, const int end_axis) const
{
    return shapeCount(shape_, start_axis, end_axis);
}

int Tensor::dimSize(const int axis) const
{
    CV_Assert(axis >= 0);
    CV_Assert(axis < shape_.size());

    return shape_[axis];
}

int Tensor::dimNum() const
{
    return sh...",16,src\vkcom\src\tensor.cpp,cv.dnn,11,dnn,1
460750,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN

Tensor::Tensor(Format fmt, VkBufferUsageFlags usageFlag) : size_in_byte_(0), format_(fmt), usageFlag_(usageFlag)
{
}

Tensor::Tensor(const char* data, std::vector<int>& shape, Format fmt, VkBufferUsageFlags usageFlag)
               : size_in_byte_(0), format_(fmt), usageFlag_(usageFlag)
{
    reshape(data, shape);
}

void* Tensor::map()
{
    void *p;

    VK_CHECK_RESULT(vkMapMemory(kDevice, buffer_->getVkMemory(),
                                0, size_in_byte_, 0, (void **)&p));

    return p;
}

void Tensor::unMap()
{
    vkUnmapMemory(kDevice, buffer_->getVkMemory());
}

Shape Tensor::getShape() const
{
    return shape_;
}

int Tensor::count(const int start_axis, const int end_axis) const
{
    return shapeCount(shape_, start_axis, end_axis);
}

int Tensor::dimSize(const int axis) const
{
    CV_Assert(axis >= 0);
    CV_Assert(axis < shape_.size());

    return shape_[axis];
}

int Tensor::dimNum() const
{
    return shape_.size();
}

...",32,src\vkcom\src\tensor.cpp,cv.dnn.vkcom,11,vkcom,1
460752,NAMESPACE_BLOCK,<empty>,,src\vkcom\vulkan\function_list.inl.hpp,src\vkcom\vulkan\function_list.inl.hpp:<global>,,<global>,1
460763,NAMESPACE_BLOCK,<empty>,,src\vkcom\vulkan\vk_functions.cpp,src\vkcom\vulkan\vk_functions.cpp:<global>,,<global>,1
460774,NAMESPACE_BLOCK,<empty>,,src\vkcom\vulkan\vk_functions.hpp,src\vkcom\vulkan\vk_functions.hpp:<global>,,<global>,1
460799,NAMESPACE_BLOCK,<empty>,,src\vkcom\vulkan\vk_loader.cpp,src\vkcom\vulkan\vk_loader.cpp:<global>,,<global>,1
460803,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN
static VulkanHandle handle = nullptr;

bool loadVulkanFunctions(VkInstance& instance)
{
#define VK_FUNC(fun) \
    fun = (PFN_##fun)vkGetInstanceProcAddr(instance, #fun);

#define VK_FUNC_MANDATORY(fun) \
    VK_FUNC(fun) \
    if(!fun) \
    { \
      fprintf(stderr, ""Could not load Vulkan function: %s !\n"", #fun); \
      return false; \
    }

#include ""function_list.inl.hpp""
    return true;
}

bool loadVulkanGlobalFunctions()
{
#define VK_GLOBAL_LEVEL_FUNC(fun) \
    fun = (PFN_##fun)vkGetInstanceProcAddr(nullptr, #fun);

#define VK_GLOBAL_LEVEL_FUNC_MANDATORY(fun) \
    VK_GLOBAL_LEVEL_FUNC(fun) \
    if(!fun) \
    { \
      fprintf(stderr, ""Could not load global Vulkan function: %s !\n"", #fun); \
      return false; \
    }

#include ""function_list.inl.hpp""
    return true;
}

bool loadVulkanEntry()
{
    if (handle == nullptr)
        return false;

    vkGetInstanceProcAddr = GET_VK_ENTRY_POINT(handle);
 ...",1,src\vkcom\vulkan\vk_loader.cpp,cv,59,cv,1
460804,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN
static VulkanHandle handle = nullptr;

bool loadVulkanFunctions(VkInstance& instance)
{
#define VK_FUNC(fun) \
    fun = (PFN_##fun)vkGetInstanceProcAddr(instance, #fun);

#define VK_FUNC_MANDATORY(fun) \
    VK_FUNC(fun) \
    if(!fun) \
    { \
      fprintf(stderr, ""Could not load Vulkan function: %s !\n"", #fun); \
      return false; \
    }

#include ""function_list.inl.hpp""
    return true;
}

bool loadVulkanGlobalFunctions()
{
#define VK_GLOBAL_LEVEL_FUNC(fun) \
    fun = (PFN_##fun)vkGetInstanceProcAddr(nullptr, #fun);

#define VK_GLOBAL_LEVEL_FUNC_MANDATORY(fun) \
    VK_GLOBAL_LEVEL_FUNC(fun) \
    if(!fun) \
    { \
      fprintf(stderr, ""Could not load global Vulkan function: %s !\n"", #fun); \
      return false; \
    }

#include ""function_list.inl.hpp""
    return true;
}

bool loadVulkanEntry()
{
    if (handle == nullptr)
        return false;

    vkGetInstanceProcAddr = GET_VK_ENTRY_POINT(handle);
    if (!vkGetIn...",16,src\vkcom\vulkan\vk_loader.cpp,cv.dnn,59,dnn,1
460805,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN
static VulkanHandle handle = nullptr;

bool loadVulkanFunctions(VkInstance& instance)
{
#define VK_FUNC(fun) \
    fun = (PFN_##fun)vkGetInstanceProcAddr(instance, #fun);

#define VK_FUNC_MANDATORY(fun) \
    VK_FUNC(fun) \
    if(!fun) \
    { \
      fprintf(stderr, ""Could not load Vulkan function: %s !\n"", #fun); \
      return false; \
    }

#include ""function_list.inl.hpp""
    return true;
}

bool loadVulkanGlobalFunctions()
{
#define VK_GLOBAL_LEVEL_FUNC(fun) \
    fun = (PFN_##fun)vkGetInstanceProcAddr(nullptr, #fun);

#define VK_GLOBAL_LEVEL_FUNC_MANDATORY(fun) \
    VK_GLOBAL_LEVEL_FUNC(fun) \
    if(!fun) \
    { \
      fprintf(stderr, ""Could not load global Vulkan function: %s !\n"", #fun); \
      return false; \
    }

#include ""function_list.inl.hpp""
    return true;
}

bool loadVulkanEntry()
{
    if (handle == nullptr)
        return false;

    vkGetInstanceProcAddr = GET_VK_ENTRY_POINT(handle);
    if (!vkGetInstanceProcAddr)
...",32,src\vkcom\vulkan\vk_loader.cpp,cv.dnn.vkcom,59,vkcom,1
460809,NAMESPACE_BLOCK,<empty>,,src\vkcom\vulkan\vk_loader.hpp,src\vkcom\vulkan\vk_loader.hpp:<global>,,<global>,1
460813,NAMESPACE_BLOCK,"namespace cv { namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN
bool loadVulkanLibrary();
bool loadVulkanEntry();
bool loadVulkanGlobalFunctions();
bool loadVulkanFunctions(VkInstance& instance);
#endif // HAVE_VULKAN

}}}",1,src\vkcom\vulkan\vk_loader.hpp,cv,15,cv,1
460814,NAMESPACE_BLOCK,"namespace dnn { namespace vkcom {

#ifdef HAVE_VULKAN
bool loadVulkanLibrary();
bool loadVulkanEntry();
bool loadVulkanGlobalFunctions();
bool loadVulkanFunctions(VkInstance& instance);
#endif // HAVE_VULKAN

}}",16,src\vkcom\vulkan\vk_loader.hpp,cv.dnn,15,dnn,1
460815,NAMESPACE_BLOCK,"namespace vkcom {

#ifdef HAVE_VULKAN
bool loadVulkanLibrary();
bool loadVulkanEntry();
bool loadVulkanGlobalFunctions();
bool loadVulkanFunctions(VkInstance& instance);
#endif // HAVE_VULKAN

}",32,src\vkcom\vulkan\vk_loader.hpp,cv.dnn.vkcom,15,vkcom,1
467024,NAMESPACE_BLOCK,<empty>,,<includes>,<includes>:<global>,,<global>,1
